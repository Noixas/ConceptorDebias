{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of WEAT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsedoc/ConceptorDebias/blob/master/Copy_of_WEAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "l8Me4_OyFyKo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# WEAT Algorithm\n",
        "## Test Statistic"
      ]
    },
    {
      "metadata": {
        "id": "mf6_liysF8en",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# returns s(w, A, B) for all w in W (passed as argument). Shape: n_words (in W) x 1\n",
        "def swAB(W, A, B):\n",
        "  #Calculate cosine-similarity between W and A, W and B\n",
        "  #print(\"W: \", W.shape, \" A: \", A.shape, \" B: \", B.shape)\n",
        "  WA = cosine_similarity(W,A)\n",
        "  WB = cosine_similarity(W,B)\n",
        "  #print('WA shape: ', WA.shape)\n",
        "  #Take mean along columns\n",
        "  WAmean = np.mean(WA, axis = 1)\n",
        "  WBmean = np.mean(WB, axis = 1)\n",
        "  \n",
        "  #print('sWAB shape: ', WAmean.shape)\n",
        "  \n",
        "  return (WAmean - WBmean)\n",
        "  \n",
        "def test_statistic(X, Y, A, B):\n",
        "  return (sum(swAB(X, A, B)) - sum(swAB(Y, A, B)))\n",
        "\n",
        "def weat_effect_size(X, Y, A, B, embd):\n",
        "  #Convert the set of words to matrix\n",
        "  \n",
        "  Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
        "  Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
        "  Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
        "  Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
        "  \n",
        "  # Find X U Y\n",
        "  XuY = list(set(X).union(Y))\n",
        "  XuYmat = []\n",
        "  for w in XuY:\n",
        "    if w.lower() in embd:\n",
        "      XuYmat.append(embd[w.lower()])\n",
        "  XuYmat = np.array(XuYmat)\n",
        "#   print('X U Y Shape: ', XuYmat.shape)\n",
        "#   print('X Shape: ', Xmat.shape)\n",
        "#   print('Y Shape: ', Ymat.shape)\n",
        "#   print('A Shape: ', Amat.shape)\n",
        "#   print('B Shape: ', Bmat.shape)\n",
        "  \n",
        "  d = (np.mean(swAB(Xmat,Amat,Bmat)) - np.mean(swAB(Ymat,Amat,Bmat)))/np.std(swAB(XuYmat, Amat, Bmat))\n",
        "  \n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d8foGwVSGI16",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## P-Value"
      ]
    },
    {
      "metadata": {
        "id": "ZDy-duFOFj71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "from itertools import combinations, filterfalse\n",
        "\n",
        "def random_permutation(iterable, r=None):\n",
        "  pool = tuple(iterable)\n",
        "  r = len(pool) if r is None else r\n",
        "  return tuple(random.sample(pool, r))\n",
        "\n",
        "def weat_p_value(X, Y, A, B, embd, sample):\n",
        "  size_of_permutation = min(len(X), len(Y))\n",
        "  X_Y = X + Y\n",
        "  test_stats_over_permutation = []\n",
        "  \n",
        "  Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
        "  Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
        "  Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
        "  Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
        "  \n",
        "  if not sample:\n",
        "      permutations = combinations(X_Y, size_of_permutation)\n",
        "  else:\n",
        "      permutations = [random_permutation(X_Y, size_of_permutation) for s in range(sample)]\n",
        "      \n",
        "  for Xi in permutations:\n",
        "    Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
        "    Ximat = np.array([embd[w.lower()] for w in Xi if w.lower() in embd])\n",
        "    Yimat = np.array([embd[w.lower()] for w in Yi if w.lower() in embd])\n",
        "    test_stats_over_permutation.append(test_statistic(Ximat, Yimat, Amat, Bmat))\n",
        "    \n",
        "  unperturbed = test_statistic(Xmat, Ymat, Amat, Bmat)\n",
        "  \n",
        "  is_over = np.array([o > unperturbed for o in test_stats_over_permutation])\n",
        "  #print(\"All: \", test_stats_over_permutation)\n",
        "  #print(\"Unpertrubed: \", unperturbed)\n",
        "  return is_over.sum() / is_over.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NbRMmhwbGL98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test on sample input"
      ]
    },
    {
      "metadata": {
        "id": "upscdvtwFm0l",
        "colab_type": "code",
        "outputId": "c9706781-f440-4e9f-86a0-4e3a0ca7ef9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "X = [\"bagpipe\", \"cello\", \"guitar\", \"lute\", \"trombone\", \"banjo\", \"clarinet\", \"harmonica\", \"mandolin\", \"trumpet\", \"bassoon\", \"drum\", \"harp\", \"oboe\", \"tuba\", \"bell\", \"fiddle\", \"harpsichord\", \"piano\", \"viola\", \"bongo\",\n",
        "\"flute\", \"horn\", \"saxophone\", \"violin\"] #Instruments\n",
        "Y = [\"arrow\", \"club\", \"gun\", \"missile\", \"spear\", \"axe\", \"dagger\", \"harpoon\", \"pistol\", \"sword\", \"blade\", \"dynamite\", \"hatchet\", \"rifle\", \"tank\", \"bomb\", \"firearm\", \"knife\", \"shotgun\", \"teargas\", \"cannon\", \"grenade\",\n",
        "    \"mace\", \"slingshot\", \"whip\"] #Weapons\n",
        "A = [\"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\",\n",
        "    \"happy\", \"laughter\", \"paradise\", \"vacation\"] #Pleasant\n",
        "B = [\"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\",\n",
        "    \"vomit\", \"agony\", \"prison\"] #Unpleasant\n",
        "\n",
        "#Load word embeddings\n",
        "#load gensim formatted Full Glove embeddings\n",
        "#!gdown https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = ''\n",
        "\n",
        "\n",
        "glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
        "print('The glove embedding has been loaded!')\n",
        "\n",
        "print('WEAT d = ', weat_effect_size(X, Y, A, B, glove))\n",
        "print('WEAT p = ', weat_p_value(X, Y, A, B, glove, 1000))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The glove embedding has been loaded!\n",
            "WEAT d =  1.5495627\n",
            "Unpertrubed:  2.2905553244054317\n",
            "WEAT p =  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rfwYNFl2ckGy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## WEAT with conceptor debiased embeddings"
      ]
    },
    {
      "metadata": {
        "id": "xEuq-Lt5ctV2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compute the conceptor matrix for all words and gender specific words."
      ]
    },
    {
      "metadata": {
        "id": "HAG4oqwIc1Z3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Compute the conceptor matrix\n",
        "def process_cn_matrix(subspace, alpha = 2):\n",
        "  #print(\"starting...\")\n",
        "  #x = orig_embd.vectors\n",
        "  #print(subspace.shape)\n",
        "  \n",
        "  #Calculate the correlation matrix\n",
        "  R = subspace.dot(subspace.T)/(subspace.shape[1])\n",
        "  #print(\"R calculated\")\n",
        "  \n",
        "  #Calculate the conceptor matrix\n",
        "  C = R @ (np.linalg.inv(R + alpha ** (-2) * np.eye(subspace.shape[0])))\n",
        "  #print(\"C calculated\")\n",
        "  \n",
        "  #Calculate the negation of the conceptor matrix\n",
        "  negC = np.eye(subspace.shape[0]) - C\n",
        "  #print(\"negC calculated\")\n",
        "  \n",
        "  return negC\n",
        "\n",
        "def apply_conceptor(x, C):\n",
        "  #Post-process the vocab matrix\n",
        "  newX = (C @ x).T\n",
        "  print(newX.shape)\n",
        "  return newX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GHaXxx5L3Ugb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load embeddings of all words from the ref. wordlist from a specific embedding"
      ]
    },
    {
      "metadata": {
        "id": "8WBI1VED3bpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Arguments - embd: The word embeddings in the form of a dict || wikiWordsPath: List of words to be considered\n",
        "def load_all_vectors(embd, wikiWordsPath):\n",
        "  all_words_index = {}\n",
        "  all_words_mat = []\n",
        "  with open(wikiWordsPath, \"r+\") as f_in:\n",
        "    ind = 0\n",
        "    for line in f_in:\n",
        "      word = line.split(' ')[0]\n",
        "      if word in embd:\n",
        "        all_words_index[word] = ind\n",
        "        all_words_mat.append(embd[word])\n",
        "        ind = ind+1\n",
        "        \n",
        "  return all_words_index, all_words_mat\n",
        "\n",
        "def load_subspace_vectors(embd, subspace_words):\n",
        "  subspace_embd_mat = []\n",
        "  ind = 0\n",
        "  for word in subspace_words:\n",
        "    if word in embd:\n",
        "      subspace_embd_mat.append(embd[word])\n",
        "      ind = ind+1\n",
        "      \n",
        "  return subspace_embd_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hH87LMqmQuhb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "431942b5-df61-4eac-e0fa-701ffb32fad7"
      },
      "cell_type": "code",
      "source": [
        "all_words_index, all_words_mat = load_all_vectors(glove, wikiWordsPath)\n",
        "\n",
        "aaa = list(all_words_index.keys())\n",
        "X = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "print(X)\n",
        "a = ['T' for w in X if w.lower() in aaa]\n",
        "print(a)\n",
        "print(np.array(a).shape)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Brad', 'Brendan', 'Geoffrey', 'Greg', 'Brett', 'Jay', 'Matthew', 'Neil', 'Todd', 'Allison', 'Anne', 'Carrie', 'Emily', 'Jill', 'Laurie', 'Kristen', 'Meredith', 'Sarah']\n",
            "['T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T']\n",
            "(18,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vjtc01gf8aR4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Conceptor all words and store it in a dictonary"
      ]
    },
    {
      "metadata": {
        "id": "EzfTYgq0I9WW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load all word lists - Subspace"
      ]
    },
    {
      "metadata": {
        "id": "D4kndsi_DwNr",
        "colab_type": "code",
        "outputId": "173123e6-69e3-465d-b333-5edc4739864a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "cell_type": "code",
      "source": [
        "#If running on local\n",
        "!mkdir content\n",
        "%cd content\n",
        "# General word list\n",
        "!wget https://raw.githubusercontent.com/IlyaSemenov/wikipedia-word-frequency/master/results/enwiki-20150602-words-frequency.txt\n",
        "!git clone https://github.com/PrincetonML/SIF\n",
        "    \n",
        "# Gender word lists\n",
        "!git clone https://github.com/uclanlp/gn_glove\n",
        "!git clone https://github.com/uclanlp/corefBias\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/female.txt\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/male.txt\n",
        "    \n",
        "# our code for debiasing -- also includes word lists    \n",
        "!git clone https://github.com/jsedoc/ConceptorDebias"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘content’: File exists\n",
            "/home/saketk/ConceptorDebias/content\n",
            "--2019-02-28 22:28:27--  https://raw.githubusercontent.com/IlyaSemenov/wikipedia-word-frequency/master/results/enwiki-20150602-words-frequency.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23741395 (23M) [text/plain]\n",
            "Saving to: ‘enwiki-20150602-words-frequency.txt’\n",
            "\n",
            "enwiki-20150602-wor 100%[===================>]  22.64M  29.6MB/s    in 0.8s    \n",
            "\n",
            "2019-02-28 22:28:28 (29.6 MB/s) - ‘enwiki-20150602-words-frequency.txt’ saved [23741395/23741395]\n",
            "\n",
            "fatal: destination path 'SIF' already exists and is not an empty directory.\n",
            "fatal: destination path 'gn_glove' already exists and is not an empty directory.\n",
            "fatal: destination path 'corefBias' already exists and is not an empty directory.\n",
            "--2019-02-28 22:28:28--  https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/female.txt\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35751 (35K) [text/plain]\n",
            "Saving to: ‘female.txt’\n",
            "\n",
            "female.txt          100%[===================>]  34.91K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2019-02-28 22:28:28 (2.50 MB/s) - ‘female.txt’ saved [35751/35751]\n",
            "\n",
            "--2019-02-28 22:28:29--  https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/male.txt\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20466 (20K) [text/plain]\n",
            "Saving to: ‘male.txt’\n",
            "\n",
            "male.txt            100%[===================>]  19.99K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-02-28 22:28:29 (265 MB/s) - ‘male.txt’ saved [20466/20466]\n",
            "\n",
            "fatal: destination path 'ConceptorDebias' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "etG2ntoMTMiP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "c8154449-29d5-46a3-d193-1e82e652e054"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CN.ipynb\t\t\t\t     flair_bert.ipynb\r\n",
            "ConceptorDebias\t\t\t\t     flair_elmo_bert.ipynb\r\n",
            "conceptor_fxns.py\t\t\t     gn_glove\r\n",
            "ConceptorPostPro_DEMO.ipynb\t\t     Greenwald_1998_Perez_2010_lists.py\r\n",
            "content\t\t\t\t\t     LICENSE\r\n",
            "Copy_of_WEAT.ipynb\t\t\t     load_word_lists.py\r\n",
            "corefBias\t\t\t\t     male.txt\r\n",
            "Debias_BERT.ipynb\t\t\t     plot_pc.py\r\n",
            "Debias_Contextualized_Representations.ipynb  __pycache__\r\n",
            "Debiasing_WE.ipynb\t\t\t     README.md\r\n",
            "Debias_paper_section_1.ipynb\t\t     Re_implement_CN.ipynb\r\n",
            "EEC_with_CN.ipynb\t\t\t     SIF\r\n",
            "enwiki-20150602-words-frequency.txt\t     WEAT.ipynb\r\n",
            "female.txt\t\t\t\t     WEAT_lists.py\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "orBvfpqCFOfa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from load_word_lists import *\n",
        "\n",
        "gender_list_pronouns = WEATLists.W_7_Male_terms + WEATLists.W_7_Female_terms + WEATLists.W_8_Male_terms + WEATLists.W_8_Female_terms\n",
        "gender_list_pronouns = list(set(gender_list_pronouns))\n",
        "\n",
        "gender_list_extended = male_vino_extra + female_vino_extra + male_gnGlove + female_gnGlove\n",
        "gender_list_extended = list(set(gender_list_extended))\n",
        "\n",
        "gender_list_propernouns = male_cmu + female_cmu\n",
        "gender_list_propernouns = list(set(gender_list_propernouns))\n",
        "\n",
        "gender_list_all = gender_list_pronouns + gender_list_extended + gender_list_propernouns\n",
        "gender_list_all = list(set(gender_list_all))\n",
        "\n",
        "race_list = WEATLists.W_3_Unused_full_list_European_American_names + WEATLists.W_3_European_American_names + WEATLists.W_3_Unused_full_list_African_American_names + WEATLists.W_3_African_American_names + WEATLists.W_4_Unused_full_list_European_American_names + WEATLists.W_4_European_American_names + WEATLists.W_4_Unused_full_list_African_American_names + WEATLists.W_4_African_American_names + WEATLists.W_5_Unused_full_list_European_American_names + WEATLists.W_5_European_American_names + WEATLists.W_5_Unused_full_list_African_American_names + WEATLists.W_5_African_American_names \n",
        "race_list = list(set(race_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kEiBEVI2I2WR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load different embeddings"
      ]
    },
    {
      "metadata": {
        "id": "Nrlqu2xvJH_6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Glove"
      ]
    },
    {
      "metadata": {
        "id": "PFNSjmY8I1Ut",
        "colab_type": "code",
        "outputId": "d4d8e94f-6d99-4fbd-ad25-1f2c9d1c2bb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Load word embeddings\n",
        "#download gensim formatted Full Glove embeddings\n",
        "#!gdown https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = ''\n",
        "\n",
        "glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
        "print('The glove embedding has been loaded!')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The glove embedding has been loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aD-bmiCdGKYt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1108
        },
        "outputId": "cd1d3156-af53-42a0-abb6-484040a30b09"
      },
      "cell_type": "code",
      "source": [
        "X = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "print(X)\n",
        "a = [glove[w] for w in X if w.lower() in glove]\n",
        "print(np.array(a).shape)\n",
        "glove['Brad']\n",
        "#glove['brad']"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Brad', 'Brendan', 'Geoffrey', 'Greg', 'Brett', 'Jay', 'Matthew', 'Neil', 'Todd', 'Allison', 'Anne', 'Carrie', 'Emily', 'Jill', 'Laurie', 'Kristen', 'Meredith', 'Sarah']\n",
            "(18, 300)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.5609e-01, -1.5396e-01, -5.0346e-01,  2.5138e-01,  3.3952e-01,\n",
              "        2.6105e-01,  3.0257e-01, -1.1273e-01,  4.6391e-02,  2.1197e+00,\n",
              "        1.9856e-01, -3.1840e-02, -4.4469e-01, -8.6130e-03, -3.6665e-01,\n",
              "       -7.8715e-02, -1.8301e-01, -3.5935e-01, -2.9033e-01,  3.2755e-01,\n",
              "       -1.1633e-01, -5.3637e-02, -3.1589e-01, -9.0069e-02,  5.4889e-02,\n",
              "        1.0793e-01, -7.6549e-02,  3.2448e-01, -3.7830e-01, -7.1353e-03,\n",
              "       -9.2837e-02, -3.4702e-01, -4.4372e-02, -3.6582e-02,  6.6447e-02,\n",
              "        4.3170e-01,  1.4958e-01,  4.1407e-01, -7.3515e-04, -5.4028e-01,\n",
              "        3.4552e-01, -5.6781e-02, -1.9483e-01,  3.7953e-01,  4.6593e-02,\n",
              "        4.1634e-02, -5.3288e-01,  1.4703e-01,  2.6140e-02, -1.6343e-03,\n",
              "        3.7304e-01, -1.7348e-01,  2.5698e-02, -2.5498e-01, -1.9006e-01,\n",
              "       -5.8404e-01, -6.0172e-03,  4.6960e-02,  1.4949e-02,  4.7884e-01,\n",
              "        1.8709e-01,  1.1012e-01, -2.9987e-01,  3.7356e-02, -4.1855e-02,\n",
              "        1.2758e-02,  1.5549e-01, -7.8430e-02,  3.5795e-01, -2.1139e-02,\n",
              "        2.1553e-01, -1.7305e-01,  2.9180e-01,  3.4817e-01, -8.2386e-01,\n",
              "        5.0522e-02, -4.4454e-02, -1.3776e-01,  1.6891e-01, -2.1201e-01,\n",
              "        3.8606e-01,  2.7904e-02, -2.6739e-02, -1.1496e-01,  2.7703e-01,\n",
              "       -1.9411e-01,  1.8388e-01, -3.9542e-01,  1.9627e-02,  3.7480e-01,\n",
              "        2.8426e-01, -2.4735e-01,  2.1075e-01,  5.5669e-01,  1.5538e-01,\n",
              "       -3.4910e-01, -4.2342e-01, -2.9979e-01, -9.9152e-02,  4.6956e-02,\n",
              "        2.3423e-01,  4.2039e-01,  5.7573e-02,  1.3133e-01,  3.0233e-01,\n",
              "        1.8890e+00, -6.2675e-02,  3.3504e-01,  1.6672e-02,  2.1146e-01,\n",
              "        3.0714e-01, -5.6883e-02,  1.7517e-01,  4.0338e-01,  1.4808e-01,\n",
              "        6.4289e-02,  1.9331e-01, -3.7482e-01, -1.7096e-02,  6.1206e-01,\n",
              "       -6.6393e-02,  3.5268e-01,  3.2031e-01,  7.6724e-02,  4.0608e-01,\n",
              "        2.4221e-01, -5.1967e-01,  5.2104e-01,  3.0191e-01,  4.5360e-01,\n",
              "        3.1720e-01,  7.0415e-01, -8.9437e-03,  4.1537e-01, -3.0064e-01,\n",
              "       -1.2406e-01,  3.2650e-01, -2.3340e-01,  5.8629e-02, -1.4551e-01,\n",
              "       -1.1687e+00, -2.0016e-01, -1.7595e-01,  5.1038e-01,  1.3941e-01,\n",
              "       -1.3492e-01, -1.6592e-01, -6.4183e-01, -7.4643e-02,  1.2845e-01,\n",
              "        1.3640e-01, -5.3371e-01,  2.3772e-01,  3.1996e-02,  6.7226e-02,\n",
              "        9.8275e-02,  3.5349e-01,  3.0168e-02, -3.3902e-01,  2.3055e-01,\n",
              "        2.0858e-01,  1.3370e-01, -4.0881e-01, -5.6269e-01, -2.6002e-01,\n",
              "        4.5334e-01,  1.1423e-01, -2.2129e-01,  3.9015e-02, -1.7528e-01,\n",
              "        1.3944e-01, -4.0646e-01,  1.5931e-01,  5.8924e-02,  1.3164e-01,\n",
              "       -7.0105e-02, -1.7987e-01, -2.0772e-01,  4.0103e-01,  1.5782e-01,\n",
              "        3.2983e-02, -3.7376e-01, -3.6251e-01,  2.0702e-01, -2.3175e-01,\n",
              "        2.8544e-01,  2.1287e-01,  1.2014e-01, -9.5387e-02, -1.1966e-01,\n",
              "       -2.1259e-01,  2.1491e-01, -2.2424e-01,  2.0714e-01, -3.5469e-01,\n",
              "       -1.8725e-01, -1.3854e-02,  1.3169e-01,  2.9420e-02, -4.6341e-01,\n",
              "       -3.8674e-01, -4.1141e-01, -9.1650e-02, -4.1818e-01, -6.5491e-01,\n",
              "        5.0581e-02, -1.2775e-02,  1.0144e-01, -3.5621e-01,  4.9632e-01,\n",
              "       -1.0433e-01,  9.9404e-02,  4.8235e-01,  2.3474e-01, -4.7720e-01,\n",
              "        7.9219e-01, -1.1258e-02,  3.5949e-03,  2.7460e-02, -1.7296e-01,\n",
              "       -2.2248e-01,  2.2229e-01,  1.3333e-01,  4.0048e-01,  4.2337e-01,\n",
              "       -5.1213e-01, -8.2212e-02,  8.7370e-02,  1.5837e-02, -1.3151e-02,\n",
              "       -5.1821e-03,  1.0908e-01,  2.6508e-01, -4.1608e-01, -1.6802e-01,\n",
              "       -1.3486e-01,  1.7403e-01,  2.8325e-01,  3.2487e-01,  1.3839e-01,\n",
              "        4.6552e-01, -3.6659e-01,  2.4266e-01,  1.3700e-01,  4.0531e-01,\n",
              "        4.8374e-01, -2.8112e-02, -3.2147e-01,  1.3126e-01,  2.2298e-01,\n",
              "        3.6765e-02, -1.8370e-01, -1.4024e-01, -7.2459e-02, -1.9426e-01,\n",
              "       -4.2977e-01, -1.6731e-01,  1.7215e-01, -1.0831e-01,  2.5391e-01,\n",
              "        4.9705e-01, -8.7651e-02, -7.4383e-02, -8.5615e-02, -3.8821e-02,\n",
              "        3.2831e-01, -1.7713e-01, -1.5463e-01,  5.2676e-01,  1.0048e-01,\n",
              "       -4.9239e-02, -2.3351e-02,  1.5625e-01, -3.1273e-01, -4.2893e-01,\n",
              "        7.2558e-02,  1.1981e-01, -5.4989e-02,  1.1979e-01,  8.6490e-02,\n",
              "       -3.9215e-01, -2.1741e-01, -3.4306e-01,  3.9574e-01, -3.3915e-01,\n",
              "       -4.3442e-01, -4.4865e-01, -1.4487e-01,  4.9300e-02,  1.6221e-01,\n",
              "        4.6767e-01,  3.1584e-01,  2.5845e-01,  3.2365e-01,  1.0205e-01,\n",
              "        4.6215e-01, -1.1424e-01, -1.3349e-01, -3.5560e-01,  7.2325e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "metadata": {
        "id": "84af1zflJKLQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Word2vec"
      ]
    },
    {
      "metadata": {
        "id": "OmJlU2IiJMMj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62c8c185-387a-4bad-e3bb-1e4ab7022301"
      },
      "cell_type": "code",
      "source": [
        "#load gensim formatted Full Word2vec embeddings\n",
        "#!gdown https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "#!gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "  \n",
        "import gensim\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = ''\n",
        "\n",
        "word2vec = KeyedVectors.load_word2vec_format(resourceFile + 'GoogleNews-vectors-negative300.bin', binary=True)\n",
        "print('The word2vec embedding has been loaded!')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The word2vec embedding has been loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FCiZfu13JPHl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fasttext"
      ]
    },
    {
      "metadata": {
        "id": "A9zwQ2T_JRBo",
        "colab_type": "code",
        "outputId": "7e57c061-170b-4998-c708-262e029b09f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#!gdown https://drive.google.com/uc?id=1Zl6a75Ybf8do9uupmrJWKQMnvqqme4fh\n",
        "\n",
        "import gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors  \n",
        "\n",
        "resourceFile = ''\n",
        "fasttext = KeyedVectors.load_word2vec_format(resourceFile + 'fasttext.bin', binary=True)\n",
        "print('The fasttext embedding has been loaded!')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The fasttext embedding has been loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7hL2zgX6JRhG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Elmo"
      ]
    },
    {
      "metadata": {
        "id": "cXvK1wuvJTVP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!gdown https://drive.google.com/uc?id=17TK2h3cz7amgm2mCY4QCYy1yh23ZFWDU\n",
        "  \n",
        "import pickle\n",
        "data = pickle.load(open(\"elmo_embeddings_emma_brown.pkl\", \"rb\"))\n",
        "\n",
        "def pick_embeddings(corpus, sent_embs, word_list):\n",
        "    X = []\n",
        "    labels = {}\n",
        "    sents = []\n",
        "    ind = 0\n",
        "    for i, s in enumerate(corpus):\n",
        "        for j, w in enumerate(s):\n",
        "            if w.lower() in word_list:\n",
        "                X.append(sent_embs[i][j])\n",
        "                if w.lower() in labels:\n",
        "                  labels[w.lower()].append(ind)\n",
        "                else:\n",
        "                  labels[w.lower()] = [ind]\n",
        "                sents.append(s)\n",
        "                ind = ind + 1\n",
        "    return (X, labels, sents)\n",
        "  \n",
        "def get_word_list(path):\n",
        "    word_list = []\n",
        "    with open(path, \"r+\") as f_in:\n",
        "      for line in f_in:\n",
        "        word = line.split(' ')[0]\n",
        "        word_list.append(word.lower())\n",
        "\n",
        "    return word_list\n",
        "\n",
        "def load_subspace_vectors_contextual(all_mat, all_index, subspace_list):\n",
        "    subspace_mat = []\n",
        "    for w in subspace_list:\n",
        "      if w.lower() in all_index:\n",
        "        for i in all_index[w.lower()]:\n",
        "          #print(type(i))\n",
        "          subspace_mat.append(all_mat[i])\n",
        "    #subspace_mat = [all_mat[i,:] for i in all_index[w.lower()] for w in subspace_list if w.lower() in all_index]\n",
        "    print(\"Subspace: \", np.array(subspace_mat).shape)\n",
        "    return subspace_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SZy2oUe0nlxz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "#nltk.download('brown')\n",
        "\n",
        "brown_corpus = brown.sents()\n",
        "elmo = data['brown_embs']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gTsLdzKKrnn6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8c3d822c-e907-4911-f237-9c99c807e288"
      },
      "cell_type": "code",
      "source": [
        "print(np.array(elmo_brown_mat).shape)\n",
        "print(len(list(elmo_brown_index.keys())))\n",
        "print(len(wiki_words))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(972811, 768)\n",
            "34391\n",
            "188033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CdeKwRyLTm8q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2708d4fc-aeea-4ae5-f2b5-9b56811ca4ad"
      },
      "cell_type": "code",
      "source": [
        "a = [[1,2,3], [4,5,6]]\n",
        "np.mean(a, axis = 0)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.5, 3.5, 4.5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "metadata": {
        "id": "_DRlLRadJT1A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bert"
      ]
    },
    {
      "metadata": {
        "id": "MYvJ3xGRJUpV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "b41bff1a-386a-4a25-de0d-cd1f1be8bfaf"
      },
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/ed/9c755d357d33bc1931e157f537721efb5b88d2c583fe593cc09603076cc3/nltk-3.4.zip (1.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.4MB 15.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /data1/jsedoc_tmp/venvs/Debias2/lib/python3.6/site-packages (from nltk) (1.12.0)\n",
            "Collecting singledispatch (from nltk)\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/10/369f50bcd4621b263927b0a1519987a04383d4a98fb10438042ad410cf88/singledispatch-3.4.0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: singledispatch, nltk\n",
            "  Running setup.py install for nltk ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
            "\u001b[?25hSuccessfully installed nltk-3.4 singledispatch-3.4.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L541j-yEJWa4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Post-process (CN) all embeddings using a particular subspace"
      ]
    },
    {
      "metadata": {
        "id": "GeoHVWtg8jkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1051
        },
        "outputId": "d1816901-3bc0-4933-ceff-6108f6d1db30"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from ConceptorDebias.conceptor_fxns import AND\n",
        "import pandas as pd\n",
        "\n",
        "resourceFile = ''\n",
        "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "all_embd = ['elmo']\n",
        "all_subspace = ['without_conceptor','gender_list_pronouns', 'gender_list_extended','gender_list_propernouns', 'gender_list_all', 'gender_list_and']\n",
        "# all_subspace = ['without_conceptor', 'race_list']\n",
        "\n",
        "math = WEATLists.W_7_Math\n",
        "arts = WEATLists.W_7_Arts\n",
        "male = WEATLists.W_7_Male_terms\n",
        "female = WEATLists.W_7_Female_terms\n",
        "\n",
        "# white = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "# black = WEATLists.W_5_Unused_full_list_African_American_names\n",
        "# pleasant = WEATLists.W_5_Pleasant\n",
        "# unpleasant = WEATLists.W_5_Unpleasant\n",
        "\n",
        "#print(career, family, male, female)\n",
        "results = []\n",
        "\n",
        "for embd in all_embd:\n",
        "  #Initialize the embeddings to be used\n",
        "  curr_embd = eval(embd)\n",
        "  \n",
        "  #Load all embeddings in a matrix of all words in the wordlist\n",
        "  if embd == 'elmo' or embd == 'bert':\n",
        "    wiki_words = get_word_list('SIF/auxiliary_data/enwiki_vocab_min200.txt')\n",
        "    #all_words_mat, all_words_index, _ = pick_embeddings(brown_corpus, curr_embd, wiki_words)\n",
        "    print(\"All mat: \", np.array(all_words_mat).shape)\n",
        "  else:\n",
        "    all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)\n",
        "  \n",
        "  for subspace in all_subspace:\n",
        "    \n",
        "    if subspace != 'without_conceptor' and subspace != 'gender_list_and':\n",
        "      subspace_words_list = eval(subspace)\n",
        "\n",
        "    \n",
        "    if subspace != 'without_conceptor':\n",
        "      #CN all word embeddings using the respective subspace\n",
        "      if subspace == 'gender_list_and':\n",
        "        if embd == 'elmo' or embd == 'bert':\n",
        "          subspace_words_mat1 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_pronouns)\n",
        "          cn1 = process_cn_matrix(np.array(subspace_words_mat1).T)\n",
        "\n",
        "          subspace_words_mat2 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_extended)\n",
        "          cn2 = process_cn_matrix(np.array(subspace_words_mat2).T)\n",
        "\n",
        "          subspace_words_mat3 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_propernouns)\n",
        "          cn3 = process_cn_matrix(np.array(subspace_words_mat3).T)\n",
        "\n",
        "          cn = AND(cn1, AND(cn2, cn3))\n",
        "          all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "          print(\"All mat CN: \", np.array(all_words_cn).shape)\n",
        "        else:\n",
        "          subspace_words_mat1 = load_subspace_vectors(curr_embd, gender_list_pronouns)\n",
        "          cn1 = process_cn_matrix(np.array(subspace_words_mat1).T)\n",
        "\n",
        "          subspace_words_mat2 = load_subspace_vectors(curr_embd, gender_list_extended)\n",
        "          cn2 = process_cn_matrix(np.array(subspace_words_mat2).T)\n",
        "\n",
        "          subspace_words_mat3 = load_subspace_vectors(curr_embd, gender_list_propernouns)\n",
        "          cn3 = process_cn_matrix(np.array(subspace_words_mat3).T)\n",
        "\n",
        "          cn = AND(cn1, AND(cn2, cn3))\n",
        "          all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "      else:\n",
        "        #Load all embeddings of the subspace as a matrix\n",
        "        if embd == 'elmo' or embd == 'bert':\n",
        "          subspace_words_mat = load_subspace_vectors_contextual(all_words_mat, all_words_index, subspace_words_list)\n",
        "          cn = process_cn_matrix(np.array(subspace_words_mat).T)\n",
        "          all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "          print(\"Subspace mat: \", np.array(subspace_words_mat).shape)\n",
        "        else:\n",
        "          subspace_words_mat = load_subspace_vectors(curr_embd, subspace_words_list)\n",
        "          cn = process_cn_matrix(np.array(subspace_words_mat).T)\n",
        "          all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "    else:\n",
        "      all_words_cn = all_words_mat\n",
        "    all_words_cn = np.array(all_words_cn)\n",
        "    print(\"All CN: \", all_words_cn.shape)\n",
        "    #Store all conceptored words in a dictonary\n",
        "    all_words = {}\n",
        "    for word, index in all_words_index.items():\n",
        "      #print(word, index)\n",
        "      if embd == 'elmo' or embd == 'bert':\n",
        "        all_words[word] = np.mean([all_words_cn[i,:] for i in index], axis = 0)\n",
        "      else:\n",
        "        all_words[word] = all_words_cn[index,:]\n",
        "    print(\"LAST: \", np.array(all_words[\"a\"]).shape)\n",
        "    if subspace == 'without_conceptor':\n",
        "      #WITHOUT CONCEPTOR\n",
        "      d_cn = weat_effect_size(math, arts, male, female, all_words)\n",
        "      p_cn = weat_p_value(math, arts, male, female, all_words, 1000)\n",
        "      print(\"Without conceptor\")\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    else:\n",
        "      #WITH CONCEPTOR\n",
        "      d_cn = weat_effect_size(math, arts, male, female, all_words)\n",
        "      p_cn = weat_p_value(math, arts, male, female, all_words, 1000)\n",
        "      print(\"With conceptor: \", embd, subspace)\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    \n",
        "    row = [embd, subspace, d_cn, p_cn]\n",
        "    results.append(row)\n",
        "    \n",
        "pd.DataFrame(np.array(results), columns = ['Embedding', 'Subspace', 'Effect Size', 'p-value'])"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All mat:  (972811, 768)\n",
            "All CN:  (972811, 768)\n",
            "LAST:  (768,)\n",
            "Without conceptor\n",
            "WEAT d =  0.9415434\n",
            "WEAT p =  0.015\n",
            "Subspace:  (27902, 768)\n",
            "(972811, 768)\n",
            "Subspace mat:  (27902, 768)\n",
            "All CN:  (972811, 768)\n",
            "LAST:  (768,)\n",
            "With conceptor:  elmo gender_list_pronouns\n",
            "WEAT d =  0.34302947657573135\n",
            "WEAT p =  0.174\n",
            "Subspace:  (33163, 768)\n",
            "(972811, 768)\n",
            "Subspace mat:  (33163, 768)\n",
            "All CN:  (972811, 768)\n",
            "LAST:  (768,)\n",
            "With conceptor:  elmo gender_list_extended\n",
            "WEAT d =  0.3413683848252179\n",
            "WEAT p =  0.219\n",
            "Subspace:  (27392, 768)\n",
            "(972811, 768)\n",
            "Subspace mat:  (27392, 768)\n",
            "All CN:  (972811, 768)\n",
            "LAST:  (768,)\n",
            "With conceptor:  elmo gender_list_propernouns\n",
            "WEAT d =  1.1114971447774085\n",
            "WEAT p =  0.025\n",
            "Subspace:  (60555, 768)\n",
            "(972811, 768)\n",
            "Subspace mat:  (60555, 768)\n",
            "All CN:  (972811, 768)\n",
            "LAST:  (768,)\n",
            "With conceptor:  elmo gender_list_all\n",
            "WEAT d =  1.4051261712409355\n",
            "WEAT p =  0.0\n",
            "Subspace:  (27902, 768)\n",
            "Subspace:  (33163, 768)\n",
            "Subspace:  (27392, 768)\n",
            "(972811, 768)\n",
            "All mat CN:  (972811, 768)\n",
            "All CN:  (972811, 768)\n",
            "LAST:  (768,)\n",
            "With conceptor:  elmo gender_list_and\n",
            "WEAT d =  1.3059196442097278\n",
            "WEAT p =  0.008\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Embedding                 Subspace          Effect Size p-value\n",
              "0      elmo        without_conceptor            0.9415434   0.015\n",
              "1      elmo     gender_list_pronouns  0.34302947657573135   0.174\n",
              "2      elmo     gender_list_extended   0.3413683848252179   0.219\n",
              "3      elmo  gender_list_propernouns   1.1114971447774085   0.025\n",
              "4      elmo          gender_list_all   1.4051261712409355     0.0\n",
              "5      elmo          gender_list_and   1.3059196442097278   0.008"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Embedding</th>\n",
              "      <th>Subspace</th>\n",
              "      <th>Effect Size</th>\n",
              "      <th>p-value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>elmo</td>\n",
              "      <td>without_conceptor</td>\n",
              "      <td>0.9415434</td>\n",
              "      <td>0.015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>elmo</td>\n",
              "      <td>gender_list_pronouns</td>\n",
              "      <td>0.34302947657573135</td>\n",
              "      <td>0.174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>elmo</td>\n",
              "      <td>gender_list_extended</td>\n",
              "      <td>0.3413683848252179</td>\n",
              "      <td>0.219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>elmo</td>\n",
              "      <td>gender_list_propernouns</td>\n",
              "      <td>1.1114971447774085</td>\n",
              "      <td>0.025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>elmo</td>\n",
              "      <td>gender_list_all</td>\n",
              "      <td>1.4051261712409355</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>elmo</td>\n",
              "      <td>gender_list_and</td>\n",
              "      <td>1.3059196442097278</td>\n",
              "      <td>0.008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "metadata": {
        "id": "YZNxmDKB8fbB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73921ec9-be30-4682-8b2b-f27414a277b7"
      },
      "cell_type": "code",
      "source": [
        "if w.lower() not in all_words_index:\n",
        "  print(\"T\")\n",
        "#load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_pronouns)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cVmZBwykJmgn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Calculate WEAT scores"
      ]
    },
    {
      "metadata": {
        "id": "PPukMLU2ZosX",
        "colab_type": "code",
        "outputId": "9e6d2b3a-05f8-407a-d65b-b6d6db1edc2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "science = WEATLists.W_8_Science\n",
        "arts = WEATLists.W_8_Arts\n",
        "male = WEATLists.W_8_Male_terms\n",
        "female = WEATLists.W_8_Female_terms\n",
        "\n",
        "#WITHOUT CONCEPTOR\n",
        "print(\"Without conceptor\")\n",
        "print('WEAT d = ', weat_effect_size(science, arts, male, female, curr_embd))\n",
        "print('WEAT p = ', weat_p_value(science, arts, male, female, curr_embd, 1000))\n",
        "\n",
        "#WITH CONCEPTOR\n",
        "print(\"With conceptor\")\n",
        "print('WEAT d = ', weat_effect_size(science, arts, male, female, all_words))\n",
        "print('WEAT p = ', weat_p_value(science, arts, male, female, all_words, 1000))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Without conceptor\n",
            "WEAT d =  1.2780356\n",
            "Unpertrubed:  0.3456038013100624\n",
            "WEAT p =  0.003\n",
            "With conceptor\n",
            "WEAT d =  0.681611936758009\n",
            "Unpertrubed:  0.10700200023922478\n",
            "WEAT p =  0.103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yAdi5tlZGRzL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## WEAT algorithm from GITHUB gist\n",
        "REF: https://gist.github.com/SandyRogers/e5c2e938502a75dcae25216e4fae2da5"
      ]
    },
    {
      "metadata": {
        "id": "2YM0FV9SFrWC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class WEATTest(object):\n",
        "    \"\"\"\n",
        "    Perform WEAT (Word Embedding Association Test) bias tests on a language model.\n",
        "    Follows from Caliskan et al 2017 (10.1126/science.aal4230).\n",
        "    \"\"\"\n",
        "    \n",
        "    instruments = [\"bagpipe\", \"cello\", \"guitar\", \"lute\", \"trombone\", \"banjo\", \"clarinet\", \"harmonica\", \"mandolin\", \"trumpet\", \"bassoon\", \"drum\", \"harp\", \"oboe\", \"tuba\", \"bell\", \"fiddle\", \"harpsichord\", \"piano\", \"viola\", \"bongo\",\n",
        "\"flute\", \"horn\", \"saxophone\", \"violin\"]\n",
        "    weapons = [\"arrow\", \"club\", \"gun\", \"missile\", \"spear\", \"axe\", \"dagger\", \"harpoon\", \"pistol\", \"sword\", \"blade\", \"dynamite\", \"hatchet\", \"rifle\", \"tank\", \"bomb\", \"firearm\", \"knife\", \"shotgun\", \"teargas\", \"cannon\", \"grenade\",\n",
        "    \"mace\", \"slingshot\", \"whip\"]\n",
        "    flowers = [\"aster\", \"clover\", \"hyacinth\", \"marigold\", \"poppy\", \"azalea\", \"crocus\", \"iris\", \"orchid\", \"rose\", \"blue-bell\", \"daffodil\", \"lilac\", \"pansy\", \"tulip\", \"buttercup\", \"daisy\", \"lily\", \"peony\", \"violet\", \"carnation\", \"gladiola\", \"magnolia\", \"petunia\", \"zinnia\"]\n",
        "    insects = [\"ant\", \"caterpillar\", \"flea\", \"locust\", \"spider\", \"bedbug\", \"centipede\", \"fly\", \"maggot\", \"tarantula\", \"bee\", \"cockroach\", \"gnat\", \"mosquito\", \"termite\", \"beetle\", \"cricket\", \"hornet\", \"moth\", \"wasp\", \"blackfly\",\n",
        "    \"dragonfly\", \"horsefly\", \"roach\", \"weevil\"]\n",
        "    pleasant = [\"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\",\n",
        "    \"happy\", \"laughter\", \"paradise\", \"vacation\"]\n",
        "    unpleasant = [\"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\",\n",
        "    \"vomit\", \"agony\", \"prison\"]\n",
        "    math = [\"math\", \"algebra\", \"geometry\", \"calculus\", \"equations\", \"computation\", \"numbers\", \"addition\"]\n",
        "    arts = [\"poetry\", \"art\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\", \"sculpture\"]\n",
        "    male = [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"his\", \"son\"]\n",
        "    female = [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \"hers\", \"daughter\"]\n",
        "    science = [\"science\", \"technology\", \"physics\", \"chemistry\", \"Einstein\", \"NASA\", \"experiment\", \"astronomy\"]\n",
        "    arts_2 = [\"poetry\", \"art\", \"Shakespeare\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\"]\n",
        "    male_2 = [\"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\"]\n",
        "    female_2 = [\"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"he\"]\n",
        "    mental_disease = [\"sad\", \"hopeless\", \"gloomy\", \"tearful\", \"miserable\", \"depressed\"]\n",
        "    physical_disease = [\"sick\", \"illness\", \"influenza\", \"disease\", \"virus\", \"cancer\"]\n",
        "    temporary = [\"impermanent\", \"unstable\", \"variable\", \"fleeting\", \"short-term\", \"brief\", \"occasional\"]\n",
        "    permanent = [\"stable\", \"always\", \"constant\", \"persistent\", \"chronic\", \"prolonged\", \"forever\"]\n",
        "    \n",
        "    def __init__(self, model):\n",
        "        \"\"\"Setup a Word Embedding Association Test for a given spaCy language model.\n",
        "        \n",
        "        EXAMPLE:\n",
        "            >>> nlp = spacy.load('en_core_web_md')\n",
        "            >>> test = WEATTest(nlp)\n",
        "            >>> test.run_test(WEATTest.instruments, WEATTest.weapon, WEATTest.pleasant, WEATTest.unpleasant)\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "\n",
        "    @staticmethod\n",
        "    def word_association_with_attribute(self, w, A, B):\n",
        "        return np.mean([cosine_similarity(np.array(w).reshape(1,-1),np.array(a).reshape(1,-1)) for a in A]) - np.mean([cosine_similarity(np.array(w).reshape(1,-1),np.array(b).reshape(1,-1)) for b in B])\n",
        "\n",
        "    @staticmethod\n",
        "    def differential_assoication(self, X, Y, A, B):\n",
        "        return np.sum([self.word_association_with_attribute(self, x, A, B) for x in X]) - np.sum([self.word_association_with_attribute(self, y, A, B) for y in Y])\n",
        "\n",
        "    @staticmethod\n",
        "    def weat_effect_size(self, X, Y, A, B):\n",
        "        return (\n",
        "            np.mean([self.word_association_with_attribute(self, x, A, B) for x in X]) -\n",
        "            np.mean([self.word_association_with_attribute(self, y, A, B) for y in Y])\n",
        "        ) / np.std([self.word_association_with_attribute(self, w, A, B) for w in X + Y])\n",
        "\n",
        "    @staticmethod\n",
        "    def random_permutation(self, iterable, r=None):\n",
        "        pool = tuple(iterable)\n",
        "        r = len(pool) if r is None else r\n",
        "        return tuple(random.sample(pool, r))\n",
        "\n",
        "    @staticmethod\n",
        "    def weat_p_value(self, X, Y, A, B, sample):\n",
        "        size_of_permutation = min(len(X), len(Y))\n",
        "        X_Y = X + Y\n",
        "        observed_test_stats_over_permutations = []\n",
        "\n",
        "        if not sample:\n",
        "            permutations = combinations(X_Y, size_of_permutation)\n",
        "        else:\n",
        "            permutations = [self.random_permutation(self, X_Y, size_of_permutation) for s in range(sample)]\n",
        "        print(np.array(X_Y).shape)\n",
        "        for Xi in permutations:\n",
        "            Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
        "            observed_test_stats_over_permutations.append(self.differential_assoication(self, Xi, Yi, A, B))\n",
        "\n",
        "        unperturbed = self.differential_assoication(self, X, Y, A, B)\n",
        "        is_over = np.array([o > unperturbed for o in observed_test_stats_over_permutations])\n",
        "        return is_over.sum() / is_over.size\n",
        "\n",
        "    @staticmethod\n",
        "    def weat_stats(X, Y, A, B, self, sample_p=None):\n",
        "        test_statistic = self.differential_assoication(self, X, Y, A, B)\n",
        "        effect_size = self.weat_effect_size(self, X, Y, A, B)\n",
        "        p = self.weat_p_value(self, X, Y, A, B, sample=sample_p)\n",
        "        return test_statistic, effect_size, p\n",
        "\n",
        "    def run_test(self, target_1, target_2, attributes_1, attributes_2, sample_p=None):\n",
        "        \"\"\"Run the WEAT test for differential association between two \n",
        "        sets of target words and two seats of attributes.\n",
        "        \n",
        "        EXAMPLE:\n",
        "            >>> test.run_test(WEATTest.instruments, WEATTest.weapon, WEATTest.pleasant, WEATTest.unpleasant)\n",
        "            >>> test.run_test(a, b, c, d, sample_p=1000) # use 1000 permutations for p-value calculation\n",
        "            >>> test.run_test(a, b, c, d, sample_p=None) # use all possible permutations for p-value calculation\n",
        "            \n",
        "        RETURNS:\n",
        "            (d, e, p). A tuple of floats, where d is the WEAT Test statistic, \n",
        "            e is the effect size, and p is the one-sided p-value measuring the\n",
        "            (un)likeliness of the null hypothesis (which is that there is no\n",
        "            difference in association between the two target word sets and\n",
        "            the attributes).\n",
        "            \n",
        "            If e is large and p small, then differences in the model between \n",
        "            the attribute word sets match differences between the targets.\n",
        "        \"\"\"\n",
        "        X = [list(self.model[w]) for w in target_1]\n",
        "        Y = [list(self.model[w]) for w in target_2]\n",
        "        A = [list(self.model[w]) for w in attributes_1]\n",
        "        B = [list(self.model[w]) for w in attributes_2]\n",
        "        print(X)\n",
        "        return self.weat_stats(X, Y, A, B, self, sample_p)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2I9dGN5fGQw-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Code test"
      ]
    },
    {
      "metadata": {
        "id": "agNgns64tdBP",
        "colab_type": "code",
        "outputId": "05d3aae1-cb90-4a23-f675-757360296044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "#nlp = spacy.load('glove')\n",
        "test = WEATTest(glove)\n",
        "test.run_test(WEATTest.instruments, WEATTest.weapons, WEATTest.pleasant, WEATTest.unpleasant, 1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The glove embedding has been loaded!\n",
            "X Shape:  (3, 300)\n",
            "Y Shape:  (3, 300)\n",
            "A Shape:  (2, 300)\n",
            "B Shape:  (2, 300)\n",
            "X U Y Shape:  (6, 300)\n",
            "WA shape:  (3, 2)\n",
            "sWAB shape:  (3,)\n",
            "WA shape:  (3, 2)\n",
            "sWAB shape:  (3,)\n",
            "WA shape:  (6, 2)\n",
            "sWAB shape:  (6,)\n",
            "WEAT d =  1.8613524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "agexIaaUujxx",
        "colab_type": "code",
        "outputId": "9c948f30-4aed-4894-e098-40d9cbd2173a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "all_subspace = ['gender_list_pronouns','gender_list_extended','gender_list_propernouns']\n",
        "op = []\n",
        "for s in all_subspace:\n",
        "  aa = s\n",
        "  print(aa)\n",
        "  a = 10.98\n",
        "  b = 99.09\n",
        "  temp = [s, a, b]\n",
        "  op.append(temp)\n",
        "print(op)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gender_list_pronouns\n",
            "gender_list_extended\n",
            "gender_list_propernouns\n",
            "[['gender_list_pronouns', 10.98, 99.09], ['gender_list_extended', 10.98, 99.09], ['gender_list_propernouns', 10.98, 99.09]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cWeJf7sDkSCc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}