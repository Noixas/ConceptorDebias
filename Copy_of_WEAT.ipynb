{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of WEAT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsedoc/ConceptorDebias/blob/master/Copy_of_WEAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "l8Me4_OyFyKo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# WEAT Algorithm\n",
        "## Test Statistic"
      ]
    },
    {
      "metadata": {
        "id": "mf6_liysF8en",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# returns s(w, A, B) for all w in W (passed as argument). Shape: n_words (in W) x 1\n",
        "def swAB(W, A, B):\n",
        "  #Calculate cosine-similarity between W and A, W and B\n",
        "  WA = cosine_similarity(W,A)\n",
        "  WB = cosine_similarity(W,B)\n",
        "  #print('WA shape: ', WA.shape)\n",
        "  #Take mean along columns\n",
        "  WAmean = np.mean(WA, axis = 1)\n",
        "  WBmean = np.mean(WB, axis = 1)\n",
        "  \n",
        "  #print('sWAB shape: ', WAmean.shape)\n",
        "  \n",
        "  return (WAmean - WBmean)\n",
        "  \n",
        "def test_statistic(X, Y, A, B):\n",
        "  return (sum(swAB(X, A, B)) - sum(swAB(Y, A, B)))\n",
        "\n",
        "def weat_effect_size(X, Y, A, B, embd):\n",
        "  #Convert the set of words to matrix\n",
        "  Xmat = np.array([embd[w] for w in X if w in embd])\n",
        "  Ymat = np.array([embd[w] for w in Y if w in embd])\n",
        "  Amat = np.array([embd[w] for w in A if w in embd])\n",
        "  Bmat = np.array([embd[w] for w in B if w in embd])\n",
        "  \n",
        "  # Find X U Y\n",
        "  XuY = list(set(X).union(Y))\n",
        "  XuYmat = []\n",
        "  for w in XuY:\n",
        "    if w in embd:\n",
        "      XuYmat.append(embd[w])\n",
        "  XuYmat = np.array(XuYmat)\n",
        "  #print('X U Y Shape: ', XuYmat.shape)\n",
        "  \n",
        "  d = (np.mean(swAB(Xmat,Amat,Bmat)) - np.mean(swAB(Ymat,Amat,Bmat)))/np.std(swAB(XuYmat, Amat, Bmat))\n",
        "  \n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d8foGwVSGI16",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## P-Value"
      ]
    },
    {
      "metadata": {
        "id": "ZDy-duFOFj71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "from itertools import combinations, filterfalse\n",
        "\n",
        "def random_permutation(iterable, r=None):\n",
        "  pool = tuple(iterable)\n",
        "  r = len(pool) if r is None else r\n",
        "  return tuple(random.sample(pool, r))\n",
        "\n",
        "def weat_p_value(X, Y, A, B, embd, sample):\n",
        "  size_of_permutation = min(len(X), len(Y))\n",
        "  X_Y = X + Y\n",
        "  test_stats_over_permutation = []\n",
        "  \n",
        "  Xmat = np.array([embd[w] for w in X if w in embd])\n",
        "  Ymat = np.array([embd[w] for w in Y if w in embd])\n",
        "  Amat = np.array([embd[w] for w in A if w in embd])\n",
        "  Bmat = np.array([embd[w] for w in B if w in embd])\n",
        "  \n",
        "  if not sample:\n",
        "      permutations = combinations(X_Y, size_of_permutation)\n",
        "  else:\n",
        "      permutations = [random_permutation(X_Y, size_of_permutation) for s in range(sample)]\n",
        "      \n",
        "  for Xi in permutations:\n",
        "    Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
        "    Ximat = np.array([embd[w] for w in Xi if w in embd])\n",
        "    Yimat = np.array([embd[w] for w in Yi if w in embd])\n",
        "    test_stats_over_permutation.append(test_statistic(Ximat, Yimat, Amat, Bmat))\n",
        "    \n",
        "  unperturbed = test_statistic(Xmat, Ymat, Amat, Bmat)\n",
        "  \n",
        "  is_over = np.array([o > unperturbed for o in test_stats_over_permutation])\n",
        "  #print(\"All: \", test_stats_over_permutation)\n",
        "  print(\"Unpertrubed: \", unperturbed)\n",
        "  return is_over.sum() / is_over.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NbRMmhwbGL98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test on sample input"
      ]
    },
    {
      "metadata": {
        "id": "upscdvtwFm0l",
        "colab_type": "code",
        "outputId": "c9706781-f440-4e9f-86a0-4e3a0ca7ef9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "X = [\"bagpipe\", \"cello\", \"guitar\", \"lute\", \"trombone\", \"banjo\", \"clarinet\", \"harmonica\", \"mandolin\", \"trumpet\", \"bassoon\", \"drum\", \"harp\", \"oboe\", \"tuba\", \"bell\", \"fiddle\", \"harpsichord\", \"piano\", \"viola\", \"bongo\",\n",
        "\"flute\", \"horn\", \"saxophone\", \"violin\"] #Instruments\n",
        "Y = [\"arrow\", \"club\", \"gun\", \"missile\", \"spear\", \"axe\", \"dagger\", \"harpoon\", \"pistol\", \"sword\", \"blade\", \"dynamite\", \"hatchet\", \"rifle\", \"tank\", \"bomb\", \"firearm\", \"knife\", \"shotgun\", \"teargas\", \"cannon\", \"grenade\",\n",
        "    \"mace\", \"slingshot\", \"whip\"] #Weapons\n",
        "A = [\"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\",\n",
        "    \"happy\", \"laughter\", \"paradise\", \"vacation\"] #Pleasant\n",
        "B = [\"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\",\n",
        "    \"vomit\", \"agony\", \"prison\"] #Unpleasant\n",
        "\n",
        "#Load word embeddings\n",
        "#load gensim formatted Full Glove embeddings\n",
        "#!gdown https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = '/content/'\n",
        "\n",
        "\n",
        "glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
        "print('The glove embedding has been loaded!')\n",
        "\n",
        "print('WEAT d = ', weat_effect_size(X, Y, A, B, glove))\n",
        "print('WEAT p = ', weat_p_value(X, Y, A, B, glove, 1000))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The glove embedding has been loaded!\n",
            "WEAT d =  1.5495627\n",
            "Unpertrubed:  2.2905553244054317\n",
            "WEAT p =  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rfwYNFl2ckGy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## WEAT with conceptor debiased embeddings"
      ]
    },
    {
      "metadata": {
        "id": "xEuq-Lt5ctV2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compute the conceptor matrix for all words and gender specific words."
      ]
    },
    {
      "metadata": {
        "id": "HAG4oqwIc1Z3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Compute the conceptor matrix\n",
        "def post_process_cn_matrix(x, subspace, alpha = 2):\n",
        "  print(\"starting...\")\n",
        "  #x = orig_embd.vectors\n",
        "  print(subspace.shape)\n",
        "  \n",
        "  #Calculate the correlation matrix\n",
        "  R = subspace.dot(subspace.T)/(subspace.shape[1])\n",
        "  print(\"R calculated\")\n",
        "  \n",
        "  #Calculate the conceptor matrix\n",
        "  C = R @ (np.linalg.inv(R + alpha ** (-2) * np.eye(subspace.shape[0])))\n",
        "  print(\"C calculated\")\n",
        "  \n",
        "  #Calculate the negation of the conceptor matrix\n",
        "  negC = np.eye(subspace.shape[0]) - C\n",
        "  print(\"negC calculated\")\n",
        "  \n",
        "  #Post-process the vocab matrix\n",
        "  newX = (negC @ x).T\n",
        "  print(newX.shape)\n",
        "  return newX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7FusU4Guyyc1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load all vectors from **glove**"
      ]
    },
    {
      "metadata": {
        "id": "-bNIpTH7y4e2",
        "colab_type": "code",
        "outputId": "e561ec0b-c1f7-45e6-eb71-8912b2c93aec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Load word embeddings\n",
        "#download gensim formatted Full Glove embeddings\n",
        "#!gdown https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = '/content/'\n",
        "\n",
        "glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
        "print('The glove embedding has been loaded!')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The glove embedding has been loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r6ZSpFtFy_bQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load all vectors from **Word2Vec**"
      ]
    },
    {
      "metadata": {
        "id": "7UbNQ5TszI7O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load gensim formatted Full Word2vec embeddings\n",
        "!gdown https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "#!gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "  \n",
        "import gensim\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = '/content/'\n",
        "\n",
        "word2vec = KeyedVectors.load_word2vec_format(resourceFile + 'GoogleNews-vectors-negative300.bin', binary=True)\n",
        "print('The word2vec embedding has been loaded!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GHaXxx5L3Ugb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load embeddings of all words from the ref. wordlist from a specific embedding"
      ]
    },
    {
      "metadata": {
        "id": "8WBI1VED3bpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Arguments - embd: The word embeddings in the form of a dict || wikiWordsPath: List of words to be considered\n",
        "def load_all_vectors(embd, wikiWordsPath):\n",
        "  all_words_index = {}\n",
        "  all_words_mat = []\n",
        "  with open(wikiWordsPath, \"r+\") as f_in:\n",
        "    ind = 0\n",
        "    for line in f_in:\n",
        "      word = line.split(' ')[0]\n",
        "      if word in embd:\n",
        "        all_words_index[word] = ind\n",
        "        all_words_mat.append(embd[word])\n",
        "        ind = ind+1\n",
        "        \n",
        "  return all_words_index, all_words_mat\n",
        "\n",
        "def load_subspace_vectors(embd, subspace_words):\n",
        "  subspace_embd_mat = []\n",
        "  ind = 0\n",
        "  for word in subspace_words:\n",
        "    if word in embd:\n",
        "      subspace_embd_mat.append(embd[word])\n",
        "      ind = ind+1\n",
        "      \n",
        "  return subspace_embd_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vjtc01gf8aR4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Conceptor all words and store it in a dictonary"
      ]
    },
    {
      "metadata": {
        "id": "EzfTYgq0I9WW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load all word lists - Subspace"
      ]
    },
    {
      "metadata": {
        "id": "D4kndsi_DwNr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1023
        },
        "outputId": "e8e5a962-fe23-46e2-e329-17f1735f5b49"
      },
      "cell_type": "code",
      "source": [
        "# General word list\n",
        "!wget https://raw.githubusercontent.com/IlyaSemenov/wikipedia-word-frequency/master/results/enwiki-20150602-words-frequency.txt\n",
        "!git clone https://github.com/PrincetonML/SIF\n",
        "    \n",
        "# Gender word lists\n",
        "!git clone https://github.com/uclanlp/gn_glove\n",
        "!git clone https://github.com/uclanlp/corefBias\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/female.txt\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/male.txt\n",
        "    \n",
        "# our code for debiasing -- also includes word lists    \n",
        "!git clone https://github.com/jsedoc/ConceptorDebias"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-27 18:21:17--  https://raw.githubusercontent.com/IlyaSemenov/wikipedia-word-frequency/master/results/enwiki-20150602-words-frequency.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23741395 (23M) [text/plain]\n",
            "Saving to: ‘enwiki-20150602-words-frequency.txt’\n",
            "\n",
            "enwiki-20150602-wor 100%[===================>]  22.64M   113MB/s    in 0.2s    \n",
            "\n",
            "2019-02-27 18:21:19 (113 MB/s) - ‘enwiki-20150602-words-frequency.txt’ saved [23741395/23741395]\n",
            "\n",
            "Cloning into 'SIF'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (128/128), 2.80 MiB | 2.21 MiB/s, done.\n",
            "Resolving deltas: 100% (55/55), done.\n",
            "Cloning into 'gn_glove'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 114 (delta 10), reused 5 (delta 1), pack-reused 93\u001b[K\n",
            "Receiving objects: 100% (114/114), 50.97 KiB | 288.00 KiB/s, done.\n",
            "Resolving deltas: 100% (45/45), done.\n",
            "Cloning into 'corefBias'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 471 (delta 3), reused 0 (delta 0), pack-reused 457\u001b[K\n",
            "Receiving objects: 100% (471/471), 84.18 MiB | 12.24 MiB/s, done.\n",
            "Resolving deltas: 100% (273/273), done.\n",
            "--2019-02-27 18:21:37--  https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/female.txt\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35751 (35K) [text/plain]\n",
            "Saving to: ‘female.txt’\n",
            "\n",
            "female.txt          100%[===================>]  34.91K   149KB/s    in 0.2s    \n",
            "\n",
            "2019-02-27 18:21:39 (149 KB/s) - ‘female.txt’ saved [35751/35751]\n",
            "\n",
            "--2019-02-27 18:21:41--  https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/male.txt\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20466 (20K) [text/plain]\n",
            "Saving to: ‘male.txt’\n",
            "\n",
            "male.txt            100%[===================>]  19.99K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-02-27 18:21:42 (189 MB/s) - ‘male.txt’ saved [20466/20466]\n",
            "\n",
            "Cloning into 'ConceptorDebias'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (75/75), done.\u001b[K\n",
            "remote: Total 85 (delta 42), reused 22 (delta 9), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (85/85), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "orBvfpqCFOfa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from ConceptorDebias.load_word_lists import *\n",
        "\n",
        "gender_list_pronouns = WEATLists.W_7_Male_terms + WEATLists.W_7_Female_terms + WEATLists.W_8_Male_terms + WEATLists.W_8_Female_terms\n",
        "gender_list_pronouns = list(set(gender_list_pronouns))\n",
        "\n",
        "gender_list_extended = male_vino_extra + female_vino_extra + male_gnGlove + female_gnGlove\n",
        "gender_list_extended = list(set(gender_list_extended))\n",
        "\n",
        "gender_list_propernouns = male_cmu + female_cmu\n",
        "gender_list_propernouns = list(set(gender_list_propernouns))\n",
        "\n",
        "race_list = WEATLists.W_3_Unused_full_list_European_American_names + WEATLists.W_3_European_American_names + WEATLists.W_3_Unused_full_list_African_American_names + WEATLists.W_3_African_American_names + WEATLists.W_4_Unused_full_list_European_American_names + WEATLists.W_4_European_American_names + WEATLists.W_4_Unused_full_list_African_American_names + WEATLists.W_4_African_American_names + WEATLists.W_5_Unused_full_list_European_American_names + WEATLists.W_5_European_American_names + WEATLists.W_5_Unused_full_list_African_American_names + WEATLists.W_5_African_American_names \n",
        "race_list = list(set(race_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kEiBEVI2I2WR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load different embeddings"
      ]
    },
    {
      "metadata": {
        "id": "Nrlqu2xvJH_6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Glove"
      ]
    },
    {
      "metadata": {
        "id": "PFNSjmY8I1Ut",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Load word embeddings\n",
        "#download gensim formatted Full Glove embeddings\n",
        "#!gdown https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = '/content/'\n",
        "\n",
        "glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
        "print('The glove embedding has been loaded!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "84af1zflJKLQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Word2vec"
      ]
    },
    {
      "metadata": {
        "id": "OmJlU2IiJMMj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load gensim formatted Full Word2vec embeddings\n",
        "!gdown https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "#!gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "  \n",
        "import gensim\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = '/content/'\n",
        "\n",
        "word2vec = KeyedVectors.load_word2vec_format(resourceFile + 'GoogleNews-vectors-negative300.bin', binary=True)\n",
        "print('The word2vec embedding has been loaded!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FCiZfu13JPHl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fasttext"
      ]
    },
    {
      "metadata": {
        "id": "A9zwQ2T_JRBo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7hL2zgX6JRhG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Elmo"
      ]
    },
    {
      "metadata": {
        "id": "cXvK1wuvJTVP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_DRlLRadJT1A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bert"
      ]
    },
    {
      "metadata": {
        "id": "MYvJ3xGRJUpV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L541j-yEJWa4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Post-process (CN) all embeddings using a particular subspace"
      ]
    },
    {
      "metadata": {
        "id": "GeoHVWtg8jkn",
        "colab_type": "code",
        "outputId": "f781ac46-2c6e-4543-f35a-838af3912d19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "resourceFile = '/content/'\n",
        "wikiWordsPath = resourceFile + '/SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "#Initialize the embeddings to be used\n",
        "curr_embd = glove\n",
        "subspace_words_list = gender_list_pronouns\n",
        "\n",
        "#Load all embeddings in a matrix of all words in the wordlist\n",
        "all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)\n",
        "\n",
        "#Load all embeddings of the subspace as a matrix\n",
        "subspace_words_mat = load_subspace_vectors(curr_embd, subspace_words_list)\n",
        "\n",
        "#CN all word embeddings using the respective subspace\n",
        "all_words_cn = post_process_cn_matrix(np.array(all_words_mat).T, np.array(subspace_words_mat).T)\n",
        "\n",
        "#Store all conceptored words in a dictonary\n",
        "all_words = {}\n",
        "for word, index in all_words_index.items():\n",
        "  all_words[word] = all_words_cn[index,:]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting...\n",
            "(300, 22)\n",
            "R calculated\n",
            "C calculated\n",
            "negC calculated\n",
            "(128607, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cVmZBwykJmgn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Calculate WEAT scores"
      ]
    },
    {
      "metadata": {
        "id": "PPukMLU2ZosX",
        "colab_type": "code",
        "outputId": "6ea80045-37ed-4943-9552-d1e8cb0b1737",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "science = WEATLists.W_8_Science\n",
        "arts = WEATLists.W_8_Arts\n",
        "male = WEATLists.W_8_Male_terms\n",
        "female = WEATLists.W_8_Female_terms\n",
        "\n",
        "#WITHOUT CONCEPTOR\n",
        "print(\"Without conceptor\")\n",
        "print('WEAT d = ', weat_effect_size(science, arts, male, female, curr_embd))\n",
        "print('WEAT p = ', weat_p_value(science, arts, male, female, curr_embd, 1000))\n",
        "\n",
        "#WITH CONCEPTOR\n",
        "print(\"With conceptor\")\n",
        "print('WEAT d = ', weat_effect_size(science, arts, male, female, all_words))\n",
        "print('WEAT p = ', weat_p_value(science, arts, male, female, all_words, 1000))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Without conceptor\n",
            "WEAT d =  1.2780356\n",
            "Unpertrubed:  0.3456038013100624\n",
            "WEAT p =  0.008\n",
            "With conceptor\n",
            "WEAT d =  -0.3767537063989365\n",
            "Unpertrubed:  -0.025181248397179855\n",
            "WEAT p =  0.817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yAdi5tlZGRzL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## WEAT algorithm from GITHUB gist\n",
        "REF: https://gist.github.com/SandyRogers/e5c2e938502a75dcae25216e4fae2da5"
      ]
    },
    {
      "metadata": {
        "id": "2YM0FV9SFrWC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class WEATTest(object):\n",
        "    \"\"\"\n",
        "    Perform WEAT (Word Embedding Association Test) bias tests on a language model.\n",
        "    Follows from Caliskan et al 2017 (10.1126/science.aal4230).\n",
        "    \"\"\"\n",
        "    \n",
        "    instruments = [\"bagpipe\", \"cello\", \"guitar\", \"lute\", \"trombone\", \"banjo\", \"clarinet\", \"harmonica\", \"mandolin\", \"trumpet\", \"bassoon\", \"drum\", \"harp\", \"oboe\", \"tuba\", \"bell\", \"fiddle\", \"harpsichord\", \"piano\", \"viola\", \"bongo\",\n",
        "\"flute\", \"horn\", \"saxophone\", \"violin\"]\n",
        "    weapons = [\"arrow\", \"club\", \"gun\", \"missile\", \"spear\", \"axe\", \"dagger\", \"harpoon\", \"pistol\", \"sword\", \"blade\", \"dynamite\", \"hatchet\", \"rifle\", \"tank\", \"bomb\", \"firearm\", \"knife\", \"shotgun\", \"teargas\", \"cannon\", \"grenade\",\n",
        "    \"mace\", \"slingshot\", \"whip\"]\n",
        "    flowers = [\"aster\", \"clover\", \"hyacinth\", \"marigold\", \"poppy\", \"azalea\", \"crocus\", \"iris\", \"orchid\", \"rose\", \"blue-bell\", \"daffodil\", \"lilac\", \"pansy\", \"tulip\", \"buttercup\", \"daisy\", \"lily\", \"peony\", \"violet\", \"carnation\", \"gladiola\", \"magnolia\", \"petunia\", \"zinnia\"]\n",
        "    insects = [\"ant\", \"caterpillar\", \"flea\", \"locust\", \"spider\", \"bedbug\", \"centipede\", \"fly\", \"maggot\", \"tarantula\", \"bee\", \"cockroach\", \"gnat\", \"mosquito\", \"termite\", \"beetle\", \"cricket\", \"hornet\", \"moth\", \"wasp\", \"blackfly\",\n",
        "    \"dragonfly\", \"horsefly\", \"roach\", \"weevil\"]\n",
        "    pleasant = [\"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\",\n",
        "    \"happy\", \"laughter\", \"paradise\", \"vacation\"]\n",
        "    unpleasant = [\"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\",\n",
        "    \"vomit\", \"agony\", \"prison\"]\n",
        "    math = [\"math\", \"algebra\", \"geometry\", \"calculus\", \"equations\", \"computation\", \"numbers\", \"addition\"]\n",
        "    arts = [\"poetry\", \"art\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\", \"sculpture\"]\n",
        "    male = [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"his\", \"son\"]\n",
        "    female = [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \"hers\", \"daughter\"]\n",
        "    science = [\"science\", \"technology\", \"physics\", \"chemistry\", \"Einstein\", \"NASA\", \"experiment\", \"astronomy\"]\n",
        "    arts_2 = [\"poetry\", \"art\", \"Shakespeare\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\"]\n",
        "    male_2 = [\"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\"]\n",
        "    female_2 = [\"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"he\"]\n",
        "    mental_disease = [\"sad\", \"hopeless\", \"gloomy\", \"tearful\", \"miserable\", \"depressed\"]\n",
        "    physical_disease = [\"sick\", \"illness\", \"influenza\", \"disease\", \"virus\", \"cancer\"]\n",
        "    temporary = [\"impermanent\", \"unstable\", \"variable\", \"fleeting\", \"short-term\", \"brief\", \"occasional\"]\n",
        "    permanent = [\"stable\", \"always\", \"constant\", \"persistent\", \"chronic\", \"prolonged\", \"forever\"]\n",
        "    \n",
        "    def __init__(self, model):\n",
        "        \"\"\"Setup a Word Embedding Association Test for a given spaCy language model.\n",
        "        \n",
        "        EXAMPLE:\n",
        "            >>> nlp = spacy.load('en_core_web_md')\n",
        "            >>> test = WEATTest(nlp)\n",
        "            >>> test.run_test(WEATTest.instruments, WEATTest.weapon, WEATTest.pleasant, WEATTest.unpleasant)\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "\n",
        "    @staticmethod\n",
        "    def word_association_with_attribute(self, w, A, B):\n",
        "        return np.mean([cosine_similarity(np.array(w).reshape(1,-1),np.array(a).reshape(1,-1)) for a in A]) - np.mean([cosine_similarity(np.array(w).reshape(1,-1),np.array(b).reshape(1,-1)) for b in B])\n",
        "\n",
        "    @staticmethod\n",
        "    def differential_assoication(self, X, Y, A, B):\n",
        "        return np.sum([self.word_association_with_attribute(self, x, A, B) for x in X]) - np.sum([self.word_association_with_attribute(self, y, A, B) for y in Y])\n",
        "\n",
        "    @staticmethod\n",
        "    def weat_effect_size(self, X, Y, A, B):\n",
        "        return (\n",
        "            np.mean([self.word_association_with_attribute(self, x, A, B) for x in X]) -\n",
        "            np.mean([self.word_association_with_attribute(self, y, A, B) for y in Y])\n",
        "        ) / np.std([self.word_association_with_attribute(self, w, A, B) for w in X + Y])\n",
        "\n",
        "    @staticmethod\n",
        "    def random_permutation(self, iterable, r=None):\n",
        "        pool = tuple(iterable)\n",
        "        r = len(pool) if r is None else r\n",
        "        return tuple(random.sample(pool, r))\n",
        "\n",
        "    @staticmethod\n",
        "    def weat_p_value(self, X, Y, A, B, sample):\n",
        "        size_of_permutation = min(len(X), len(Y))\n",
        "        X_Y = X + Y\n",
        "        observed_test_stats_over_permutations = []\n",
        "\n",
        "        if not sample:\n",
        "            permutations = combinations(X_Y, size_of_permutation)\n",
        "        else:\n",
        "            permutations = [self.random_permutation(self, X_Y, size_of_permutation) for s in range(sample)]\n",
        "        print(np.array(X_Y).shape)\n",
        "        for Xi in permutations:\n",
        "            Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
        "            observed_test_stats_over_permutations.append(self.differential_assoication(self, Xi, Yi, A, B))\n",
        "\n",
        "        unperturbed = self.differential_assoication(self, X, Y, A, B)\n",
        "        is_over = np.array([o > unperturbed for o in observed_test_stats_over_permutations])\n",
        "        return is_over.sum() / is_over.size\n",
        "\n",
        "    @staticmethod\n",
        "    def weat_stats(X, Y, A, B, self, sample_p=None):\n",
        "        test_statistic = self.differential_assoication(self, X, Y, A, B)\n",
        "        effect_size = self.weat_effect_size(self, X, Y, A, B)\n",
        "        p = self.weat_p_value(self, X, Y, A, B, sample=sample_p)\n",
        "        return test_statistic, effect_size, p\n",
        "\n",
        "    def run_test(self, target_1, target_2, attributes_1, attributes_2, sample_p=None):\n",
        "        \"\"\"Run the WEAT test for differential association between two \n",
        "        sets of target words and two seats of attributes.\n",
        "        \n",
        "        EXAMPLE:\n",
        "            >>> test.run_test(WEATTest.instruments, WEATTest.weapon, WEATTest.pleasant, WEATTest.unpleasant)\n",
        "            >>> test.run_test(a, b, c, d, sample_p=1000) # use 1000 permutations for p-value calculation\n",
        "            >>> test.run_test(a, b, c, d, sample_p=None) # use all possible permutations for p-value calculation\n",
        "            \n",
        "        RETURNS:\n",
        "            (d, e, p). A tuple of floats, where d is the WEAT Test statistic, \n",
        "            e is the effect size, and p is the one-sided p-value measuring the\n",
        "            (un)likeliness of the null hypothesis (which is that there is no\n",
        "            difference in association between the two target word sets and\n",
        "            the attributes).\n",
        "            \n",
        "            If e is large and p small, then differences in the model between \n",
        "            the attribute word sets match differences between the targets.\n",
        "        \"\"\"\n",
        "        X = [list(self.model[w]) for w in target_1]\n",
        "        Y = [list(self.model[w]) for w in target_2]\n",
        "        A = [list(self.model[w]) for w in attributes_1]\n",
        "        B = [list(self.model[w]) for w in attributes_2]\n",
        "        print(X)\n",
        "        return self.weat_stats(X, Y, A, B, self, sample_p)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2I9dGN5fGQw-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Code test"
      ]
    },
    {
      "metadata": {
        "id": "agNgns64tdBP",
        "colab_type": "code",
        "outputId": "05d3aae1-cb90-4a23-f675-757360296044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "#nlp = spacy.load('glove')\n",
        "test = WEATTest(glove)\n",
        "test.run_test(WEATTest.instruments, WEATTest.weapons, WEATTest.pleasant, WEATTest.unpleasant, 1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The glove embedding has been loaded!\n",
            "X Shape:  (3, 300)\n",
            "Y Shape:  (3, 300)\n",
            "A Shape:  (2, 300)\n",
            "B Shape:  (2, 300)\n",
            "X U Y Shape:  (6, 300)\n",
            "WA shape:  (3, 2)\n",
            "sWAB shape:  (3,)\n",
            "WA shape:  (3, 2)\n",
            "sWAB shape:  (3,)\n",
            "WA shape:  (6, 2)\n",
            "sWAB shape:  (6,)\n",
            "WEAT d =  1.8613524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "agexIaaUujxx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}