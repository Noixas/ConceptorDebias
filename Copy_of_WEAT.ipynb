{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of WEAT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsedoc/ConceptorDebias/blob/master/Copy_of_WEAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "l8Me4_OyFyKo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# WEAT Algorithm\n",
        "## Test Statistic"
      ]
    },
    {
      "metadata": {
        "id": "mf6_liysF8en",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# returns s(w, A, B) for all w in W (passed as argument). Shape: n_words (in W) x 1\n",
        "def swAB(W, A, B):\n",
        "  #Calculate cosine-similarity between W and A, W and B\n",
        "  #print(\"W: \", W.shape, \" A: \", A.shape, \" B: \", B.shape)\n",
        "  WA = cosine_similarity(W,A)\n",
        "  WB = cosine_similarity(W,B)\n",
        "  #print('WA shape: ', WA.shape)\n",
        "  #Take mean along columns\n",
        "  WAmean = np.mean(WA, axis = 1)\n",
        "  WBmean = np.mean(WB, axis = 1)\n",
        "  \n",
        "  #print('sWAB shape: ', WAmean.shape)\n",
        "  \n",
        "  return (WAmean - WBmean)\n",
        "  \n",
        "def test_statistic(X, Y, A, B):\n",
        "  return (sum(swAB(X, A, B)) - sum(swAB(Y, A, B)))\n",
        "\n",
        "def weat_effect_size(X, Y, A, B, embd):\n",
        "  #Convert the set of words to matrix\n",
        "  \n",
        "  Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
        "  Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
        "  Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
        "  Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
        "  \n",
        "  # Find X U Y\n",
        "  XuY = list(set(X).union(Y))\n",
        "  XuYmat = []\n",
        "  for w in XuY:\n",
        "    if w.lower() in embd:\n",
        "      XuYmat.append(embd[w.lower()])\n",
        "  XuYmat = np.array(XuYmat)\n",
        "#   print('X U Y Shape: ', XuYmat.shape)\n",
        "#   print('X Shape: ', Xmat.shape)\n",
        "#   print('Y Shape: ', Ymat.shape)\n",
        "#   print('A Shape: ', Amat.shape)\n",
        "#   print('B Shape: ', Bmat.shape)\n",
        "  \n",
        "  d = (np.mean(swAB(Xmat,Amat,Bmat)) - np.mean(swAB(Ymat,Amat,Bmat)))/np.std(swAB(XuYmat, Amat, Bmat))\n",
        "  \n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d8foGwVSGI16",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## P-Value"
      ]
    },
    {
      "metadata": {
        "id": "ZDy-duFOFj71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "from itertools import combinations, filterfalse\n",
        "\n",
        "def random_permutation(iterable, r=None):\n",
        "  pool = tuple(iterable)\n",
        "  r = len(pool) if r is None else r\n",
        "  return tuple(random.sample(pool, r))\n",
        "\n",
        "def weat_p_value(X, Y, A, B, embd, sample):\n",
        "  size_of_permutation = min(len(X), len(Y))\n",
        "  X_Y = X + Y\n",
        "  test_stats_over_permutation = []\n",
        "  \n",
        "  Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
        "  Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
        "  Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
        "  Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
        "  \n",
        "  if not sample:\n",
        "      permutations = combinations(X_Y, size_of_permutation)\n",
        "  else:\n",
        "      permutations = [random_permutation(X_Y, size_of_permutation) for s in range(sample)]\n",
        "      \n",
        "  for Xi in permutations:\n",
        "    Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
        "    Ximat = np.array([embd[w.lower()] for w in Xi if w.lower() in embd])\n",
        "    Yimat = np.array([embd[w.lower()] for w in Yi if w.lower() in embd])\n",
        "    test_stats_over_permutation.append(test_statistic(Ximat, Yimat, Amat, Bmat))\n",
        "    \n",
        "  unperturbed = test_statistic(Xmat, Ymat, Amat, Bmat)\n",
        "  \n",
        "  is_over = np.array([o > unperturbed for o in test_stats_over_permutation])\n",
        "  #print(\"All: \", test_stats_over_permutation)\n",
        "  #print(\"Unpertrubed: \", unperturbed)\n",
        "  return is_over.sum() / is_over.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NbRMmhwbGL98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test on sample input"
      ]
    },
    {
      "metadata": {
        "id": "upscdvtwFm0l",
        "colab_type": "code",
        "outputId": "c9706781-f440-4e9f-86a0-4e3a0ca7ef9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "X = [\"bagpipe\", \"cello\", \"guitar\", \"lute\", \"trombone\", \"banjo\", \"clarinet\", \"harmonica\", \"mandolin\", \"trumpet\", \"bassoon\", \"drum\", \"harp\", \"oboe\", \"tuba\", \"bell\", \"fiddle\", \"harpsichord\", \"piano\", \"viola\", \"bongo\",\n",
        "\"flute\", \"horn\", \"saxophone\", \"violin\"] #Instruments\n",
        "Y = [\"arrow\", \"club\", \"gun\", \"missile\", \"spear\", \"axe\", \"dagger\", \"harpoon\", \"pistol\", \"sword\", \"blade\", \"dynamite\", \"hatchet\", \"rifle\", \"tank\", \"bomb\", \"firearm\", \"knife\", \"shotgun\", \"teargas\", \"cannon\", \"grenade\",\n",
        "    \"mace\", \"slingshot\", \"whip\"] #Weapons\n",
        "A = [\"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\",\n",
        "    \"happy\", \"laughter\", \"paradise\", \"vacation\"] #Pleasant\n",
        "B = [\"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\",\n",
        "    \"vomit\", \"agony\", \"prison\"] #Unpleasant\n",
        "\n",
        "#Load word embeddings\n",
        "#load gensim formatted Full Glove embeddings\n",
        "#!gdown https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = ''\n",
        "\n",
        "\n",
        "glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
        "print('The glove embedding has been loaded!')\n",
        "\n",
        "print('WEAT d = ', weat_effect_size(X, Y, A, B, glove))\n",
        "print('WEAT p = ', weat_p_value(X, Y, A, B, glove, 1000))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The glove embedding has been loaded!\n",
            "WEAT d =  1.5495627\n",
            "Unpertrubed:  2.2905553244054317\n",
            "WEAT p =  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rfwYNFl2ckGy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## WEAT with conceptor debiased embeddings"
      ]
    },
    {
      "metadata": {
        "id": "xEuq-Lt5ctV2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compute the conceptor matrix for all words and gender specific words."
      ]
    },
    {
      "metadata": {
        "id": "HAG4oqwIc1Z3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Compute the conceptor matrix\n",
        "def process_cn_matrix(subspace, alpha = 2):\n",
        "  #print(\"starting...\")\n",
        "  #x = orig_embd.vectors\n",
        "  #print(subspace.shape)\n",
        "  \n",
        "  #Calculate the correlation matrix\n",
        "  R = subspace.dot(subspace.T)/(subspace.shape[1])\n",
        "  #print(\"R calculated\")\n",
        "  \n",
        "  #Calculate the conceptor matrix\n",
        "  C = R @ (np.linalg.inv(R + alpha ** (-2) * np.eye(subspace.shape[0])))\n",
        "  #print(\"C calculated\")\n",
        "  \n",
        "  #Calculate the negation of the conceptor matrix\n",
        "  negC = np.eye(subspace.shape[0]) - C\n",
        "  #print(\"negC calculated\")\n",
        "  \n",
        "  return negC\n",
        "\n",
        "def apply_conceptor(x, C):\n",
        "  #Post-process the vocab matrix\n",
        "  newX = (C @ x).T\n",
        "  print(newX.shape)\n",
        "  return newX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GHaXxx5L3Ugb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load embeddings of all words from the ref. wordlist from a specific embedding"
      ]
    },
    {
      "metadata": {
        "id": "8WBI1VED3bpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Arguments - embd: The word embeddings in the form of a dict || wikiWordsPath: List of words to be considered\n",
        "def load_all_vectors(embd, wikiWordsPath):\n",
        "  all_words_index = {}\n",
        "  all_words_mat = []\n",
        "  with open(wikiWordsPath, \"r+\") as f_in:\n",
        "    ind = 0\n",
        "    for line in f_in:\n",
        "      word = line.split(' ')[0]\n",
        "      if word in embd:\n",
        "        all_words_index[word] = ind\n",
        "        all_words_mat.append(embd[word])\n",
        "        ind = ind+1\n",
        "        \n",
        "  return all_words_index, all_words_mat\n",
        "\n",
        "def load_subspace_vectors(embd, subspace_words):\n",
        "  subspace_embd_mat = []\n",
        "  ind = 0\n",
        "  for word in subspace_words:\n",
        "    if word in embd:\n",
        "      subspace_embd_mat.append(embd[word])\n",
        "      ind = ind+1\n",
        "      \n",
        "  return subspace_embd_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hH87LMqmQuhb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "431942b5-df61-4eac-e0fa-701ffb32fad7"
      },
      "cell_type": "code",
      "source": [
        "all_words_index, all_words_mat = load_all_vectors(glove, wikiWordsPath)\n",
        "\n",
        "aaa = list(all_words_index.keys())\n",
        "X = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "print(X)\n",
        "a = ['T' for w in X if w.lower() in aaa]\n",
        "print(a)\n",
        "print(np.array(a).shape)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Brad', 'Brendan', 'Geoffrey', 'Greg', 'Brett', 'Jay', 'Matthew', 'Neil', 'Todd', 'Allison', 'Anne', 'Carrie', 'Emily', 'Jill', 'Laurie', 'Kristen', 'Meredith', 'Sarah']\n",
            "['T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T']\n",
            "(18,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vjtc01gf8aR4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Conceptor all words and store it in a dictonary"
      ]
    },
    {
      "metadata": {
        "id": "EzfTYgq0I9WW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load all word lists - Subspace"
      ]
    },
    {
      "metadata": {
        "id": "D4kndsi_DwNr",
        "colab_type": "code",
        "outputId": "e917b4e8-8224-4dfb-c6c8-71dcf5406bca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "cell_type": "code",
      "source": [
        "# General word list\n",
        "!wget https://raw.githubusercontent.com/IlyaSemenov/wikipedia-word-frequency/master/results/enwiki-20150602-words-frequency.txt\n",
        "!git clone https://github.com/PrincetonML/SIF\n",
        "    \n",
        "# Gender word lists\n",
        "!git clone https://github.com/uclanlp/gn_glove\n",
        "!git clone https://github.com/uclanlp/corefBias\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/female.txt\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/male.txt\n",
        "    \n",
        "# our code for debiasing -- also includes word lists    \n",
        "!git clone https://github.com/jsedoc/ConceptorDebias"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-03 09:38:35--  https://raw.githubusercontent.com/IlyaSemenov/wikipedia-word-frequency/master/results/enwiki-20150602-words-frequency.txt\r\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\r\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23741395 (23M) [text/plain]\n",
            "Saving to: ‘enwiki-20150602-words-frequency.txt.2’\n",
            "\n",
            "enwiki-20150602-wor 100%[===================>]  22.64M  47.3MB/s    in 0.5s    \n",
            "\n",
            "2019-03-03 09:38:36 (47.3 MB/s) - ‘enwiki-20150602-words-frequency.txt.2’ saved [23741395/23741395]\n",
            "\n",
            "fatal: destination path 'SIF' already exists and is not an empty directory.\n",
            "fatal: destination path 'gn_glove' already exists and is not an empty directory.\n",
            "fatal: destination path 'corefBias' already exists and is not an empty directory.\n",
            "--2019-03-03 09:38:41--  https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/female.txt\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35751 (35K) [text/plain]\n",
            "Saving to: ‘female.txt.2’\n",
            "\n",
            "female.txt.2        100%[===================>]  34.91K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2019-03-03 09:38:41 (1.35 MB/s) - ‘female.txt.2’ saved [35751/35751]\n",
            "\n",
            "--2019-03-03 09:38:42--  https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/male.txt\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20466 (20K) [text/plain]\n",
            "Saving to: ‘male.txt.2’\n",
            "\n",
            "male.txt.2          100%[===================>]  19.99K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-03-03 09:38:43 (197 MB/s) - ‘male.txt.2’ saved [20466/20466]\n",
            "\n",
            "fatal: destination path 'ConceptorDebias' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "etG2ntoMTMiP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5c9631ee-79bc-4f03-dc94-cc877f8255f8"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConceptorDebias\t\t\t       female.txt    male.txt.1\r\n",
            "corefBias\t\t\t       female.txt.1  male.txt.2\r\n",
            "enwiki-20150602-words-frequency.txt    female.txt.2  SIF\r\n",
            "enwiki-20150602-words-frequency.txt.1  gn_glove\r\n",
            "enwiki-20150602-words-frequency.txt.2  male.txt\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "orBvfpqCFOfa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from load_word_lists import *\n",
        "\n",
        "gender_list_pronouns = WEATLists.W_7_Male_terms + WEATLists.W_7_Female_terms + WEATLists.W_8_Male_terms + WEATLists.W_8_Female_terms\n",
        "gender_list_pronouns = list(set(gender_list_pronouns))\n",
        "\n",
        "gender_list_extended = male_vino_extra + female_vino_extra + male_gnGlove + female_gnGlove\n",
        "gender_list_extended = list(set(gender_list_extended))\n",
        "\n",
        "gender_list_propernouns = male_cmu + female_cmu\n",
        "gender_list_propernouns = list(set(gender_list_propernouns))\n",
        "\n",
        "gender_list_all = gender_list_pronouns + gender_list_extended + gender_list_propernouns\n",
        "gender_list_all = list(set(gender_list_all))\n",
        "\n",
        "race_list = WEATLists.W_3_Unused_full_list_European_American_names + WEATLists.W_3_European_American_names + WEATLists.W_3_Unused_full_list_African_American_names + WEATLists.W_3_African_American_names + WEATLists.W_4_Unused_full_list_European_American_names + WEATLists.W_4_European_American_names + WEATLists.W_4_Unused_full_list_African_American_names + WEATLists.W_4_African_American_names + WEATLists.W_5_Unused_full_list_European_American_names + WEATLists.W_5_European_American_names + WEATLists.W_5_Unused_full_list_African_American_names + WEATLists.W_5_African_American_names \n",
        "race_list = list(set(race_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kEiBEVI2I2WR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load different embeddings"
      ]
    },
    {
      "metadata": {
        "id": "Nrlqu2xvJH_6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Glove"
      ]
    },
    {
      "metadata": {
        "id": "l3glDCr_NfFr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c6fd047-d0ba-4515-fcf2-68029bcc8553"
      },
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConceptorDebias  examples.desktop  nltk_data\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PFNSjmY8I1Ut",
        "colab_type": "code",
        "outputId": "fd2a228a-08e4-461d-82a6-7b01f4404774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "#Load word embeddings\n",
        "#download gensim formatted Full Glove embeddings\n",
        "!gdown https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = ''\n",
        "\n",
        "glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
        "print('The glove embedding has been loaded!')"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\r\n",
            "From: https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2\r\n",
            "To: /home/saketk/content/gensim_glove.840B.300d.txt.bin\n",
            "2.65GB [00:24, 110MB/s]\n",
            "The glove embedding has been loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aD-bmiCdGKYt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "print(X)\n",
        "a = [glove[w] for w in X if w.lower() in glove]\n",
        "print(np.array(a).shape)\n",
        "glove['Brad']\n",
        "#glove['brad']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "84af1zflJKLQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Word2vec"
      ]
    },
    {
      "metadata": {
        "id": "OmJlU2IiJMMj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5260973c-4948-4a0c-fbbc-c7443ea627a9"
      },
      "cell_type": "code",
      "source": [
        "#load gensim formatted Full Word2vec embeddings\n",
        "!gdown https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "  \n",
        "import gensim\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = ''\n",
        "\n",
        "word2vec = KeyedVectors.load_word2vec_format(resourceFile + 'GoogleNews-vectors-negative300.bin', binary=True)\n",
        "print('The word2vec embedding has been loaded!')"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\r\n",
            "From: https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\r\n",
            "To: /home/saketk/content/GoogleNews-vectors-negative300.bin.gz\n",
            "1.65GB [00:18, 89.5MB/s]\n",
            "The word2vec embedding has been loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FCiZfu13JPHl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fasttext"
      ]
    },
    {
      "metadata": {
        "id": "A9zwQ2T_JRBo",
        "colab_type": "code",
        "outputId": "973bc2f7-6515-468d-9842-66b53fa32ea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1Zl6a75Ybf8do9uupmrJWKQMnvqqme4fh\n",
        "\n",
        "import gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors  \n",
        "\n",
        "resourceFile = ''\n",
        "fasttext = KeyedVectors.load_word2vec_format(resourceFile + 'fasttext.bin', binary=True)\n",
        "print('The fasttext embedding has been loaded!')"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\r\n",
            "From: https://drive.google.com/uc?id=1Zl6a75Ybf8do9uupmrJWKQMnvqqme4fh\r\n",
            "To: /home/saketk/content/fasttext.bin\n",
            "2.42GB [00:21, 113MB/s]\n",
            "The fasttext embedding has been loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7hL2zgX6JRhG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Elmo"
      ]
    },
    {
      "metadata": {
        "id": "cXvK1wuvJTVP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=17TK2h3cz7amgm2mCY4QCYy1yh23ZFWDU\n",
        "  \n",
        "import pickle\n",
        "data = pickle.load(open(\"elmo_embeddings_emma_brown.pkl\", \"rb\"))\n",
        "\n",
        "def pick_embeddings(corpus, sent_embs):\n",
        "    X = []\n",
        "    labels = {}\n",
        "    sents = []\n",
        "    ind = 0\n",
        "    for i, s in enumerate(corpus):\n",
        "        for j, w in enumerate(s):\n",
        "            X.append(sent_embs[i][j])\n",
        "            if w.lower() in labels:\n",
        "              labels[w.lower()].append(ind)\n",
        "            else:\n",
        "              labels[w.lower()] = [ind]\n",
        "            sents.append(s)\n",
        "            ind = ind + 1\n",
        "    return (X, labels, sents)\n",
        "  \n",
        "def get_word_list(path):\n",
        "    word_list = []\n",
        "    with open(path, \"r+\") as f_in:\n",
        "      for line in f_in:\n",
        "        word = line.split(' ')[0]\n",
        "        word_list.append(word.lower())\n",
        "\n",
        "    return word_list\n",
        "\n",
        "def load_subspace_vectors_contextual(all_mat, all_index, subspace_list):\n",
        "    subspace_mat = []\n",
        "    for w in subspace_list:\n",
        "      if w.lower() in all_index:\n",
        "        for i in all_index[w.lower()]:\n",
        "          #print(type(i))\n",
        "          subspace_mat.append(all_mat[i])\n",
        "    #subspace_mat = [all_mat[i,:] for i in all_index[w.lower()] for w in subspace_list if w.lower() in all_index]\n",
        "    print(\"Subspace: \", np.array(subspace_mat).shape)\n",
        "    return subspace_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SZy2oUe0nlxz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "#nltk.download('brown')\n",
        "\n",
        "brown_corpus = brown.sents()\n",
        "elmo = data['brown_embs']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gTsLdzKKrnn6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(np.array(elmo_brown_mat).shape)\n",
        "print(len(list(elmo_brown_index.keys())))\n",
        "print(len(wiki_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CdeKwRyLTm8q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2708d4fc-aeea-4ae5-f2b5-9b56811ca4ad"
      },
      "cell_type": "code",
      "source": [
        "a = [[1,2,3], [4,5,6]]\n",
        "np.mean(a, axis = 0)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.5, 3.5, 4.5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "metadata": {
        "id": "_DRlLRadJT1A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bert"
      ]
    },
    {
      "metadata": {
        "id": "MYvJ3xGRJUpV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "80215cff-0996-44db-e5b4-871aaee48b34"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "elmo_gender_list_all.pkl       elmo_gender_list_propernouns.pkl\r\n",
            "elmo_gender_list_extended.pkl  elmo_race_list.pkl\r\n",
            "elmo_gender_list_pronouns.pkl\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "95qINnKVGEfY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2159
        },
        "outputId": "646e18f3-4e16-4095-ca9a-9fd456f0e721"
      },
      "cell_type": "code",
      "source": [
        "np.matrix(data2['type_embedings'])"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096],\n",
              "        [4096]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "metadata": {
        "id": "L541j-yEJWa4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Post-process (CN) all embeddings using a particular subspace"
      ]
    },
    {
      "metadata": {
        "id": "GeoHVWtg8jkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "outputId": "ddfb2c49-0e43-4a03-b0e8-d74c3fab3163"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from ConceptorDebias.conceptor_fxns import AND\n",
        "import pandas as pd\n",
        "\n",
        "resourceFile = ''\n",
        "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "all_embd = ['glove', 'word2vec', 'fasttext','elmo']\n",
        "# all_subspace = ['without_conceptor','gender_list_pronouns', 'gender_list_extended','gender_list_propernouns', 'gender_list_all', 'gender_list_and']\n",
        "all_subspace = ['without_conceptor', 'race_list']\n",
        "\n",
        "# career = WEATLists.W_6_Career\n",
        "# family = WEATLists.W_6_Family\n",
        "# male = WEATLists.W_6_Male_names\n",
        "# female = WEATLists.W_6_Female_names\n",
        "\n",
        "white = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "black = WEATLists.W_5_Unused_full_list_African_American_names\n",
        "pleasant = WEATLists.W_5_Pleasant\n",
        "unpleasant = WEATLists.W_5_Unpleasant\n",
        "\n",
        "# science = WEATLists.W_8_Science\n",
        "# arts = WEATLists.W_8_Arts\n",
        "# male = WEATLists.W_8_Male_terms\n",
        "# female = WEATLists.W_8_Female_terms\n",
        "\n",
        "#print(career, family, male, female)\n",
        "results = []\n",
        "\n",
        "for embd in all_embd:\n",
        "  #Initialize the embeddings to be used\n",
        "  curr_embd = eval(embd)\n",
        "  \n",
        "  #Load all embeddings in a matrix of all words in the wordlist\n",
        "  if embd == 'elmo' or embd == 'bert':\n",
        "    #wiki_words = get_word_list('SIF/auxiliary_data/enwiki_vocab_min200.txt')\n",
        "    all_words_mat, all_words_index, _ = pick_embeddings(brown_corpus, curr_embd)\n",
        "    print(\"All mat: \", np.array(all_words_mat).shape)\n",
        "    print(\"Number of words: \", len(list(all_words_index.keys())))\n",
        "  else:\n",
        "    all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)\n",
        "  \n",
        "  for subspace in all_subspace:\n",
        "    \n",
        "    if subspace != 'without_conceptor' and subspace != 'gender_list_and':\n",
        "      subspace_words_list = eval(subspace)\n",
        "\n",
        "    \n",
        "    if subspace != 'without_conceptor':\n",
        "      #CN all word embeddings using the respective subspace\n",
        "      if subspace == 'gender_list_and':\n",
        "        if embd == 'elmo' or embd == 'bert':\n",
        "          subspace_words_mat1 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_pronouns)\n",
        "          cn1 = process_cn_matrix(np.array(subspace_words_mat1).T, alpha = 8)\n",
        "\n",
        "          subspace_words_mat2 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_extended)\n",
        "          cn2 = process_cn_matrix(np.array(subspace_words_mat2).T, alpha = 3)\n",
        "\n",
        "          subspace_words_mat3 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_propernouns)\n",
        "          cn3 = process_cn_matrix(np.array(subspace_words_mat3).T, alpha = 10)\n",
        "\n",
        "          cn = AND(cn1, AND(cn2, cn3))\n",
        "          all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "          print(\"All mat CN: \", np.array(all_words_cn).shape)\n",
        "        else:\n",
        "          subspace_words_mat1 = load_subspace_vectors(curr_embd, gender_list_pronouns)\n",
        "          cn1 = process_cn_matrix(np.array(subspace_words_mat1).T)\n",
        "\n",
        "          subspace_words_mat2 = load_subspace_vectors(curr_embd, gender_list_extended)\n",
        "          cn2 = process_cn_matrix(np.array(subspace_words_mat2).T)\n",
        "\n",
        "          subspace_words_mat3 = load_subspace_vectors(curr_embd, gender_list_propernouns)\n",
        "          cn3 = process_cn_matrix(np.array(subspace_words_mat3).T)\n",
        "\n",
        "          cn = AND(cn1, AND(cn2, cn3))\n",
        "          all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "      else:\n",
        "        #Load all embeddings of the subspace as a matrix\n",
        "        if embd == 'elmo' or embd == 'bert':\n",
        "          subspace_words_mat = load_subspace_vectors_contextual(all_words_mat, all_words_index, subspace_words_list)\n",
        "          cn = process_cn_matrix(np.array(subspace_words_mat).T, alpha = 6)\n",
        "          all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "          print(\"Subspace mat: \", np.array(subspace_words_mat).shape)\n",
        "        else:\n",
        "          subspace_words_mat = load_subspace_vectors(curr_embd, subspace_words_list)\n",
        "          cn = process_cn_matrix(np.array(subspace_words_mat).T)\n",
        "          all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "    else:\n",
        "      all_words_cn = all_words_mat\n",
        "    all_words_cn = np.array(all_words_cn)\n",
        "    print(\"All CN: \", all_words_cn.shape)\n",
        "    #Store all conceptored words in a dictonary\n",
        "    all_words = {}\n",
        "    for word, index in all_words_index.items():\n",
        "      #print(word, index)\n",
        "      if embd == 'elmo' or embd == 'bert':\n",
        "        all_words[word] = np.mean([all_words_cn[i,:] for i in index], axis = 0)\n",
        "      else:\n",
        "        all_words[word] = all_words_cn[index,:]\n",
        "#     print(\"LAST: \", np.array(all_words[\"a\"]).shape)\n",
        "    if subspace == 'without_conceptor':\n",
        "      #WITHOUT CONCEPTOR\n",
        "      d_cn = weat_effect_size(white, black, pleasant, unpleasant, all_words)\n",
        "      p_cn = weat_p_value(white, black, pleasant, unpleasant, all_words, 1000)\n",
        "      print(\"Without conceptor\")\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    else:\n",
        "      #WITH CONCEPTOR\n",
        "      d_cn = weat_effect_size(white, black, pleasant, unpleasant, all_words)\n",
        "      p_cn = weat_p_value(white, black, pleasant, unpleasant, all_words, 1000)\n",
        "      print(\"With conceptor: \", embd, subspace)\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    \n",
        "    row = [embd, subspace, d_cn, p_cn]\n",
        "    results.append(row)\n",
        "    \n",
        "pd.DataFrame(np.array(results), columns = ['Embedding', 'Subspace', 'Effect Size', 'p-value'])"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All CN:  (128607, 300)\n",
            "Without conceptor\n",
            "WEAT d =  1.3542663\n",
            "WEAT p =  0.0\n",
            "(128607, 300)\n",
            "All CN:  (128607, 300)\n",
            "With conceptor:  glove race_list\n",
            "WEAT d =  0.6888005504919007\n",
            "WEAT p =  0.007\n",
            "All CN:  (76078, 300)\n",
            "Without conceptor\n",
            "WEAT d =  -0.27154148\n",
            "WEAT p =  0.265\n",
            "(76078, 300)\n",
            "All CN:  (76078, 300)\n",
            "With conceptor:  word2vec race_list\n",
            "WEAT d =  -0.5498752519776564\n",
            "WEAT p =  0.724\n",
            "All CN:  (119127, 300)\n",
            "Without conceptor\n",
            "WEAT d =  0.41335136\n",
            "WEAT p =  0.038\n",
            "(119127, 300)\n",
            "All CN:  (119127, 300)\n",
            "With conceptor:  fasttext race_list\n",
            "WEAT d =  -0.2710636522184819\n",
            "WEAT p =  0.573\n",
            "All mat:  (1161192, 768)\n",
            "Number of words:  49815\n",
            "All CN:  (1161192, 768)\n",
            "Without conceptor\n",
            "WEAT d =  1.3741117\n",
            "WEAT p =  0.0\n",
            "Subspace:  (751, 768)\n",
            "(1161192, 768)\n",
            "Subspace mat:  (751, 768)\n",
            "All CN:  (1161192, 768)\n",
            "With conceptor:  elmo race_list\n",
            "WEAT d =  -0.45138745986044126\n",
            "WEAT p =  0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Embedding           Subspace           Effect Size p-value\n",
              "0     glove  without_conceptor             1.3542663     0.0\n",
              "1     glove          race_list    0.6888005504919007   0.007\n",
              "2  word2vec  without_conceptor           -0.27154148   0.265\n",
              "3  word2vec          race_list   -0.5498752519776564   0.724\n",
              "4  fasttext  without_conceptor            0.41335136   0.038\n",
              "5  fasttext          race_list   -0.2710636522184819   0.573\n",
              "6      elmo  without_conceptor             1.3741117     0.0\n",
              "7      elmo          race_list  -0.45138745986044126     0.2"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Embedding</th>\n",
              "      <th>Subspace</th>\n",
              "      <th>Effect Size</th>\n",
              "      <th>p-value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>glove</td>\n",
              "      <td>without_conceptor</td>\n",
              "      <td>1.3542663</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>glove</td>\n",
              "      <td>race_list</td>\n",
              "      <td>0.6888005504919007</td>\n",
              "      <td>0.007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>word2vec</td>\n",
              "      <td>without_conceptor</td>\n",
              "      <td>-0.27154148</td>\n",
              "      <td>0.265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>word2vec</td>\n",
              "      <td>race_list</td>\n",
              "      <td>-0.5498752519776564</td>\n",
              "      <td>0.724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fasttext</td>\n",
              "      <td>without_conceptor</td>\n",
              "      <td>0.41335136</td>\n",
              "      <td>0.038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>fasttext</td>\n",
              "      <td>race_list</td>\n",
              "      <td>-0.2710636522184819</td>\n",
              "      <td>0.573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>elmo</td>\n",
              "      <td>without_conceptor</td>\n",
              "      <td>1.3741117</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>elmo</td>\n",
              "      <td>race_list</td>\n",
              "      <td>-0.45138745986044126</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "metadata": {
        "id": "YZNxmDKB8fbB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73921ec9-be30-4682-8b2b-f27414a277b7"
      },
      "cell_type": "code",
      "source": [
        "if w.lower() not in all_words_index:\n",
        "  print(\"T\")\n",
        "#load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_pronouns)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cVmZBwykJmgn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Calculate WEAT scores"
      ]
    },
    {
      "metadata": {
        "id": "PPukMLU2ZosX",
        "colab_type": "code",
        "outputId": "9e6d2b3a-05f8-407a-d65b-b6d6db1edc2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "science = WEATLists.W_8_Science\n",
        "arts = WEATLists.W_8_Arts\n",
        "male = WEATLists.W_8_Male_terms\n",
        "female = WEATLists.W_8_Female_terms\n",
        "\n",
        "#WITHOUT CONCEPTOR\n",
        "print(\"Without conceptor\")\n",
        "print('WEAT d = ', weat_effect_size(science, arts, male, female, curr_embd))\n",
        "print('WEAT p = ', weat_p_value(science, arts, male, female, curr_embd, 1000))\n",
        "\n",
        "#WITH CONCEPTOR\n",
        "print(\"With conceptor\")\n",
        "print('WEAT d = ', weat_effect_size(science, arts, male, female, all_words))\n",
        "print('WEAT p = ', weat_p_value(science, arts, male, female, all_words, 1000))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Without conceptor\n",
            "WEAT d =  1.2780356\n",
            "Unpertrubed:  0.3456038013100624\n",
            "WEAT p =  0.003\n",
            "With conceptor\n",
            "WEAT d =  0.681611936758009\n",
            "Unpertrubed:  0.10700200023922478\n",
            "WEAT p =  0.103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yAdi5tlZGRzL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## WEAT algorithm from GITHUB gist\n",
        "REF: https://gist.github.com/SandyRogers/e5c2e938502a75dcae25216e4fae2da5"
      ]
    },
    {
      "metadata": {
        "id": "2YM0FV9SFrWC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class WEATTest(object):\n",
        "    \"\"\"\n",
        "    Perform WEAT (Word Embedding Association Test) bias tests on a language model.\n",
        "    Follows from Caliskan et al 2017 (10.1126/science.aal4230).\n",
        "    \"\"\"\n",
        "    \n",
        "    instruments = [\"bagpipe\", \"cello\", \"guitar\", \"lute\", \"trombone\", \"banjo\", \"clarinet\", \"harmonica\", \"mandolin\", \"trumpet\", \"bassoon\", \"drum\", \"harp\", \"oboe\", \"tuba\", \"bell\", \"fiddle\", \"harpsichord\", \"piano\", \"viola\", \"bongo\",\n",
        "\"flute\", \"horn\", \"saxophone\", \"violin\"]\n",
        "    weapons = [\"arrow\", \"club\", \"gun\", \"missile\", \"spear\", \"axe\", \"dagger\", \"harpoon\", \"pistol\", \"sword\", \"blade\", \"dynamite\", \"hatchet\", \"rifle\", \"tank\", \"bomb\", \"firearm\", \"knife\", \"shotgun\", \"teargas\", \"cannon\", \"grenade\",\n",
        "    \"mace\", \"slingshot\", \"whip\"]\n",
        "    flowers = [\"aster\", \"clover\", \"hyacinth\", \"marigold\", \"poppy\", \"azalea\", \"crocus\", \"iris\", \"orchid\", \"rose\", \"blue-bell\", \"daffodil\", \"lilac\", \"pansy\", \"tulip\", \"buttercup\", \"daisy\", \"lily\", \"peony\", \"violet\", \"carnation\", \"gladiola\", \"magnolia\", \"petunia\", \"zinnia\"]\n",
        "    insects = [\"ant\", \"caterpillar\", \"flea\", \"locust\", \"spider\", \"bedbug\", \"centipede\", \"fly\", \"maggot\", \"tarantula\", \"bee\", \"cockroach\", \"gnat\", \"mosquito\", \"termite\", \"beetle\", \"cricket\", \"hornet\", \"moth\", \"wasp\", \"blackfly\",\n",
        "    \"dragonfly\", \"horsefly\", \"roach\", \"weevil\"]\n",
        "    pleasant = [\"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\",\n",
        "    \"happy\", \"laughter\", \"paradise\", \"vacation\"]\n",
        "    unpleasant = [\"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\",\n",
        "    \"vomit\", \"agony\", \"prison\"]\n",
        "    math = [\"math\", \"algebra\", \"geometry\", \"calculus\", \"equations\", \"computation\", \"numbers\", \"addition\"]\n",
        "    arts = [\"poetry\", \"art\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\", \"sculpture\"]\n",
        "    male = [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"his\", \"son\"]\n",
        "    female = [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \"hers\", \"daughter\"]\n",
        "    science = [\"science\", \"technology\", \"physics\", \"chemistry\", \"Einstein\", \"NASA\", \"experiment\", \"astronomy\"]\n",
        "    arts_2 = [\"poetry\", \"art\", \"Shakespeare\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\"]\n",
        "    male_2 = [\"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\"]\n",
        "    female_2 = [\"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"he\"]\n",
        "    mental_disease = [\"sad\", \"hopeless\", \"gloomy\", \"tearful\", \"miserable\", \"depressed\"]\n",
        "    physical_disease = [\"sick\", \"illness\", \"influenza\", \"disease\", \"virus\", \"cancer\"]\n",
        "    temporary = [\"impermanent\", \"unstable\", \"variable\", \"fleeting\", \"short-term\", \"brief\", \"occasional\"]\n",
        "    permanent = [\"stable\", \"always\", \"constant\", \"persistent\", \"chronic\", \"prolonged\", \"forever\"]\n",
        "    \n",
        "    def __init__(self, model):\n",
        "        \"\"\"Setup a Word Embedding Association Test for a given spaCy language model.\n",
        "        \n",
        "        EXAMPLE:\n",
        "            >>> nlp = spacy.load('en_core_web_md')\n",
        "            >>> test = WEATTest(nlp)\n",
        "            >>> test.run_test(WEATTest.instruments, WEATTest.weapon, WEATTest.pleasant, WEATTest.unpleasant)\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "\n",
        "    @staticmethod\n",
        "    def word_association_with_attribute(self, w, A, B):\n",
        "        return np.mean([cosine_similarity(np.array(w).reshape(1,-1),np.array(a).reshape(1,-1)) for a in A]) - np.mean([cosine_similarity(np.array(w).reshape(1,-1),np.array(b).reshape(1,-1)) for b in B])\n",
        "\n",
        "    @staticmethod\n",
        "    def differential_assoication(self, X, Y, A, B):\n",
        "        return np.sum([self.word_association_with_attribute(self, x, A, B) for x in X]) - np.sum([self.word_association_with_attribute(self, y, A, B) for y in Y])\n",
        "\n",
        "    @staticmethod\n",
        "    def weat_effect_size(self, X, Y, A, B):\n",
        "        return (\n",
        "            np.mean([self.word_association_with_attribute(self, x, A, B) for x in X]) -\n",
        "            np.mean([self.word_association_with_attribute(self, y, A, B) for y in Y])\n",
        "        ) / np.std([self.word_association_with_attribute(self, w, A, B) for w in X + Y])\n",
        "\n",
        "    @staticmethod\n",
        "    def random_permutation(self, iterable, r=None):\n",
        "        pool = tuple(iterable)\n",
        "        r = len(pool) if r is None else r\n",
        "        return tuple(random.sample(pool, r))\n",
        "\n",
        "    @staticmethod\n",
        "    def weat_p_value(self, X, Y, A, B, sample):\n",
        "        size_of_permutation = min(len(X), len(Y))\n",
        "        X_Y = X + Y\n",
        "        observed_test_stats_over_permutations = []\n",
        "\n",
        "        if not sample:\n",
        "            permutations = combinations(X_Y, size_of_permutation)\n",
        "        else:\n",
        "            permutations = [self.random_permutation(self, X_Y, size_of_permutation) for s in range(sample)]\n",
        "        print(np.array(X_Y).shape)\n",
        "        for Xi in permutations:\n",
        "            Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
        "            observed_test_stats_over_permutations.append(self.differential_assoication(self, Xi, Yi, A, B))\n",
        "\n",
        "        unperturbed = self.differential_assoication(self, X, Y, A, B)\n",
        "        is_over = np.array([o > unperturbed for o in observed_test_stats_over_permutations])\n",
        "        return is_over.sum() / is_over.size\n",
        "\n",
        "    @staticmethod\n",
        "    def weat_stats(X, Y, A, B, self, sample_p=None):\n",
        "        test_statistic = self.differential_assoication(self, X, Y, A, B)\n",
        "        effect_size = self.weat_effect_size(self, X, Y, A, B)\n",
        "        p = self.weat_p_value(self, X, Y, A, B, sample=sample_p)\n",
        "        return test_statistic, effect_size, p\n",
        "\n",
        "    def run_test(self, target_1, target_2, attributes_1, attributes_2, sample_p=None):\n",
        "        \"\"\"Run the WEAT test for differential association between two \n",
        "        sets of target words and two seats of attributes.\n",
        "        \n",
        "        EXAMPLE:\n",
        "            >>> test.run_test(WEATTest.instruments, WEATTest.weapon, WEATTest.pleasant, WEATTest.unpleasant)\n",
        "            >>> test.run_test(a, b, c, d, sample_p=1000) # use 1000 permutations for p-value calculation\n",
        "            >>> test.run_test(a, b, c, d, sample_p=None) # use all possible permutations for p-value calculation\n",
        "            \n",
        "        RETURNS:\n",
        "            (d, e, p). A tuple of floats, where d is the WEAT Test statistic, \n",
        "            e is the effect size, and p is the one-sided p-value measuring the\n",
        "            (un)likeliness of the null hypothesis (which is that there is no\n",
        "            difference in association between the two target word sets and\n",
        "            the attributes).\n",
        "            \n",
        "            If e is large and p small, then differences in the model between \n",
        "            the attribute word sets match differences between the targets.\n",
        "        \"\"\"\n",
        "        X = [list(self.model[w]) for w in target_1]\n",
        "        Y = [list(self.model[w]) for w in target_2]\n",
        "        A = [list(self.model[w]) for w in attributes_1]\n",
        "        B = [list(self.model[w]) for w in attributes_2]\n",
        "        print(X)\n",
        "        return self.weat_stats(X, Y, A, B, self, sample_p)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2I9dGN5fGQw-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Code test"
      ]
    },
    {
      "metadata": {
        "id": "agNgns64tdBP",
        "colab_type": "code",
        "outputId": "05d3aae1-cb90-4a23-f675-757360296044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "#nlp = spacy.load('glove')\n",
        "test = WEATTest(glove)\n",
        "test.run_test(WEATTest.instruments, WEATTest.weapons, WEATTest.pleasant, WEATTest.unpleasant, 1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The glove embedding has been loaded!\n",
            "X Shape:  (3, 300)\n",
            "Y Shape:  (3, 300)\n",
            "A Shape:  (2, 300)\n",
            "B Shape:  (2, 300)\n",
            "X U Y Shape:  (6, 300)\n",
            "WA shape:  (3, 2)\n",
            "sWAB shape:  (3,)\n",
            "WA shape:  (3, 2)\n",
            "sWAB shape:  (3,)\n",
            "WA shape:  (6, 2)\n",
            "sWAB shape:  (6,)\n",
            "WEAT d =  1.8613524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "agexIaaUujxx",
        "colab_type": "code",
        "outputId": "985b4438-7976-47b7-fc24-9c5ebf4ee83a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_W_10_Old_peoples_names.pkl\r\n",
            "bert_W_10_Pleasant.pkl\r\n",
            "bert_W_10_Unpleasant.pkl\r\n",
            "bert_W_10_Young_peoples_names.pkl\r\n",
            "bert_W_1_Flowers.pkl\r\n",
            "bert_W_1_Insects.pkl\r\n",
            "bert_W_1_Pleasant.pkl\r\n",
            "bert_W_1_Unpleasant.pkl\r\n",
            "bert_W_2_Instruments.pkl\r\n",
            "bert_W_2_Pleasant.pkl\r\n",
            "bert_W_2_Unpleasant.pkl\r\n",
            "bert_W_2_Weapons.pkl\r\n",
            "bert_W_3_African_American_names.pkl\r\n",
            "bert_W_3_European_American_names.pkl\r\n",
            "bert_W_3_Pleasant.pkl\r\n",
            "bert_W_3_Unpleasant.pkl\r\n",
            "bert_W_3_Unused_full_list_African_American_names.pkl\r\n",
            "bert_W_3_Unused_full_list_European_American_names.pkl\r\n",
            "bert_W_4_African_American_names.pkl\r\n",
            "bert_W_4_European_American_names.pkl\r\n",
            "bert_W_4_Pleasant.pkl\r\n",
            "bert_W_4_Unpleasant.pkl\r\n",
            "bert_W_4_Unused_full_list_African_American_names.pkl\r\n",
            "bert_W_4_Unused_full_list_European_American_names.pkl\r\n",
            "bert_W_5_African_American_names.pkl\r\n",
            "bert_W_5_European_American_names.pkl\r\n",
            "bert_W_5_Pleasant.pkl\r\n",
            "bert_W_5_Unpleasant.pkl\r\n",
            "bert_W_5_Unused_full_list_African_American_names.pkl\r\n",
            "bert_W_5_Unused_full_list_European_American_names.pkl\r\n",
            "bert_W_6_Career.pkl\r\n",
            "bert_W_6_Family.pkl\r\n",
            "bert_W_6_Female_names.pkl\r\n",
            "bert_W_6_Male_names.pkl\r\n",
            "bert_W_7_Arts.pkl\r\n",
            "bert_W_7_Female_terms.pkl\r\n",
            "bert_W_7_Male_terms.pkl\r\n",
            "bert_W_7_Math.pkl\r\n",
            "bert_W_8_Arts.pkl\r\n",
            "bert_W_8_Female_terms.pkl\r\n",
            "bert_W_8_Male_terms.pkl\r\n",
            "bert_W_8_Science.pkl\r\n",
            "bert_W_9_Mental_disease.pkl\r\n",
            "bert_W_9_Permanent.pkl\r\n",
            "bert_W_9_Physical_disease.pkl\r\n",
            "bert_W_9_Temporary.pkl\r\n",
            "bert_WEFAT_1_Careers.pkl\r\n",
            "bert_WEFAT_1_Female_attributes.pkl\r\n",
            "bert_WEFAT_1_Male_attributes.pkl\r\n",
            "bert_WEFAT_2_Androgynous_Names.pkl\r\n",
            "bert_WEFAT_2_Female_attributes.pkl\r\n",
            "bert_WEFAT_2_Male_attributes.pkl\r\n",
            "big_bert_gender_list_all.pkl\r\n",
            "big_bert_gender_list_extended.pkl\r\n",
            "big_bert_gender_list_pronouns.pkl\r\n",
            "big_bert_gender_list_propernouns.pkl\r\n",
            "big_bert_race_list.pkl\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hDL0wCtSwoOx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ab416fa3-e2cd-4775-bae4-a6d62b2302ef"
      },
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# all = {}\n",
        "# for filename in os.listdir('/home/saketk/bert'):\n",
        "#   all[filename] = pickle.load(open(filename, \"rb\"))\n",
        "res = all['big_bert_gender_list_extended.pkl']['type_embedings']\n",
        "w = []\n",
        "for name in all:\n",
        "  res = np.concatenate((res, all[name]['type_embedings']))\n",
        "  w += all[name]['words']\n",
        "print(res.shape)\n",
        "print(len(set(w)))\n",
        "w = [aaa.lower() for aaa in w]\n",
        "cn_pronouns = all['big_bert_gender_list_pronouns.pkl']['GnegC']\n",
        "cn_propernouns = all['big_bert_gender_list_propernouns.pkl']['GnegC']\n",
        "cn_extended = all['big_bert_gender_list_extended.pkl']['GnegC']\n",
        "cn_all = all['big_bert_gender_list_all.pkl']['GnegC']\n",
        "cn_race = all['big_bert_race_list.pkl']['GnegC']\n",
        "\n",
        "print(np.array(cn_all).shape)"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(975, 4096)\n",
            "493\n",
            "(4096, 4096)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cWeJf7sDkSCc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "4baff298-96f6-4ed7-cb29-d378392861b2"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from ConceptorDebias.conceptor_fxns import AND\n",
        "import pandas as pd\n",
        "\n",
        "resourceFile = ''\n",
        "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "all_embd = ['bert']\n",
        "all_subspace = ['without_conceptor','gender_list_pronouns', 'gender_list_extended','gender_list_propernouns', 'gender_list_all', 'gender_list_and']\n",
        "# all_subspace = ['without_conceptor', 'race_list']\n",
        "\n",
        "all_words_mat = res\n",
        "all_words_index = {}\n",
        "print(len(set(w)))\n",
        "for i,a in enumerate(w):\n",
        "  all_words_index[a] = i\n",
        "# all_words_index = {wr:i for i,wr in enumerate(w)}\n",
        "print(\"All_words_mat: \", all_words_mat.shape)\n",
        "print(\"Index: \", len(all_words_index.keys()))\n",
        "career = WEATLists.W_8_Science\n",
        "family = WEATLists.W_8_Arts\n",
        "male = WEATLists.W_8_Male_terms\n",
        "female = WEATLists.W_8_Female_terms\n",
        "\n",
        "# white = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "# black = WEATLists.W_5_Unused_full_list_African_American_names\n",
        "# pleasant = WEATLists.W_5_Pleasant\n",
        "# unpleasant = WEATLists.W_5_Unpleasant\n",
        "embd = 'bert'\n",
        "results = []\n",
        "for subspace in all_subspace:\n",
        "    \n",
        "  if subspace != 'without_conceptor' and subspace != 'gender_list_and':\n",
        "    subspace_words_list = eval(subspace)\n",
        "\n",
        "  if subspace != 'without_conceptor':\n",
        "    #CN all word embeddings using the respective subspace\n",
        "    if subspace == 'gender_list_and':\n",
        "      cn = AND(cn_pronouns, AND(cn_extended, cn_propernouns))\n",
        "      all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "      print(\"All mat CN: \", np.array(all_words_cn).shape)\n",
        "    else:\n",
        "      #Load all embeddings of the subspace as a matrix\n",
        "      if subspace == 'gender_list_pronouns':\n",
        "        cn = cn_pronouns\n",
        "      elif subspace == 'gender_list_propernouns':\n",
        "        cn = cn_propernouns\n",
        "      elif subspace == 'gender_list_extended':\n",
        "        cn = cn_extended\n",
        "      elif subspace == 'gender_list_all':\n",
        "        cn = cn_all\n",
        "        \n",
        "      print(\"CN shape: \", np.array(cn).shape)\n",
        "      all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "      print(\"Subspace mat: \", np.array(subspace_words_mat).shape)\n",
        "      \n",
        "  else:\n",
        "    all_words_cn = all_words_mat\n",
        "  all_words_cn = np.array(all_words_cn)\n",
        "  print(\"All CN: \", all_words_cn.shape)\n",
        "  #Store all conceptored words in a dictonary\n",
        "  all_words = {}\n",
        "  for word, index in all_words_index.items():\n",
        "    all_words[word] = all_words_cn[index,:]\n",
        "  print(\"D: \", len(all_words.keys()))\n",
        "  if subspace == 'without_conceptor':\n",
        "    #WITHOUT CONCEPTOR\n",
        "    d_cn = weat_effect_size(career, family, male, female, all_words)\n",
        "    p_cn = weat_p_value(career, family, male, female, all_words, 1000)\n",
        "    print(\"Without conceptor\")\n",
        "    print('WEAT d = ', d_cn)\n",
        "    print('WEAT p = ', p_cn)\n",
        "  else:\n",
        "    #WITH CONCEPTOR\n",
        "    d_cn = weat_effect_size(career, family, male, female, all_words)\n",
        "    p_cn = weat_p_value(career, family, male, female, all_words, 1000)\n",
        "    print(\"With conceptor: \", embd, subspace)\n",
        "    print('WEAT d = ', d_cn)\n",
        "    print('WEAT p = ', p_cn)\n",
        "\n",
        "  row = [embd, subspace, d_cn, p_cn]\n",
        "  results.append(row)\n",
        "\n",
        "  \n",
        "  \n",
        "pd.DataFrame(np.array(results), columns = ['Embedding', 'Subspace', 'Effect Size', 'p-value'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "493\n",
            "All_words_mat:  (975, 4096)\n",
            "Index:  493\n",
            "All CN:  (975, 4096)\n",
            "D:  493\n",
            "Without conceptor\n",
            "WEAT d =  -0.9136145\n",
            "WEAT p =  0.884\n",
            "CN shape:  (4096, 4096)\n",
            "(975, 4096)\n",
            "Subspace mat:  (751, 768)\n",
            "All CN:  (975, 4096)\n",
            "D:  493\n",
            "With conceptor:  bert gender_list_pronouns\n",
            "WEAT d =  -1.2300879460348944\n",
            "WEAT p =  0.974\n",
            "CN shape:  (4096, 4096)\n",
            "(975, 4096)\n",
            "Subspace mat:  (751, 768)\n",
            "All CN:  (975, 4096)\n",
            "D:  493\n",
            "With conceptor:  bert gender_list_extended\n",
            "WEAT d =  -1.1197445478394272\n",
            "WEAT p =  0.994\n",
            "CN shape:  (4096, 4096)\n",
            "(975, 4096)\n",
            "Subspace mat:  (751, 768)\n",
            "All CN:  (975, 4096)\n",
            "D:  493\n",
            "With conceptor:  bert gender_list_propernouns\n",
            "WEAT d =  -0.9280666738182892\n",
            "WEAT p =  0.922\n",
            "CN shape:  (4096, 4096)\n",
            "(975, 4096)\n",
            "Subspace mat:  (751, 768)\n",
            "All CN:  (975, 4096)\n",
            "D:  493\n",
            "With conceptor:  bert gender_list_all\n",
            "WEAT d =  -0.3839682787101548\n",
            "WEAT p =  0.704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "56ho4eDSz0tT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "cf57e4e1-872a-4c1d-9ed2-4ecf2357791f"
      },
      "cell_type": "code",
      "source": [
        "a = [[1,2,3,4],[3,2,1,4]]\n",
        "b = [[1,2,3,4],[6,7,8,9],[1,4,3,2]]\n",
        "d = [[33,44,55,66]]\n",
        "c = np.concatenate((a,b,d))\n",
        "print(c)"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1  2  3  4]\n",
            " [ 3  2  1  4]\n",
            " [ 1  2  3  4]\n",
            " [ 6  7  8  9]\n",
            " [ 1  4  3  2]\n",
            " [33 44 55 66]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gbg0wsR_Ykym",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hard Debiasing"
      ]
    },
    {
      "metadata": {
        "id": "HNe9mk7FYnsf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2414f8ed-77c5-4832-a79c-9e530b0d80aa"
      },
      "cell_type": "code",
      "source": [
        "!mkdir debiaswe_tutorial\n",
        "%cd debiaswe_tutorial\n",
        "!git clone https://github.com/tolga-b/debiaswe.git"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/saketk/content/debiaswe_tutorial\n",
            "Cloning into 'debiaswe'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Total 91 (delta 0), reused 0 (delta 0), pack-reused 91\u001b[K\n",
            "Unpacking objects: 100% (91/91), done.\n",
            "Checking connectivity... done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cJWTFo4uYpu-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from debiaswe import debiaswe\n",
        "from debiaswe.debiaswe import we\n",
        "import debiaswe as dwe\n",
        "#import debiaswe.we as we\n",
        "from debiaswe.debiaswe.we import WordEmbedding\n",
        "from debiaswe.debiaswe.data import load_professions\n",
        "from debiaswe.debiaswe import debias\n",
        "from debiaswe.debiaswe.debias import debias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-W45lz9-Yxa_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a017b31-a3f5-45f9-9110-a031934d8a74"
      },
      "cell_type": "code",
      "source": [
        "%cd debiaswe"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/saketk/content/debiaswe_tutorial/debiaswe\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L7Ln5pufZAqe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "11c31685-ec09-44e5-b402-f7dbc88c36f3"
      },
      "cell_type": "code",
      "source": [
        "#!gdown https://drive.google.com/uc?id=1NH6jcrg8SXbnhpIXRIXF_-KUE7wGxGaG\n",
        "!unzip w2v_gnews_small.zip\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  w2v_gnews_small.zip\n",
            "  inflating: w2v_gnews_small.txt     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "so5kfYCaY4_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "ec36761e-4a8b-4635-8353-509b4a7d6f3b"
      },
      "cell_type": "code",
      "source": [
        "# Lets load some gender related word lists to help us with debiasing\n",
        "with open('./data/definitional_pairs.json', \"r\") as f:\n",
        "    defs = json.load(f) #gender definitional words\n",
        "print(\"definitional\", defs)\n",
        "defs_list = []\n",
        "for pair in defs:\n",
        "  defs_list.append(pair[0])\n",
        "  defs_list.append(pair[1])\n",
        "\n",
        "with open('./data/equalize_pairs.json', \"r\") as f:\n",
        "    equalize_pairs = json.load(f) \n",
        "print(\"Equalize pairs\", equalize_pairs)\n",
        "\n",
        "with open('./data/gender_specific_seed.json', \"r\") as f:\n",
        "    gender_specific_words = json.load(f)\n",
        "print(\"gender specific\", len(gender_specific_words), gender_specific_words[:10])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "definitional [['woman', 'man'], ['girl', 'boy'], ['she', 'he'], ['mother', 'father'], ['daughter', 'son'], ['gal', 'guy'], ['female', 'male'], ['her', 'his'], ['herself', 'himself'], ['Mary', 'John']]\n",
            "Equalize pairs [['monastery', 'convent'], ['spokesman', 'spokeswoman'], ['Catholic_priest', 'nun'], ['Dad', 'Mom'], ['Men', 'Women'], ['councilman', 'councilwoman'], ['grandpa', 'grandma'], ['grandsons', 'granddaughters'], ['prostate_cancer', 'ovarian_cancer'], ['testosterone', 'estrogen'], ['uncle', 'aunt'], ['wives', 'husbands'], ['Father', 'Mother'], ['Grandpa', 'Grandma'], ['He', 'She'], ['boy', 'girl'], ['boys', 'girls'], ['brother', 'sister'], ['brothers', 'sisters'], ['businessman', 'businesswoman'], ['chairman', 'chairwoman'], ['colt', 'filly'], ['congressman', 'congresswoman'], ['dad', 'mom'], ['dads', 'moms'], ['dudes', 'gals'], ['ex_girlfriend', 'ex_boyfriend'], ['father', 'mother'], ['fatherhood', 'motherhood'], ['fathers', 'mothers'], ['fella', 'granny'], ['fraternity', 'sorority'], ['gelding', 'mare'], ['gentleman', 'lady'], ['gentlemen', 'ladies'], ['grandfather', 'grandmother'], ['grandson', 'granddaughter'], ['he', 'she'], ['himself', 'herself'], ['his', 'her'], ['king', 'queen'], ['kings', 'queens'], ['male', 'female'], ['males', 'females'], ['man', 'woman'], ['men', 'women'], ['nephew', 'niece'], ['prince', 'princess'], ['schoolboy', 'schoolgirl'], ['son', 'daughter'], ['sons', 'daughters'], ['twin_brother', 'twin_sister']]\n",
            "gender specific 218 ['actress', 'actresses', 'aunt', 'aunts', 'bachelor', 'ballerina', 'barbershop', 'baritone', 'beard', 'beards']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6zA1bZjmZJp6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fc303b7e-0356-4d49-ec9e-3af1d9463983"
      },
      "cell_type": "code",
      "source": [
        "E = WordEmbedding('w2v_gnews_small.txt')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** Reading data from w2v_gnews_small.txt\n",
            "(26423, 300)\n",
            "26423 words of dimension 300 : in, for, that, is, ..., Jay, Leroy, Brad, Jermaine\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G5Dfl7gtbAU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "de9a178c-bbb0-4b0d-84b6-a2ef5759d31a"
      },
      "cell_type": "code",
      "source": [
        "E1 =  WordEmbedding('../GoogleNews-vectors-negative300.bin')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** Reading data from ../GoogleNews-vectors-negative300.bin\n",
            "(3000000, 300)\n",
            "3000000 words of dimension 300 : </s>, in, for, that, ..., Bim_Skala_Bim, Mezze_Cafe, pulverizes_boulders, snowcapped_Caucasus\n",
            "3000000 words of dimension 300 : </s>, in, for, that, ..., Bim_Skala_Bim, Mezze_Cafe, pulverizes_boulders, snowcapped_Caucasus\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mRuvESCSbCMU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bEzUV_AOtRAd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mu et. al. Hard Debiasing"
      ]
    },
    {
      "metadata": {
        "id": "a0ZvHFuFtV_v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "fb09562d-d9f2-4e4c-f300-231a90784a5a"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "def hard_debias(all_words, subspace):\n",
        "  all_words = np.array(all_words)\n",
        "  subspace = np.array(subspace)\n",
        "  print(all_words.shape, \" \", subspace.shape)\n",
        "  pca = PCA(n_components = 1)\n",
        "  pca.fit(subspace)\n",
        "  pc1 = np.array(pca.components_)\n",
        "  \n",
        "  temp = (pc1.T @ (pc1 @ all_words.T)).T\n",
        "  ret = all_words - temp\n",
        "  \n",
        "  return ret\n",
        "  \n",
        "  \n",
        "subspace = [[1,2,3,4],[11,2,3,12]]\n",
        "\n",
        "all_words = [[1,2,3,4], [5,6,7,8],[21,3,1,4],[11,2,3,12], [6,7,4,3]]\n",
        "print(hard_debias(all_words, subspace))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 4)   (2, 4)\n",
            "[[-1.56097561  2.          3.          1.95121951]\n",
            " [-1.95121951  6.          7.          2.43902439]\n",
            " [ 6.24390244  3.          1.         -7.80487805]\n",
            " [-1.56097561  2.          3.          1.95121951]\n",
            " [ 0.87804878  7.          4.         -1.09756098]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bUi9D076vREC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "e3aae475-794d-4fdf-9a9d-0bdde844a7eb"
      },
      "cell_type": "code",
      "source": [
        "subspace = np.array([1,2,3,4])\n",
        "subspace = subspace.T\n",
        "\n",
        "aaa = np.array([2,4,1,3,10])\n",
        "b = np.multiply(aaa, subspace)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-3268b8333c9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maaa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maaa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubspace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,) (4,) "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ESv2ZU7r1PV4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "outputId": "2c58b60d-d7e3-4242-9c52-799604ffa8ad"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "resourceFile = ''\n",
        "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "all_embd = ['glove', 'word2vec', 'fasttext','elmo']\n",
        "all_subspace = ['without_debiasing', 'gender_list_extended','gender_list_propernouns','gender_list_pronouns', 'gender_list_all']\n",
        "# all_subspace = ['without_debiasing', 'race_list']\n",
        "\n",
        "# science = WEATLists.W_8_Science\n",
        "# arts = WEATLists.W_8_Arts\n",
        "# male = WEATLists.W_8_Male_terms\n",
        "# female = WEATLists.W_8_Female_terms\n",
        "\n",
        "white = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "black = WEATLists.W_5_Unused_full_list_African_American_names\n",
        "pleasant = WEATLists.W_5_Pleasant\n",
        "unpleasant = WEATLists.W_5_Unpleasant\n",
        "\n",
        "#print(career, family, male, female)\n",
        "results = []\n",
        "\n",
        "for embd in all_embd:\n",
        "  #Initialize the embeddings to be used\n",
        "  curr_embd = eval(embd)\n",
        "  \n",
        "  #Load all embeddings in a matrix of all words in the wordlist\n",
        "  if embd == 'elmo' or embd == 'bert':\n",
        "    all_words_mat, all_words_index, _ = pick_embeddings(brown_corpus, curr_embd)\n",
        "    print(\"All mat: \", np.array(all_words_mat).shape)\n",
        "    print(\"Number of words: \", len(list(all_words_index.keys())))\n",
        "  else:\n",
        "    all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)\n",
        "  \n",
        "  for subspace in all_subspace:\n",
        "    \n",
        "    if subspace != 'without_debiasing' and subspace != 'gender_list_and':\n",
        "      subspace_words_list = eval(subspace)\n",
        "\n",
        "    \n",
        "    if subspace != 'without_debiasing':\n",
        "      #Load all embeddings of the subspace as a matrix\n",
        "      if embd == 'elmo' or embd == 'bert':\n",
        "        subspace_words_mat = load_subspace_vectors_contextual(all_words_mat, all_words_index, subspace_words_list)\n",
        "        all_words_cn = hard_debias(all_words_mat, subspace_words_mat)\n",
        "        print(\"Subspace mat: \", np.array(subspace_words_mat).shape)\n",
        "      else:\n",
        "        subspace_words_mat = load_subspace_vectors(curr_embd, subspace_words_list)\n",
        "        all_words_cn = hard_debias(all_words_mat, subspace_words_mat)\n",
        "    else:\n",
        "      all_words_cn = all_words_mat\n",
        "    all_words_cn = np.array(all_words_cn)\n",
        "    print(\"All CN: \", all_words_cn.shape)\n",
        "    #Store all conceptored words in a dictonary\n",
        "    all_words = {}\n",
        "    for word, index in all_words_index.items():\n",
        "      #print(word, index)\n",
        "      if embd == 'elmo' or embd == 'bert':\n",
        "        all_words[word] = np.mean([all_words_cn[i,:] for i in index], axis = 0)\n",
        "      else:\n",
        "        all_words[word] = all_words_cn[index,:]\n",
        "    \n",
        "    if subspace == 'without_debiasing':\n",
        "      #WITHOUT CONCEPTOR\n",
        "      d_cn = weat_effect_size(white, black, pleasant, unpleasant, all_words)\n",
        "      p_cn = weat_p_value(white, black, pleasant, unpleasant, all_words, 1000)\n",
        "      print(\"Without debiasing\")\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    else:\n",
        "      #WITH CONCEPTOR\n",
        "      d_cn = weat_effect_size(white, black, pleasant, unpleasant, all_words)\n",
        "      p_cn = weat_p_value(white, black, pleasant, unpleasant, all_words, 1000)\n",
        "      print(\"With debiasing: \", embd, subspace)\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    \n",
        "    row = [embd, subspace, d_cn, p_cn]\n",
        "    results.append(row)\n",
        "    \n",
        "pd.DataFrame(np.array(results), columns = ['Embedding', 'Subspace', 'Effect Size', 'p-value'])"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All CN:  (128607, 300)\n",
            "Without debiasing\n",
            "WEAT d =  1.3542663\n",
            "WEAT p =  0.0\n",
            "(128607, 300)   (115, 300)\n",
            "All CN:  (128607, 300)\n",
            "With debiasing:  glove race_list\n",
            "WEAT d =  0.73598015\n",
            "WEAT p =  0.005\n",
            "All CN:  (76078, 300)\n",
            "Without debiasing\n",
            "WEAT d =  -0.27154148\n",
            "WEAT p =  0.263\n",
            "(76078, 300)   (117, 300)\n",
            "All CN:  (76078, 300)\n",
            "With debiasing:  word2vec race_list\n",
            "WEAT d =  -0.2778863\n",
            "WEAT p =  0.255\n",
            "All CN:  (119127, 300)\n",
            "Without debiasing\n",
            "WEAT d =  0.41335136\n",
            "WEAT p =  0.039\n",
            "(119127, 300)   (114, 300)\n",
            "All CN:  (119127, 300)\n",
            "With debiasing:  fasttext race_list\n",
            "WEAT d =  0.21699712\n",
            "WEAT p =  0.097\n",
            "All mat:  (1161192, 768)\n",
            "Number of words:  49815\n",
            "All CN:  (1161192, 768)\n",
            "Without debiasing\n",
            "WEAT d =  1.3741117\n",
            "WEAT p =  0.0\n",
            "Subspace:  (751, 768)\n",
            "(1161192, 768)   (751, 768)\n",
            "Subspace mat:  (751, 768)\n",
            "All CN:  (1161192, 768)\n",
            "With debiasing:  elmo race_list\n",
            "WEAT d =  1.0398372\n",
            "WEAT p =  0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Embedding           Subspace  Effect Size p-value\n",
              "0     glove  without_debiasing    1.3542663     0.0\n",
              "1     glove          race_list   0.73598015   0.005\n",
              "2  word2vec  without_debiasing  -0.27154148   0.263\n",
              "3  word2vec          race_list   -0.2778863   0.255\n",
              "4  fasttext  without_debiasing   0.41335136   0.039\n",
              "5  fasttext          race_list   0.21699712   0.097\n",
              "6      elmo  without_debiasing    1.3741117     0.0\n",
              "7      elmo          race_list    1.0398372     0.0"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Embedding</th>\n",
              "      <th>Subspace</th>\n",
              "      <th>Effect Size</th>\n",
              "      <th>p-value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>glove</td>\n",
              "      <td>without_debiasing</td>\n",
              "      <td>1.3542663</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>glove</td>\n",
              "      <td>race_list</td>\n",
              "      <td>0.73598015</td>\n",
              "      <td>0.005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>word2vec</td>\n",
              "      <td>without_debiasing</td>\n",
              "      <td>-0.27154148</td>\n",
              "      <td>0.263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>word2vec</td>\n",
              "      <td>race_list</td>\n",
              "      <td>-0.2778863</td>\n",
              "      <td>0.255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>fasttext</td>\n",
              "      <td>without_debiasing</td>\n",
              "      <td>0.41335136</td>\n",
              "      <td>0.039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>fasttext</td>\n",
              "      <td>race_list</td>\n",
              "      <td>0.21699712</td>\n",
              "      <td>0.097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>elmo</td>\n",
              "      <td>without_debiasing</td>\n",
              "      <td>1.3741117</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>elmo</td>\n",
              "      <td>race_list</td>\n",
              "      <td>1.0398372</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "metadata": {
        "id": "1_nEDSMGNXZB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iP9IM02Ln-qp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bolukbasi hard debiasing"
      ]
    },
    {
      "metadata": {
        "id": "N-5CfftKoGCt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def doPCA(pairs, mat, index, num_components = 5):\n",
        "    matrix = []\n",
        "    for a, b in pairs:\n",
        "        center = (mat[index[a.lower()]] + mat[index[b.lower()]])/2\n",
        "        matrix.append(mat[index[a.lower()]] - center)\n",
        "        matrix.append(mat[index[b.lower()]] - center)\n",
        "    matrix = np.array(matrix)\n",
        "    pca = PCA(n_components = num_components)\n",
        "    pca.fit(matrix)\n",
        "    # bar(range(num_components), pca.explained_variance_ratio_)\n",
        "    return pca\n",
        "\n",
        "def drop(u, v):\n",
        "    return u - v * u.dot(v) / v.dot(v)\n",
        "  \n",
        "def normalize(all_words_mat):\n",
        "    all_words_mat /= np.linalg.norm(all_words_mat, axis=1)[:, np.newaxis]\n",
        "    return all_words_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GWOwX9wHoB-7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def debias(all_words_mat, all_words_index, gender_specific_words, definitional, equalize):\n",
        "    gender_direction = doPCA(definitional, all_words_mat, all_words_index).components_[0]\n",
        "    specific_set = set(gender_specific_words)\n",
        "    for w in list(all_words_index.keys()):\n",
        "        if w not in specific_set:\n",
        "            all_words_mat[all_words_index[w.lower()]] = drop(all_words_mat[all_words_index[w.lower()]], gender_direction)\n",
        "    all_words_mat = normalize(all_words_mat)\n",
        "    candidates = {x for e1, e2 in equalize for x in [(e1.lower(), e2.lower()),\n",
        "                                                     (e1.title(), e2.title()),\n",
        "                                                     (e1.upper(), e2.upper())]}\n",
        "    print(candidates)\n",
        "    for (a, b) in candidates:\n",
        "        if (a.lower() in all_words_index and b.lower() in all_words_index):\n",
        "            y = drop((all_words_mat[all_words_index[a.lower()]] + all_words_mat[all_words_index[b.lower()]]) / 2, gender_direction)\n",
        "            z = np.sqrt(1 - np.linalg.norm(y)**2)\n",
        "            if (all_words_mat[all_words_index[a.lower()]] - all_words_mat[all_words_index[b.lower()]]).dot(gender_direction) < 0:\n",
        "                z = -z\n",
        "            all_words_mat[all_words_index[a.lower()]] = z * gender_direction + y\n",
        "            all_words_mat[all_words_index[b.lower()]] = -z * gender_direction + y\n",
        "    all_words_mat = normalize(all_words_mat)\n",
        "    return all_words_mat\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t9h4VxZfqluU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1979
        },
        "outputId": "b426be22-8cc9-4e7c-e67b-b0acc7a65b0a"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "resourceFile = ''\n",
        "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "%cd debiaswe_tutorial/debiaswe/\n",
        "# Lets load some gender related word lists to help us with debiasing\n",
        "with open('./data/definitional_pairs.json', \"r\") as f:\n",
        "    defs = json.load(f) #gender definitional words\n",
        "print(\"definitional\", defs)\n",
        "defs_list = []\n",
        "for pair in defs:\n",
        "  defs_list.append(pair[0])\n",
        "  defs_list.append(pair[1])\n",
        "\n",
        "with open('./data/equalize_pairs.json', \"r\") as f:\n",
        "    equalize_pairs = json.load(f) \n",
        "print(\"Equalize pairs\", equalize_pairs)\n",
        "\n",
        "%cd ../../\n",
        "!ls\n",
        "\n",
        "all_embd = ['glove', 'word2vec', 'fasttext']\n",
        "all_subspace = ['without_debiasing', 'gender_list_extended','gender_list_propernouns','gender_list_pronouns', 'gender_list_all']\n",
        "#all_subspace = ['without_debiasing', 'race_list']\n",
        "\n",
        "math = WEATLists.W_7_Math\n",
        "arts = WEATLists.W_7_Arts\n",
        "male = WEATLists.W_7_Male_terms\n",
        "female = WEATLists.W_7_Female_terms\n",
        "\n",
        "# white = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "# black = WEATLists.W_5_Unused_full_list_African_American_names\n",
        "# pleasant = WEATLists.W_5_Pleasant\n",
        "# unpleasant = WEATLists.W_5_Unpleasant\n",
        "\n",
        "#print(career, family, male, female)\n",
        "results = []\n",
        "\n",
        "for embd in all_embd:\n",
        "  #Initialize the embeddings to be used\n",
        "  curr_embd = eval(embd)\n",
        "  \n",
        "  #Load all embeddings in a matrix of all words in the wordlist\n",
        "  if embd == 'elmo' or embd == 'bert':\n",
        "    all_words_mat, all_words_index, _ = pick_embeddings(brown_corpus, curr_embd)\n",
        "    print(\"All mat: \", np.array(all_words_mat).shape)\n",
        "    print(\"Number of words: \", len(list(all_words_index.keys())))\n",
        "  else:\n",
        "    all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)\n",
        "  \n",
        "  for subspace in all_subspace:\n",
        "    \n",
        "    if subspace != 'without_debiasing' and subspace != 'gender_list_and':\n",
        "      subspace_words_list = eval(subspace)\n",
        "\n",
        "    \n",
        "    if subspace != 'without_debiasing':\n",
        "      #Load all embeddings of the subspace as a matrix\n",
        "      if embd == 'elmo' or embd == 'bert':\n",
        "        #subspace_words_mat = load_subspace_vectors_contextual(all_words_mat, all_words_index, subspace_words_list)\n",
        "        all_words_cn = debias(all_words_mat, all_words_index, subspace_words_list, defs, equalize_pairs)\n",
        "        print(\"Subspace mat: \", np.array(subspace_words_mat).shape)\n",
        "      else:\n",
        "        #subspace_words_mat = load_subspace_vectors(curr_embd, subspace_words_list)\n",
        "        all_words_cn = debias(all_words_mat, all_words_index, subspace_words_list, defs, equalize_pairs)\n",
        "    else:\n",
        "      all_words_cn = all_words_mat\n",
        "    all_words_cn = np.array(all_words_cn)\n",
        "    print(\"All CN: \", all_words_cn.shape)\n",
        "    #Store all conceptored words in a dictonary\n",
        "    all_words = {}\n",
        "    for word, index in all_words_index.items():\n",
        "      #print(word, index)\n",
        "      if embd == 'elmo' or embd == 'bert':\n",
        "        all_words[word] = np.mean([all_words_cn[i,:] for i in index], axis = 0)\n",
        "      else:\n",
        "        all_words[word] = all_words_cn[index,:]\n",
        "    \n",
        "    if subspace == 'without_debiasing':\n",
        "      #WITHOUT CONCEPTOR\n",
        "      d_cn = weat_effect_size(math, arts, male, female, all_words)\n",
        "      p_cn = weat_p_value(math, arts, male, female, all_words, 1000)\n",
        "      print(\"Without debiasing\")\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    else:\n",
        "      #WITH CONCEPTOR\n",
        "      d_cn = weat_effect_size(math, arts, male, female, all_words)\n",
        "      p_cn = weat_p_value(math, arts, male, female, all_words, 1000)\n",
        "      print(\"With debiasing: \", embd, subspace)\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    \n",
        "    row = [embd, subspace, d_cn, p_cn]\n",
        "    results.append(row)\n",
        "    \n",
        "pd.DataFrame(np.array(results), columns = ['Embedding', 'Subspace', 'Effect Size', 'p-value'])"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/saketk/content/debiaswe_tutorial/debiaswe\n",
            "definitional [['woman', 'man'], ['girl', 'boy'], ['she', 'he'], ['mother', 'father'], ['daughter', 'son'], ['gal', 'guy'], ['female', 'male'], ['her', 'his'], ['herself', 'himself'], ['Mary', 'John']]\n",
            "Equalize pairs [['monastery', 'convent'], ['spokesman', 'spokeswoman'], ['Catholic_priest', 'nun'], ['Dad', 'Mom'], ['Men', 'Women'], ['councilman', 'councilwoman'], ['grandpa', 'grandma'], ['grandsons', 'granddaughters'], ['prostate_cancer', 'ovarian_cancer'], ['testosterone', 'estrogen'], ['uncle', 'aunt'], ['wives', 'husbands'], ['Father', 'Mother'], ['Grandpa', 'Grandma'], ['He', 'She'], ['boy', 'girl'], ['boys', 'girls'], ['brother', 'sister'], ['brothers', 'sisters'], ['businessman', 'businesswoman'], ['chairman', 'chairwoman'], ['colt', 'filly'], ['congressman', 'congresswoman'], ['dad', 'mom'], ['dads', 'moms'], ['dudes', 'gals'], ['ex_girlfriend', 'ex_boyfriend'], ['father', 'mother'], ['fatherhood', 'motherhood'], ['fathers', 'mothers'], ['fella', 'granny'], ['fraternity', 'sorority'], ['gelding', 'mare'], ['gentleman', 'lady'], ['gentlemen', 'ladies'], ['grandfather', 'grandmother'], ['grandson', 'granddaughter'], ['he', 'she'], ['himself', 'herself'], ['his', 'her'], ['king', 'queen'], ['kings', 'queens'], ['male', 'female'], ['males', 'females'], ['man', 'woman'], ['men', 'women'], ['nephew', 'niece'], ['prince', 'princess'], ['schoolboy', 'schoolgirl'], ['son', 'daughter'], ['sons', 'daughters'], ['twin_brother', 'twin_sister']]\n",
            "/home/saketk/content\n",
            "ConceptorDebias\t\t\t       female.txt.2\n",
            "corefBias\t\t\t       gensim_glove.840B.300d.txt.bin\n",
            "debiaswe_tutorial\t\t       gn_glove\n",
            "enwiki-20150602-words-frequency.txt    GoogleNews-vectors-negative300.bin\n",
            "enwiki-20150602-words-frequency.txt.1  male.txt\n",
            "enwiki-20150602-words-frequency.txt.2  male.txt.1\n",
            "fasttext.bin\t\t\t       male.txt.2\n",
            "female.txt\t\t\t       SIF\n",
            "female.txt.1\n",
            "All CN:  (128607, 300)\n",
            "Without debiasing\n",
            "WEAT d =  1.0896145\n",
            "WEAT p =  0.019\n",
            "{('GENTLEMEN', 'LADIES'), ('Catholic_Priest', 'Nun'), ('Testosterone', 'Estrogen'), ('Monastery', 'Convent'), ('male', 'female'), ('Fraternity', 'Sorority'), ('Grandsons', 'Granddaughters'), ('PRINCE', 'PRINCESS'), ('monastery', 'convent'), ('his', 'her'), ('DAD', 'MOM'), ('Dads', 'Moms'), ('brother', 'sister'), ('UNCLE', 'AUNT'), ('GENTLEMAN', 'LADY'), ('prince', 'princess'), ('Son', 'Daughter'), ('Himself', 'Herself'), ('SCHOOLBOY', 'SCHOOLGIRL'), ('dudes', 'gals'), ('Grandfather', 'Grandmother'), ('Congressman', 'Congresswoman'), ('BROTHER', 'SISTER'), ('Nephew', 'Niece'), ('twin_brother', 'twin_sister'), ('GRANDFATHER', 'GRANDMOTHER'), ('Wives', 'Husbands'), ('grandsons', 'granddaughters'), ('Grandson', 'Granddaughter'), ('Males', 'Females'), ('Gelding', 'Mare'), ('schoolboy', 'schoolgirl'), ('HIS', 'HER'), ('He', 'She'), ('PROSTATE_CANCER', 'OVARIAN_CANCER'), ('SONS', 'DAUGHTERS'), ('Businessman', 'Businesswoman'), ('FATHERS', 'MOTHERS'), ('Fella', 'Granny'), ('uncle', 'aunt'), ('BOY', 'GIRL'), ('gelding', 'mare'), ('DUDES', 'GALS'), ('Prince', 'Princess'), ('FELLA', 'GRANNY'), ('kings', 'queens'), ('FATHERHOOD', 'MOTHERHOOD'), ('COLT', 'FILLY'), ('Boy', 'Girl'), ('himself', 'herself'), ('BUSINESSMAN', 'BUSINESSWOMAN'), ('grandson', 'granddaughter'), ('HE', 'SHE'), ('CATHOLIC_PRIEST', 'NUN'), ('Dudes', 'Gals'), ('spokesman', 'spokeswoman'), ('MALE', 'FEMALE'), ('Councilman', 'Councilwoman'), ('testosterone', 'estrogen'), ('Fathers', 'Mothers'), ('councilman', 'councilwoman'), ('Prostate_Cancer', 'Ovarian_Cancer'), ('businessman', 'businesswoman'), ('Schoolboy', 'Schoolgirl'), ('Man', 'Woman'), ('Boys', 'Girls'), ('SPOKESMAN', 'SPOKESWOMAN'), ('Uncle', 'Aunt'), ('fella', 'granny'), ('HIMSELF', 'HERSELF'), ('GRANDSONS', 'GRANDDAUGHTERS'), ('His', 'Her'), ('catholic_priest', 'nun'), ('Gentlemen', 'Ladies'), ('King', 'Queen'), ('congressman', 'congresswoman'), ('WIVES', 'HUSBANDS'), ('MONASTERY', 'CONVENT'), ('son', 'daughter'), ('Fatherhood', 'Motherhood'), ('dad', 'mom'), ('MAN', 'WOMAN'), ('gentlemen', 'ladies'), ('DADS', 'MOMS'), ('ex_girlfriend', 'ex_boyfriend'), ('Chairman', 'Chairwoman'), ('TESTOSTERONE', 'ESTROGEN'), ('Gentleman', 'Lady'), ('MEN', 'WOMEN'), ('EX_GIRLFRIEND', 'EX_BOYFRIEND'), ('Ex_Girlfriend', 'Ex_Boyfriend'), ('SON', 'DAUGHTER'), ('COUNCILMAN', 'COUNCILWOMAN'), ('GRANDPA', 'GRANDMA'), ('king', 'queen'), ('brothers', 'sisters'), ('males', 'females'), ('Brother', 'Sister'), ('Sons', 'Daughters'), ('KING', 'QUEEN'), ('Dad', 'Mom'), ('fatherhood', 'motherhood'), ('FRATERNITY', 'SORORITY'), ('Kings', 'Queens'), ('CONGRESSMAN', 'CONGRESSWOMAN'), ('Colt', 'Filly'), ('Men', 'Women'), ('nephew', 'niece'), ('colt', 'filly'), ('grandpa', 'grandma'), ('GRANDSON', 'GRANDDAUGHTER'), ('men', 'women'), ('KINGS', 'QUEENS'), ('BOYS', 'GIRLS'), ('wives', 'husbands'), ('fraternity', 'sorority'), ('BROTHERS', 'SISTERS'), ('MALES', 'FEMALES'), ('man', 'woman'), ('boys', 'girls'), ('TWIN_BROTHER', 'TWIN_SISTER'), ('Father', 'Mother'), ('chairman', 'chairwoman'), ('FATHER', 'MOTHER'), ('Grandpa', 'Grandma'), ('sons', 'daughters'), ('Spokesman', 'Spokeswoman'), ('he', 'she'), ('father', 'mother'), ('GELDING', 'MARE'), ('dads', 'moms'), ('gentleman', 'lady'), ('fathers', 'mothers'), ('NEPHEW', 'NIECE'), ('grandfather', 'grandmother'), ('Male', 'Female'), ('Brothers', 'Sisters'), ('CHAIRMAN', 'CHAIRWOMAN'), ('Twin_Brother', 'Twin_Sister'), ('prostate_cancer', 'ovarian_cancer'), ('boy', 'girl')}\n",
            "All CN:  (128607, 300)\n",
            "With debiasing:  glove gender_list_extended\n",
            "WEAT d =  -0.60132134\n",
            "WEAT p =  0.863\n",
            "{('GENTLEMEN', 'LADIES'), ('Catholic_Priest', 'Nun'), ('Testosterone', 'Estrogen'), ('Monastery', 'Convent'), ('male', 'female'), ('Fraternity', 'Sorority'), ('Grandsons', 'Granddaughters'), ('PRINCE', 'PRINCESS'), ('monastery', 'convent'), ('his', 'her'), ('DAD', 'MOM'), ('Dads', 'Moms'), ('brother', 'sister'), ('UNCLE', 'AUNT'), ('GENTLEMAN', 'LADY'), ('prince', 'princess'), ('Son', 'Daughter'), ('Himself', 'Herself'), ('SCHOOLBOY', 'SCHOOLGIRL'), ('dudes', 'gals'), ('Grandfather', 'Grandmother'), ('Congressman', 'Congresswoman'), ('BROTHER', 'SISTER'), ('Nephew', 'Niece'), ('twin_brother', 'twin_sister'), ('GRANDFATHER', 'GRANDMOTHER'), ('Wives', 'Husbands'), ('grandsons', 'granddaughters'), ('Grandson', 'Granddaughter'), ('Males', 'Females'), ('Gelding', 'Mare'), ('schoolboy', 'schoolgirl'), ('HIS', 'HER'), ('He', 'She'), ('PROSTATE_CANCER', 'OVARIAN_CANCER'), ('SONS', 'DAUGHTERS'), ('Businessman', 'Businesswoman'), ('FATHERS', 'MOTHERS'), ('Fella', 'Granny'), ('uncle', 'aunt'), ('BOY', 'GIRL'), ('gelding', 'mare'), ('DUDES', 'GALS'), ('Prince', 'Princess'), ('FELLA', 'GRANNY'), ('kings', 'queens'), ('FATHERHOOD', 'MOTHERHOOD'), ('COLT', 'FILLY'), ('Boy', 'Girl'), ('himself', 'herself'), ('BUSINESSMAN', 'BUSINESSWOMAN'), ('grandson', 'granddaughter'), ('HE', 'SHE'), ('CATHOLIC_PRIEST', 'NUN'), ('Dudes', 'Gals'), ('spokesman', 'spokeswoman'), ('MALE', 'FEMALE'), ('Councilman', 'Councilwoman'), ('testosterone', 'estrogen'), ('Fathers', 'Mothers'), ('councilman', 'councilwoman'), ('Prostate_Cancer', 'Ovarian_Cancer'), ('businessman', 'businesswoman'), ('Schoolboy', 'Schoolgirl'), ('Man', 'Woman'), ('Boys', 'Girls'), ('SPOKESMAN', 'SPOKESWOMAN'), ('Uncle', 'Aunt'), ('fella', 'granny'), ('HIMSELF', 'HERSELF'), ('GRANDSONS', 'GRANDDAUGHTERS'), ('His', 'Her'), ('catholic_priest', 'nun'), ('Gentlemen', 'Ladies'), ('King', 'Queen'), ('congressman', 'congresswoman'), ('WIVES', 'HUSBANDS'), ('MONASTERY', 'CONVENT'), ('son', 'daughter'), ('Fatherhood', 'Motherhood'), ('dad', 'mom'), ('MAN', 'WOMAN'), ('gentlemen', 'ladies'), ('DADS', 'MOMS'), ('ex_girlfriend', 'ex_boyfriend'), ('Chairman', 'Chairwoman'), ('TESTOSTERONE', 'ESTROGEN'), ('Gentleman', 'Lady'), ('MEN', 'WOMEN'), ('EX_GIRLFRIEND', 'EX_BOYFRIEND'), ('Ex_Girlfriend', 'Ex_Boyfriend'), ('SON', 'DAUGHTER'), ('COUNCILMAN', 'COUNCILWOMAN'), ('GRANDPA', 'GRANDMA'), ('king', 'queen'), ('brothers', 'sisters'), ('males', 'females'), ('Brother', 'Sister'), ('Sons', 'Daughters'), ('KING', 'QUEEN'), ('Dad', 'Mom'), ('fatherhood', 'motherhood'), ('FRATERNITY', 'SORORITY'), ('Kings', 'Queens'), ('CONGRESSMAN', 'CONGRESSWOMAN'), ('Colt', 'Filly'), ('Men', 'Women'), ('nephew', 'niece'), ('colt', 'filly'), ('grandpa', 'grandma'), ('GRANDSON', 'GRANDDAUGHTER'), ('men', 'women'), ('KINGS', 'QUEENS'), ('BOYS', 'GIRLS'), ('wives', 'husbands'), ('fraternity', 'sorority'), ('BROTHERS', 'SISTERS'), ('MALES', 'FEMALES'), ('man', 'woman'), ('boys', 'girls'), ('TWIN_BROTHER', 'TWIN_SISTER'), ('Father', 'Mother'), ('chairman', 'chairwoman'), ('FATHER', 'MOTHER'), ('Grandpa', 'Grandma'), ('sons', 'daughters'), ('Spokesman', 'Spokeswoman'), ('he', 'she'), ('father', 'mother'), ('GELDING', 'MARE'), ('dads', 'moms'), ('gentleman', 'lady'), ('fathers', 'mothers'), ('NEPHEW', 'NIECE'), ('grandfather', 'grandmother'), ('Male', 'Female'), ('Brothers', 'Sisters'), ('CHAIRMAN', 'CHAIRWOMAN'), ('Twin_Brother', 'Twin_Sister'), ('prostate_cancer', 'ovarian_cancer'), ('boy', 'girl')}\n",
            "All CN:  (128607, 300)\n",
            "With debiasing:  glove gender_list_propernouns\n",
            "WEAT d =  -0.57953775\n",
            "WEAT p =  0.864\n",
            "{('GENTLEMEN', 'LADIES'), ('Catholic_Priest', 'Nun'), ('Testosterone', 'Estrogen'), ('Monastery', 'Convent'), ('male', 'female'), ('Fraternity', 'Sorority'), ('Grandsons', 'Granddaughters'), ('PRINCE', 'PRINCESS'), ('monastery', 'convent'), ('his', 'her'), ('DAD', 'MOM'), ('Dads', 'Moms'), ('brother', 'sister'), ('UNCLE', 'AUNT'), ('GENTLEMAN', 'LADY'), ('prince', 'princess'), ('Son', 'Daughter'), ('Himself', 'Herself'), ('SCHOOLBOY', 'SCHOOLGIRL'), ('dudes', 'gals'), ('Grandfather', 'Grandmother'), ('Congressman', 'Congresswoman'), ('BROTHER', 'SISTER'), ('Nephew', 'Niece'), ('twin_brother', 'twin_sister'), ('GRANDFATHER', 'GRANDMOTHER'), ('Wives', 'Husbands'), ('grandsons', 'granddaughters'), ('Grandson', 'Granddaughter'), ('Males', 'Females'), ('Gelding', 'Mare'), ('schoolboy', 'schoolgirl'), ('HIS', 'HER'), ('He', 'She'), ('PROSTATE_CANCER', 'OVARIAN_CANCER'), ('SONS', 'DAUGHTERS'), ('Businessman', 'Businesswoman'), ('FATHERS', 'MOTHERS'), ('Fella', 'Granny'), ('uncle', 'aunt'), ('BOY', 'GIRL'), ('gelding', 'mare'), ('DUDES', 'GALS'), ('Prince', 'Princess'), ('FELLA', 'GRANNY'), ('kings', 'queens'), ('FATHERHOOD', 'MOTHERHOOD'), ('COLT', 'FILLY'), ('Boy', 'Girl'), ('himself', 'herself'), ('BUSINESSMAN', 'BUSINESSWOMAN'), ('grandson', 'granddaughter'), ('HE', 'SHE'), ('CATHOLIC_PRIEST', 'NUN'), ('Dudes', 'Gals'), ('spokesman', 'spokeswoman'), ('MALE', 'FEMALE'), ('Councilman', 'Councilwoman'), ('testosterone', 'estrogen'), ('Fathers', 'Mothers'), ('councilman', 'councilwoman'), ('Prostate_Cancer', 'Ovarian_Cancer'), ('businessman', 'businesswoman'), ('Schoolboy', 'Schoolgirl'), ('Man', 'Woman'), ('Boys', 'Girls'), ('SPOKESMAN', 'SPOKESWOMAN'), ('Uncle', 'Aunt'), ('fella', 'granny'), ('HIMSELF', 'HERSELF'), ('GRANDSONS', 'GRANDDAUGHTERS'), ('His', 'Her'), ('catholic_priest', 'nun'), ('Gentlemen', 'Ladies'), ('King', 'Queen'), ('congressman', 'congresswoman'), ('WIVES', 'HUSBANDS'), ('MONASTERY', 'CONVENT'), ('son', 'daughter'), ('Fatherhood', 'Motherhood'), ('dad', 'mom'), ('MAN', 'WOMAN'), ('gentlemen', 'ladies'), ('DADS', 'MOMS'), ('ex_girlfriend', 'ex_boyfriend'), ('Chairman', 'Chairwoman'), ('TESTOSTERONE', 'ESTROGEN'), ('Gentleman', 'Lady'), ('MEN', 'WOMEN'), ('EX_GIRLFRIEND', 'EX_BOYFRIEND'), ('Ex_Girlfriend', 'Ex_Boyfriend'), ('SON', 'DAUGHTER'), ('COUNCILMAN', 'COUNCILWOMAN'), ('GRANDPA', 'GRANDMA'), ('king', 'queen'), ('brothers', 'sisters'), ('males', 'females'), ('Brother', 'Sister'), ('Sons', 'Daughters'), ('KING', 'QUEEN'), ('Dad', 'Mom'), ('fatherhood', 'motherhood'), ('FRATERNITY', 'SORORITY'), ('Kings', 'Queens'), ('CONGRESSMAN', 'CONGRESSWOMAN'), ('Colt', 'Filly'), ('Men', 'Women'), ('nephew', 'niece'), ('colt', 'filly'), ('grandpa', 'grandma'), ('GRANDSON', 'GRANDDAUGHTER'), ('men', 'women'), ('KINGS', 'QUEENS'), ('BOYS', 'GIRLS'), ('wives', 'husbands'), ('fraternity', 'sorority'), ('BROTHERS', 'SISTERS'), ('MALES', 'FEMALES'), ('man', 'woman'), ('boys', 'girls'), ('TWIN_BROTHER', 'TWIN_SISTER'), ('Father', 'Mother'), ('chairman', 'chairwoman'), ('FATHER', 'MOTHER'), ('Grandpa', 'Grandma'), ('sons', 'daughters'), ('Spokesman', 'Spokeswoman'), ('he', 'she'), ('father', 'mother'), ('GELDING', 'MARE'), ('dads', 'moms'), ('gentleman', 'lady'), ('fathers', 'mothers'), ('NEPHEW', 'NIECE'), ('grandfather', 'grandmother'), ('Male', 'Female'), ('Brothers', 'Sisters'), ('CHAIRMAN', 'CHAIRWOMAN'), ('Twin_Brother', 'Twin_Sister'), ('prostate_cancer', 'ovarian_cancer'), ('boy', 'girl')}\n",
            "All CN:  (128607, 300)\n",
            "With debiasing:  glove gender_list_pronouns\n",
            "WEAT d =  -0.5332187\n",
            "WEAT p =  0.845\n",
            "{('GENTLEMEN', 'LADIES'), ('Catholic_Priest', 'Nun'), ('Testosterone', 'Estrogen'), ('Monastery', 'Convent'), ('male', 'female'), ('Fraternity', 'Sorority'), ('Grandsons', 'Granddaughters'), ('PRINCE', 'PRINCESS'), ('monastery', 'convent'), ('his', 'her'), ('DAD', 'MOM'), ('Dads', 'Moms'), ('brother', 'sister'), ('UNCLE', 'AUNT'), ('GENTLEMAN', 'LADY'), ('prince', 'princess'), ('Son', 'Daughter'), ('Himself', 'Herself'), ('SCHOOLBOY', 'SCHOOLGIRL'), ('dudes', 'gals'), ('Grandfather', 'Grandmother'), ('Congressman', 'Congresswoman'), ('BROTHER', 'SISTER'), ('Nephew', 'Niece'), ('twin_brother', 'twin_sister'), ('GRANDFATHER', 'GRANDMOTHER'), ('Wives', 'Husbands'), ('grandsons', 'granddaughters'), ('Grandson', 'Granddaughter'), ('Males', 'Females'), ('Gelding', 'Mare'), ('schoolboy', 'schoolgirl'), ('HIS', 'HER'), ('He', 'She'), ('PROSTATE_CANCER', 'OVARIAN_CANCER'), ('SONS', 'DAUGHTERS'), ('Businessman', 'Businesswoman'), ('FATHERS', 'MOTHERS'), ('Fella', 'Granny'), ('uncle', 'aunt'), ('BOY', 'GIRL'), ('gelding', 'mare'), ('DUDES', 'GALS'), ('Prince', 'Princess'), ('FELLA', 'GRANNY'), ('kings', 'queens'), ('FATHERHOOD', 'MOTHERHOOD'), ('COLT', 'FILLY'), ('Boy', 'Girl'), ('himself', 'herself'), ('BUSINESSMAN', 'BUSINESSWOMAN'), ('grandson', 'granddaughter'), ('HE', 'SHE'), ('CATHOLIC_PRIEST', 'NUN'), ('Dudes', 'Gals'), ('spokesman', 'spokeswoman'), ('MALE', 'FEMALE'), ('Councilman', 'Councilwoman'), ('testosterone', 'estrogen'), ('Fathers', 'Mothers'), ('councilman', 'councilwoman'), ('Prostate_Cancer', 'Ovarian_Cancer'), ('businessman', 'businesswoman'), ('Schoolboy', 'Schoolgirl'), ('Man', 'Woman'), ('Boys', 'Girls'), ('SPOKESMAN', 'SPOKESWOMAN'), ('Uncle', 'Aunt'), ('fella', 'granny'), ('HIMSELF', 'HERSELF'), ('GRANDSONS', 'GRANDDAUGHTERS'), ('His', 'Her'), ('catholic_priest', 'nun'), ('Gentlemen', 'Ladies'), ('King', 'Queen'), ('congressman', 'congresswoman'), ('WIVES', 'HUSBANDS'), ('MONASTERY', 'CONVENT'), ('son', 'daughter'), ('Fatherhood', 'Motherhood'), ('dad', 'mom'), ('MAN', 'WOMAN'), ('gentlemen', 'ladies'), ('DADS', 'MOMS'), ('ex_girlfriend', 'ex_boyfriend'), ('Chairman', 'Chairwoman'), ('TESTOSTERONE', 'ESTROGEN'), ('Gentleman', 'Lady'), ('MEN', 'WOMEN'), ('EX_GIRLFRIEND', 'EX_BOYFRIEND'), ('Ex_Girlfriend', 'Ex_Boyfriend'), ('SON', 'DAUGHTER'), ('COUNCILMAN', 'COUNCILWOMAN'), ('GRANDPA', 'GRANDMA'), ('king', 'queen'), ('brothers', 'sisters'), ('males', 'females'), ('Brother', 'Sister'), ('Sons', 'Daughters'), ('KING', 'QUEEN'), ('Dad', 'Mom'), ('fatherhood', 'motherhood'), ('FRATERNITY', 'SORORITY'), ('Kings', 'Queens'), ('CONGRESSMAN', 'CONGRESSWOMAN'), ('Colt', 'Filly'), ('Men', 'Women'), ('nephew', 'niece'), ('colt', 'filly'), ('grandpa', 'grandma'), ('GRANDSON', 'GRANDDAUGHTER'), ('men', 'women'), ('KINGS', 'QUEENS'), ('BOYS', 'GIRLS'), ('wives', 'husbands'), ('fraternity', 'sorority'), ('BROTHERS', 'SISTERS'), ('MALES', 'FEMALES'), ('man', 'woman'), ('boys', 'girls'), ('TWIN_BROTHER', 'TWIN_SISTER'), ('Father', 'Mother'), ('chairman', 'chairwoman'), ('FATHER', 'MOTHER'), ('Grandpa', 'Grandma'), ('sons', 'daughters'), ('Spokesman', 'Spokeswoman'), ('he', 'she'), ('father', 'mother'), ('GELDING', 'MARE'), ('dads', 'moms'), ('gentleman', 'lady'), ('fathers', 'mothers'), ('NEPHEW', 'NIECE'), ('grandfather', 'grandmother'), ('Male', 'Female'), ('Brothers', 'Sisters'), ('CHAIRMAN', 'CHAIRWOMAN'), ('Twin_Brother', 'Twin_Sister'), ('prostate_cancer', 'ovarian_cancer'), ('boy', 'girl')}\n",
            "All CN:  (128607, 300)\n",
            "With debiasing:  glove gender_list_all\n",
            "WEAT d =  -0.5322254\n",
            "WEAT p =  0.821\n",
            "All CN:  (76078, 300)\n",
            "Without debiasing\n",
            "WEAT d =  0.99810755\n",
            "WEAT p =  0.022\n",
            "{('GENTLEMEN', 'LADIES'), ('Catholic_Priest', 'Nun'), ('Testosterone', 'Estrogen'), ('Monastery', 'Convent'), ('male', 'female'), ('Fraternity', 'Sorority'), ('Grandsons', 'Granddaughters'), ('PRINCE', 'PRINCESS'), ('monastery', 'convent'), ('his', 'her'), ('DAD', 'MOM'), ('Dads', 'Moms'), ('brother', 'sister'), ('UNCLE', 'AUNT'), ('GENTLEMAN', 'LADY'), ('prince', 'princess'), ('Son', 'Daughter'), ('Himself', 'Herself'), ('SCHOOLBOY', 'SCHOOLGIRL'), ('dudes', 'gals'), ('Grandfather', 'Grandmother'), ('Congressman', 'Congresswoman'), ('BROTHER', 'SISTER'), ('Nephew', 'Niece'), ('twin_brother', 'twin_sister'), ('GRANDFATHER', 'GRANDMOTHER'), ('Wives', 'Husbands'), ('grandsons', 'granddaughters'), ('Grandson', 'Granddaughter'), ('Males', 'Females'), ('Gelding', 'Mare'), ('schoolboy', 'schoolgirl'), ('HIS', 'HER'), ('He', 'She'), ('PROSTATE_CANCER', 'OVARIAN_CANCER'), ('SONS', 'DAUGHTERS'), ('Businessman', 'Businesswoman'), ('FATHERS', 'MOTHERS'), ('Fella', 'Granny'), ('uncle', 'aunt'), ('BOY', 'GIRL'), ('gelding', 'mare'), ('DUDES', 'GALS'), ('Prince', 'Princess'), ('FELLA', 'GRANNY'), ('kings', 'queens'), ('FATHERHOOD', 'MOTHERHOOD'), ('COLT', 'FILLY'), ('Boy', 'Girl'), ('himself', 'herself'), ('BUSINESSMAN', 'BUSINESSWOMAN'), ('grandson', 'granddaughter'), ('HE', 'SHE'), ('CATHOLIC_PRIEST', 'NUN'), ('Dudes', 'Gals'), ('spokesman', 'spokeswoman'), ('MALE', 'FEMALE'), ('Councilman', 'Councilwoman'), ('testosterone', 'estrogen'), ('Fathers', 'Mothers'), ('councilman', 'councilwoman'), ('Prostate_Cancer', 'Ovarian_Cancer'), ('businessman', 'businesswoman'), ('Schoolboy', 'Schoolgirl'), ('Man', 'Woman'), ('Boys', 'Girls'), ('SPOKESMAN', 'SPOKESWOMAN'), ('Uncle', 'Aunt'), ('fella', 'granny'), ('HIMSELF', 'HERSELF'), ('GRANDSONS', 'GRANDDAUGHTERS'), ('His', 'Her'), ('catholic_priest', 'nun'), ('Gentlemen', 'Ladies'), ('King', 'Queen'), ('congressman', 'congresswoman'), ('WIVES', 'HUSBANDS'), ('MONASTERY', 'CONVENT'), ('son', 'daughter'), ('Fatherhood', 'Motherhood'), ('dad', 'mom'), ('MAN', 'WOMAN'), ('gentlemen', 'ladies'), ('DADS', 'MOMS'), ('ex_girlfriend', 'ex_boyfriend'), ('Chairman', 'Chairwoman'), ('TESTOSTERONE', 'ESTROGEN'), ('Gentleman', 'Lady'), ('MEN', 'WOMEN'), ('EX_GIRLFRIEND', 'EX_BOYFRIEND'), ('Ex_Girlfriend', 'Ex_Boyfriend'), ('SON', 'DAUGHTER'), ('COUNCILMAN', 'COUNCILWOMAN'), ('GRANDPA', 'GRANDMA'), ('king', 'queen'), ('brothers', 'sisters'), ('males', 'females'), ('Brother', 'Sister'), ('Sons', 'Daughters'), ('KING', 'QUEEN'), ('Dad', 'Mom'), ('fatherhood', 'motherhood'), ('FRATERNITY', 'SORORITY'), ('Kings', 'Queens'), ('CONGRESSMAN', 'CONGRESSWOMAN'), ('Colt', 'Filly'), ('Men', 'Women'), ('nephew', 'niece'), ('colt', 'filly'), ('grandpa', 'grandma'), ('GRANDSON', 'GRANDDAUGHTER'), ('men', 'women'), ('KINGS', 'QUEENS'), ('BOYS', 'GIRLS'), ('wives', 'husbands'), ('fraternity', 'sorority'), ('BROTHERS', 'SISTERS'), ('MALES', 'FEMALES'), ('man', 'woman'), ('boys', 'girls'), ('TWIN_BROTHER', 'TWIN_SISTER'), ('Father', 'Mother'), ('chairman', 'chairwoman'), ('FATHER', 'MOTHER'), ('Grandpa', 'Grandma'), ('sons', 'daughters'), ('Spokesman', 'Spokeswoman'), ('he', 'she'), ('father', 'mother'), ('GELDING', 'MARE'), ('dads', 'moms'), ('gentleman', 'lady'), ('fathers', 'mothers'), ('NEPHEW', 'NIECE'), ('grandfather', 'grandmother'), ('Male', 'Female'), ('Brothers', 'Sisters'), ('CHAIRMAN', 'CHAIRWOMAN'), ('Twin_Brother', 'Twin_Sister'), ('prostate_cancer', 'ovarian_cancer'), ('boy', 'girl')}\n",
            "All CN:  (76078, 300)\n",
            "With debiasing:  word2vec gender_list_extended\n",
            "WEAT d =  -1.1440278\n",
            "WEAT p =  0.995\n",
            "{('GENTLEMEN', 'LADIES'), ('Catholic_Priest', 'Nun'), ('Testosterone', 'Estrogen'), ('Monastery', 'Convent'), ('male', 'female'), ('Fraternity', 'Sorority'), ('Grandsons', 'Granddaughters'), ('PRINCE', 'PRINCESS'), ('monastery', 'convent'), ('his', 'her'), ('DAD', 'MOM'), ('Dads', 'Moms'), ('brother', 'sister'), ('UNCLE', 'AUNT'), ('GENTLEMAN', 'LADY'), ('prince', 'princess'), ('Son', 'Daughter'), ('Himself', 'Herself'), ('SCHOOLBOY', 'SCHOOLGIRL'), ('dudes', 'gals'), ('Grandfather', 'Grandmother'), ('Congressman', 'Congresswoman'), ('BROTHER', 'SISTER'), ('Nephew', 'Niece'), ('twin_brother', 'twin_sister'), ('GRANDFATHER', 'GRANDMOTHER'), ('Wives', 'Husbands'), ('grandsons', 'granddaughters'), ('Grandson', 'Granddaughter'), ('Males', 'Females'), ('Gelding', 'Mare'), ('schoolboy', 'schoolgirl'), ('HIS', 'HER'), ('He', 'She'), ('PROSTATE_CANCER', 'OVARIAN_CANCER'), ('SONS', 'DAUGHTERS'), ('Businessman', 'Businesswoman'), ('FATHERS', 'MOTHERS'), ('Fella', 'Granny'), ('uncle', 'aunt'), ('BOY', 'GIRL'), ('gelding', 'mare'), ('DUDES', 'GALS'), ('Prince', 'Princess'), ('FELLA', 'GRANNY'), ('kings', 'queens'), ('FATHERHOOD', 'MOTHERHOOD'), ('COLT', 'FILLY'), ('Boy', 'Girl'), ('himself', 'herself'), ('BUSINESSMAN', 'BUSINESSWOMAN'), ('grandson', 'granddaughter'), ('HE', 'SHE'), ('CATHOLIC_PRIEST', 'NUN'), ('Dudes', 'Gals'), ('spokesman', 'spokeswoman'), ('MALE', 'FEMALE'), ('Councilman', 'Councilwoman'), ('testosterone', 'estrogen'), ('Fathers', 'Mothers'), ('councilman', 'councilwoman'), ('Prostate_Cancer', 'Ovarian_Cancer'), ('businessman', 'businesswoman'), ('Schoolboy', 'Schoolgirl'), ('Man', 'Woman'), ('Boys', 'Girls'), ('SPOKESMAN', 'SPOKESWOMAN'), ('Uncle', 'Aunt'), ('fella', 'granny'), ('HIMSELF', 'HERSELF'), ('GRANDSONS', 'GRANDDAUGHTERS'), ('His', 'Her'), ('catholic_priest', 'nun'), ('Gentlemen', 'Ladies'), ('King', 'Queen'), ('congressman', 'congresswoman'), ('WIVES', 'HUSBANDS'), ('MONASTERY', 'CONVENT'), ('son', 'daughter'), ('Fatherhood', 'Motherhood'), ('dad', 'mom'), ('MAN', 'WOMAN'), ('gentlemen', 'ladies'), ('DADS', 'MOMS'), ('ex_girlfriend', 'ex_boyfriend'), ('Chairman', 'Chairwoman'), ('TESTOSTERONE', 'ESTROGEN'), ('Gentleman', 'Lady'), ('MEN', 'WOMEN'), ('EX_GIRLFRIEND', 'EX_BOYFRIEND'), ('Ex_Girlfriend', 'Ex_Boyfriend'), ('SON', 'DAUGHTER'), ('COUNCILMAN', 'COUNCILWOMAN'), ('GRANDPA', 'GRANDMA'), ('king', 'queen'), ('brothers', 'sisters'), ('males', 'females'), ('Brother', 'Sister'), ('Sons', 'Daughters'), ('KING', 'QUEEN'), ('Dad', 'Mom'), ('fatherhood', 'motherhood'), ('FRATERNITY', 'SORORITY'), ('Kings', 'Queens'), ('CONGRESSMAN', 'CONGRESSWOMAN'), ('Colt', 'Filly'), ('Men', 'Women'), ('nephew', 'niece'), ('colt', 'filly'), ('grandpa', 'grandma'), ('GRANDSON', 'GRANDDAUGHTER'), ('men', 'women'), ('KINGS', 'QUEENS'), ('BOYS', 'GIRLS'), ('wives', 'husbands'), ('fraternity', 'sorority'), ('BROTHERS', 'SISTERS'), ('MALES', 'FEMALES'), ('man', 'woman'), ('boys', 'girls'), ('TWIN_BROTHER', 'TWIN_SISTER'), ('Father', 'Mother'), ('chairman', 'chairwoman'), ('FATHER', 'MOTHER'), ('Grandpa', 'Grandma'), ('sons', 'daughters'), ('Spokesman', 'Spokeswoman'), ('he', 'she'), ('father', 'mother'), ('GELDING', 'MARE'), ('dads', 'moms'), ('gentleman', 'lady'), ('fathers', 'mothers'), ('NEPHEW', 'NIECE'), ('grandfather', 'grandmother'), ('Male', 'Female'), ('Brothers', 'Sisters'), ('CHAIRMAN', 'CHAIRWOMAN'), ('Twin_Brother', 'Twin_Sister'), ('prostate_cancer', 'ovarian_cancer'), ('boy', 'girl')}\n",
            "All CN:  (76078, 300)\n",
            "With debiasing:  word2vec gender_list_propernouns\n",
            "WEAT d =  -1.1701102\n",
            "WEAT p =  0.996\n",
            "{('GENTLEMEN', 'LADIES'), ('Catholic_Priest', 'Nun'), ('Testosterone', 'Estrogen'), ('Monastery', 'Convent'), ('male', 'female'), ('Fraternity', 'Sorority'), ('Grandsons', 'Granddaughters'), ('PRINCE', 'PRINCESS'), ('monastery', 'convent'), ('his', 'her'), ('DAD', 'MOM'), ('Dads', 'Moms'), ('brother', 'sister'), ('UNCLE', 'AUNT'), ('GENTLEMAN', 'LADY'), ('prince', 'princess'), ('Son', 'Daughter'), ('Himself', 'Herself'), ('SCHOOLBOY', 'SCHOOLGIRL'), ('dudes', 'gals'), ('Grandfather', 'Grandmother'), ('Congressman', 'Congresswoman'), ('BROTHER', 'SISTER'), ('Nephew', 'Niece'), ('twin_brother', 'twin_sister'), ('GRANDFATHER', 'GRANDMOTHER'), ('Wives', 'Husbands'), ('grandsons', 'granddaughters'), ('Grandson', 'Granddaughter'), ('Males', 'Females'), ('Gelding', 'Mare'), ('schoolboy', 'schoolgirl'), ('HIS', 'HER'), ('He', 'She'), ('PROSTATE_CANCER', 'OVARIAN_CANCER'), ('SONS', 'DAUGHTERS'), ('Businessman', 'Businesswoman'), ('FATHERS', 'MOTHERS'), ('Fella', 'Granny'), ('uncle', 'aunt'), ('BOY', 'GIRL'), ('gelding', 'mare'), ('DUDES', 'GALS'), ('Prince', 'Princess'), ('FELLA', 'GRANNY'), ('kings', 'queens'), ('FATHERHOOD', 'MOTHERHOOD'), ('COLT', 'FILLY'), ('Boy', 'Girl'), ('himself', 'herself'), ('BUSINESSMAN', 'BUSINESSWOMAN'), ('grandson', 'granddaughter'), ('HE', 'SHE'), ('CATHOLIC_PRIEST', 'NUN'), ('Dudes', 'Gals'), ('spokesman', 'spokeswoman'), ('MALE', 'FEMALE'), ('Councilman', 'Councilwoman'), ('testosterone', 'estrogen'), ('Fathers', 'Mothers'), ('councilman', 'councilwoman'), ('Prostate_Cancer', 'Ovarian_Cancer'), ('businessman', 'businesswoman'), ('Schoolboy', 'Schoolgirl'), ('Man', 'Woman'), ('Boys', 'Girls'), ('SPOKESMAN', 'SPOKESWOMAN'), ('Uncle', 'Aunt'), ('fella', 'granny'), ('HIMSELF', 'HERSELF'), ('GRANDSONS', 'GRANDDAUGHTERS'), ('His', 'Her'), ('catholic_priest', 'nun'), ('Gentlemen', 'Ladies'), ('King', 'Queen'), ('congressman', 'congresswoman'), ('WIVES', 'HUSBANDS'), ('MONASTERY', 'CONVENT'), ('son', 'daughter'), ('Fatherhood', 'Motherhood'), ('dad', 'mom'), ('MAN', 'WOMAN'), ('gentlemen', 'ladies'), ('DADS', 'MOMS'), ('ex_girlfriend', 'ex_boyfriend'), ('Chairman', 'Chairwoman'), ('TESTOSTERONE', 'ESTROGEN'), ('Gentleman', 'Lady'), ('MEN', 'WOMEN'), ('EX_GIRLFRIEND', 'EX_BOYFRIEND'), ('Ex_Girlfriend', 'Ex_Boyfriend'), ('SON', 'DAUGHTER'), ('COUNCILMAN', 'COUNCILWOMAN'), ('GRANDPA', 'GRANDMA'), ('king', 'queen'), ('brothers', 'sisters'), ('males', 'females'), ('Brother', 'Sister'), ('Sons', 'Daughters'), ('KING', 'QUEEN'), ('Dad', 'Mom'), ('fatherhood', 'motherhood'), ('FRATERNITY', 'SORORITY'), ('Kings', 'Queens'), ('CONGRESSMAN', 'CONGRESSWOMAN'), ('Colt', 'Filly'), ('Men', 'Women'), ('nephew', 'niece'), ('colt', 'filly'), ('grandpa', 'grandma'), ('GRANDSON', 'GRANDDAUGHTER'), ('men', 'women'), ('KINGS', 'QUEENS'), ('BOYS', 'GIRLS'), ('wives', 'husbands'), ('fraternity', 'sorority'), ('BROTHERS', 'SISTERS'), ('MALES', 'FEMALES'), ('man', 'woman'), ('boys', 'girls'), ('TWIN_BROTHER', 'TWIN_SISTER'), ('Father', 'Mother'), ('chairman', 'chairwoman'), ('FATHER', 'MOTHER'), ('Grandpa', 'Grandma'), ('sons', 'daughters'), ('Spokesman', 'Spokeswoman'), ('he', 'she'), ('father', 'mother'), ('GELDING', 'MARE'), ('dads', 'moms'), ('gentleman', 'lady'), ('fathers', 'mothers'), ('NEPHEW', 'NIECE'), ('grandfather', 'grandmother'), ('Male', 'Female'), ('Brothers', 'Sisters'), ('CHAIRMAN', 'CHAIRWOMAN'), ('Twin_Brother', 'Twin_Sister'), ('prostate_cancer', 'ovarian_cancer'), ('boy', 'girl')}\n",
            "All CN:  (76078, 300)\n",
            "With debiasing:  word2vec gender_list_pronouns\n",
            "WEAT d =  -1.0880654\n",
            "WEAT p =  0.991\n",
            "{('GENTLEMEN', 'LADIES'), ('Catholic_Priest', 'Nun'), ('Testosterone', 'Estrogen'), ('Monastery', 'Convent'), ('male', 'female'), ('Fraternity', 'Sorority'), ('Grandsons', 'Granddaughters'), ('PRINCE', 'PRINCESS'), ('monastery', 'convent'), ('his', 'her'), ('DAD', 'MOM'), ('Dads', 'Moms'), ('brother', 'sister'), ('UNCLE', 'AUNT'), ('GENTLEMAN', 'LADY'), ('prince', 'princess'), ('Son', 'Daughter'), ('Himself', 'Herself'), ('SCHOOLBOY', 'SCHOOLGIRL'), ('dudes', 'gals'), ('Grandfather', 'Grandmother'), ('Congressman', 'Congresswoman'), ('BROTHER', 'SISTER'), ('Nephew', 'Niece'), ('twin_brother', 'twin_sister'), ('GRANDFATHER', 'GRANDMOTHER'), ('Wives', 'Husbands'), ('grandsons', 'granddaughters'), ('Grandson', 'Granddaughter'), ('Males', 'Females'), ('Gelding', 'Mare'), ('schoolboy', 'schoolgirl'), ('HIS', 'HER'), ('He', 'She'), ('PROSTATE_CANCER', 'OVARIAN_CANCER'), ('SONS', 'DAUGHTERS'), ('Businessman', 'Businesswoman'), ('FATHERS', 'MOTHERS'), ('Fella', 'Granny'), ('uncle', 'aunt'), ('BOY', 'GIRL'), ('gelding', 'mare'), ('DUDES', 'GALS'), ('Prince', 'Princess'), ('FELLA', 'GRANNY'), ('kings', 'queens'), ('FATHERHOOD', 'MOTHERHOOD'), ('COLT', 'FILLY'), ('Boy', 'Girl'), ('himself', 'herself'), ('BUSINESSMAN', 'BUSINESSWOMAN'), ('grandson', 'granddaughter'), ('HE', 'SHE'), ('CATHOLIC_PRIEST', 'NUN'), ('Dudes', 'Gals'), ('spokesman', 'spokeswoman'), ('MALE', 'FEMALE'), ('Councilman', 'Councilwoman'), ('testosterone', 'estrogen'), ('Fathers', 'Mothers'), ('councilman', 'councilwoman'), ('Prostate_Cancer', 'Ovarian_Cancer'), ('businessman', 'businesswoman'), ('Schoolboy', 'Schoolgirl'), ('Man', 'Woman'), ('Boys', 'Girls'), ('SPOKESMAN', 'SPOKESWOMAN'), ('Uncle', 'Aunt'), ('fella', 'granny'), ('HIMSELF', 'HERSELF'), ('GRANDSONS', 'GRANDDAUGHTERS'), ('His', 'Her'), ('catholic_priest', 'nun'), ('Gentlemen', 'Ladies'), ('King', 'Queen'), ('congressman', 'congresswoman'), ('WIVES', 'HUSBANDS'), ('MONASTERY', 'CONVENT'), ('son', 'daughter'), ('Fatherhood', 'Motherhood'), ('dad', 'mom'), ('MAN', 'WOMAN'), ('gentlemen', 'ladies'), ('DADS', 'MOMS'), ('ex_girlfriend', 'ex_boyfriend'), ('Chairman', 'Chairwoman'), ('TESTOSTERONE', 'ESTROGEN'), ('Gentleman', 'Lady'), ('MEN', 'WOMEN'), ('EX_GIRLFRIEND', 'EX_BOYFRIEND'), ('Ex_Girlfriend', 'Ex_Boyfriend'), ('SON', 'DAUGHTER'), ('COUNCILMAN', 'COUNCILWOMAN'), ('GRANDPA', 'GRANDMA'), ('king', 'queen'), ('brothers', 'sisters'), ('males', 'females'), ('Brother', 'Sister'), ('Sons', 'Daughters'), ('KING', 'QUEEN'), ('Dad', 'Mom'), ('fatherhood', 'motherhood'), ('FRATERNITY', 'SORORITY'), ('Kings', 'Queens'), ('CONGRESSMAN', 'CONGRESSWOMAN'), ('Colt', 'Filly'), ('Men', 'Women'), ('nephew', 'niece'), ('colt', 'filly'), ('grandpa', 'grandma'), ('GRANDSON', 'GRANDDAUGHTER'), ('men', 'women'), ('KINGS', 'QUEENS'), ('BOYS', 'GIRLS'), ('wives', 'husbands'), ('fraternity', 'sorority'), ('BROTHERS', 'SISTERS'), ('MALES', 'FEMALES'), ('man', 'woman'), ('boys', 'girls'), ('TWIN_BROTHER', 'TWIN_SISTER'), ('Father', 'Mother'), ('chairman', 'chairwoman'), ('FATHER', 'MOTHER'), ('Grandpa', 'Grandma'), ('sons', 'daughters'), ('Spokesman', 'Spokeswoman'), ('he', 'she'), ('father', 'mother'), ('GELDING', 'MARE'), ('dads', 'moms'), ('gentleman', 'lady'), ('fathers', 'mothers'), ('NEPHEW', 'NIECE'), ('grandfather', 'grandmother'), ('Male', 'Female'), ('Brothers', 'Sisters'), ('CHAIRMAN', 'CHAIRWOMAN'), ('Twin_Brother', 'Twin_Sister'), ('prostate_cancer', 'ovarian_cancer'), ('boy', 'girl')}\n",
            "All CN:  (76078, 300)\n",
            "With debiasing:  word2vec gender_list_all\n",
            "WEAT d =  -1.0729237\n",
            "WEAT p =  0.988\n",
            "All CN:  (119127, 300)\n",
            "Without debiasing\n",
            "WEAT d =  1.1896381\n",
            "WEAT p =  0.008\n",
            "{('GENTLEMEN', 'LADIES'), ('Catholic_Priest', 'Nun'), ('Testosterone', 'Estrogen'), ('Monastery', 'Convent'), ('male', 'female'), ('Fraternity', 'Sorority'), ('Grandsons', 'Granddaughters'), ('PRINCE', 'PRINCESS'), ('monastery', 'convent'), ('his', 'her'), ('DAD', 'MOM'), ('Dads', 'Moms'), ('brother', 'sister'), ('UNCLE', 'AUNT'), ('GENTLEMAN', 'LADY'), ('prince', 'princess'), ('Son', 'Daughter'), ('Himself', 'Herself'), ('SCHOOLBOY', 'SCHOOLGIRL'), ('dudes', 'gals'), ('Grandfather', 'Grandmother'), ('Congressman', 'Congresswoman'), ('BROTHER', 'SISTER'), ('Nephew', 'Niece'), ('twin_brother', 'twin_sister'), ('GRANDFATHER', 'GRANDMOTHER'), ('Wives', 'Husbands'), ('grandsons', 'granddaughters'), ('Grandson', 'Granddaughter'), ('Males', 'Females'), ('Gelding', 'Mare'), ('schoolboy', 'schoolgirl'), ('HIS', 'HER'), ('He', 'She'), ('PROSTATE_CANCER', 'OVARIAN_CANCER'), ('SONS', 'DAUGHTERS'), ('Businessman', 'Businesswoman'), ('FATHERS', 'MOTHERS'), ('Fella', 'Granny'), ('uncle', 'aunt'), ('BOY', 'GIRL'), ('gelding', 'mare'), ('DUDES', 'GALS'), ('Prince', 'Princess'), ('FELLA', 'GRANNY'), ('kings', 'queens'), ('FATHERHOOD', 'MOTHERHOOD'), ('COLT', 'FILLY'), ('Boy', 'Girl'), ('himself', 'herself'), ('BUSINESSMAN', 'BUSINESSWOMAN'), ('grandson', 'granddaughter'), ('HE', 'SHE'), ('CATHOLIC_PRIEST', 'NUN'), ('Dudes', 'Gals'), ('spokesman', 'spokeswoman'), ('MALE', 'FEMALE'), ('Councilman', 'Councilwoman'), ('testosterone', 'estrogen'), ('Fathers', 'Mothers'), ('councilman', 'councilwoman'), ('Prostate_Cancer', 'Ovarian_Cancer'), ('businessman', 'businesswoman'), ('Schoolboy', 'Schoolgirl'), ('Man', 'Woman'), ('Boys', 'Girls'), ('SPOKESMAN', 'SPOKESWOMAN'), ('Uncle', 'Aunt'), ('fella', 'granny'), ('HIMSELF', 'HERSELF'), ('GRANDSONS', 'GRANDDAUGHTERS'), ('His', 'Her'), ('catholic_priest', 'nun'), ('Gentlemen', 'Ladies'), ('King', 'Queen'), ('congressman', 'congresswoman'), ('WIVES', 'HUSBANDS'), ('MONASTERY', 'CONVENT'), ('son', 'daughter'), ('Fatherhood', 'Motherhood'), ('dad', 'mom'), ('MAN', 'WOMAN'), ('gentlemen', 'ladies'), ('DADS', 'MOMS'), ('ex_girlfriend', 'ex_boyfriend'), ('Chairman', 'Chairwoman'), ('TESTOSTERONE', 'ESTROGEN'), ('Gentleman', 'Lady'), ('MEN', 'WOMEN'), ('EX_GIRLFRIEND', 'EX_BOYFRIEND'), ('Ex_Girlfriend', 'Ex_Boyfriend'), ('SON', 'DAUGHTER'), ('COUNCILMAN', 'COUNCILWOMAN'), ('GRANDPA', 'GRANDMA'), ('king', 'queen'), ('brothers', 'sisters'), ('males', 'females'), ('Brother', 'Sister'), ('Sons', 'Daughters'), ('KING', 'QUEEN'), ('Dad', 'Mom'), ('fatherhood', 'motherhood'), ('FRATERNITY', 'SORORITY'), ('Kings', 'Queens'), ('CONGRESSMAN', 'CONGRESSWOMAN'), ('Colt', 'Filly'), ('Men', 'Women'), ('nephew', 'niece'), ('colt', 'filly'), ('grandpa', 'grandma'), ('GRANDSON', 'GRANDDAUGHTER'), ('men', 'women'), ('KINGS', 'QUEENS'), ('BOYS', 'GIRLS'), ('wives', 'husbands'), ('fraternity', 'sorority'), ('BROTHERS', 'SISTERS'), ('MALES', 'FEMALES'), ('man', 'woman'), ('boys', 'girls'), ('TWIN_BROTHER', 'TWIN_SISTER'), ('Father', 'Mother'), ('chairman', 'chairwoman'), ('FATHER', 'MOTHER'), ('Grandpa', 'Grandma'), ('sons', 'daughters'), ('Spokesman', 'Spokeswoman'), ('he', 'she'), ('father', 'mother'), ('GELDING', 'MARE'), ('dads', 'moms'), ('gentleman', 'lady'), ('fathers', 'mothers'), ('NEPHEW', 'NIECE'), ('grandfather', 'grandmother'), ('Male', 'Female'), ('Brothers', 'Sisters'), ('CHAIRMAN', 'CHAIRWOMAN'), ('Twin_Brother', 'Twin_Sister'), ('prostate_cancer', 'ovarian_cancer'), ('boy', 'girl')}\n",
            "All CN:  (119127, 300)\n",
            "With debiasing:  fasttext gender_list_extended\n",
            "WEAT d =  0.20830178\n",
            "WEAT p =  0.353\n",
            "{('GENTLEMEN', 'LADIES'), ('Catholic_Priest', 'Nun'), ('Testosterone', 'Estrogen'), ('Monastery', 'Convent'), ('male', 'female'), ('Fraternity', 'Sorority'), ('Grandsons', 'Granddaughters'), ('PRINCE', 'PRINCESS'), ('monastery', 'convent'), ('his', 'her'), ('DAD', 'MOM'), ('Dads', 'Moms'), ('brother', 'sister'), ('UNCLE', 'AUNT'), ('GENTLEMAN', 'LADY'), ('prince', 'princess'), ('Son', 'Daughter'), ('Himself', 'Herself'), ('SCHOOLBOY', 'SCHOOLGIRL'), ('dudes', 'gals'), ('Grandfather', 'Grandmother'), ('Congressman', 'Congresswoman'), ('BROTHER', 'SISTER'), ('Nephew', 'Niece'), ('twin_brother', 'twin_sister'), ('GRANDFATHER', 'GRANDMOTHER'), ('Wives', 'Husbands'), ('grandsons', 'granddaughters'), ('Grandson', 'Granddaughter'), ('Males', 'Females'), ('Gelding', 'Mare'), ('schoolboy', 'schoolgirl'), ('HIS', 'HER'), ('He', 'She'), ('PROSTATE_CANCER', 'OVARIAN_CANCER'), ('SONS', 'DAUGHTERS'), ('Businessman', 'Businesswoman'), ('FATHERS', 'MOTHERS'), ('Fella', 'Granny'), ('uncle', 'aunt'), ('BOY', 'GIRL'), ('gelding', 'mare'), ('DUDES', 'GALS'), ('Prince', 'Princess'), ('FELLA', 'GRANNY'), ('kings', 'queens'), ('FATHERHOOD', 'MOTHERHOOD'), ('COLT', 'FILLY'), ('Boy', 'Girl'), ('himself', 'herself'), ('BUSINESSMAN', 'BUSINESSWOMAN'), ('grandson', 'granddaughter'), ('HE', 'SHE'), ('CATHOLIC_PRIEST', 'NUN'), ('Dudes', 'Gals'), ('spokesman', 'spokeswoman'), ('MALE', 'FEMALE'), ('Councilman', 'Councilwoman'), ('testosterone', 'estrogen'), ('Fathers', 'Mothers'), ('councilman', 'councilwoman'), ('Prostate_Cancer', 'Ovarian_Cancer'), ('businessman', 'businesswoman'), ('Schoolboy', 'Schoolgirl'), ('Man', 'Woman'), ('Boys', 'Girls'), ('SPOKESMAN', 'SPOKESWOMAN'), ('Uncle', 'Aunt'), ('fella', 'granny'), ('HIMSELF', 'HERSELF'), ('GRANDSONS', 'GRANDDAUGHTERS'), ('His', 'Her'), ('catholic_priest', 'nun'), ('Gentlemen', 'Ladies'), ('King', 'Queen'), ('congressman', 'congresswoman'), ('WIVES', 'HUSBANDS'), ('MONASTERY', 'CONVENT'), ('son', 'daughter'), ('Fatherhood', 'Motherhood'), ('dad', 'mom'), ('MAN', 'WOMAN'), ('gentlemen', 'ladies'), ('DADS', 'MOMS'), ('ex_girlfriend', 'ex_boyfriend'), ('Chairman', 'Chairwoman'), ('TESTOSTERONE', 'ESTROGEN'), ('Gentleman', 'Lady'), ('MEN', 'WOMEN'), ('EX_GIRLFRIEND', 'EX_BOYFRIEND'), ('Ex_Girlfriend', 'Ex_Boyfriend'), ('SON', 'DAUGHTER'), ('COUNCILMAN', 'COUNCILWOMAN'), ('GRANDPA', 'GRANDMA'), ('king', 'queen'), ('brothers', 'sisters'), ('males', 'females'), ('Brother', 'Sister'), ('Sons', 'Daughters'), ('KING', 'QUEEN'), ('Dad', 'Mom'), ('fatherhood', 'motherhood'), ('FRATERNITY', 'SORORITY'), ('Kings', 'Queens'), ('CONGRESSMAN', 'CONGRESSWOMAN'), ('Colt', 'Filly'), ('Men', 'Women'), ('nephew', 'niece'), ('colt', 'filly'), ('grandpa', 'grandma'), ('GRANDSON', 'GRANDDAUGHTER'), ('men', 'women'), ('KINGS', 'QUEENS'), ('BOYS', 'GIRLS'), ('wives', 'husbands'), ('fraternity', 'sorority'), ('BROTHERS', 'SISTERS'), ('MALES', 'FEMALES'), ('man', 'woman'), ('boys', 'girls'), ('TWIN_BROTHER', 'TWIN_SISTER'), ('Father', 'Mother'), ('chairman', 'chairwoman'), ('FATHER', 'MOTHER'), ('Grandpa', 'Grandma'), ('sons', 'daughters'), ('Spokesman', 'Spokeswoman'), ('he', 'she'), ('father', 'mother'), ('GELDING', 'MARE'), ('dads', 'moms'), ('gentleman', 'lady'), ('fathers', 'mothers'), ('NEPHEW', 'NIECE'), ('grandfather', 'grandmother'), ('Male', 'Female'), ('Brothers', 'Sisters'), ('CHAIRMAN', 'CHAIRWOMAN'), ('Twin_Brother', 'Twin_Sister'), ('prostate_cancer', 'ovarian_cancer'), ('boy', 'girl')}\n",
            "All CN:  (119127, 300)\n",
            "With debiasing:  fasttext gender_list_propernouns\n",
            "WEAT d =  0.15312241\n",
            "WEAT p =  0.403\n",
            "{('GENTLEMEN', 'LADIES'), ('Catholic_Priest', 'Nun'), ('Testosterone', 'Estrogen'), ('Monastery', 'Convent'), ('male', 'female'), ('Fraternity', 'Sorority'), ('Grandsons', 'Granddaughters'), ('PRINCE', 'PRINCESS'), ('monastery', 'convent'), ('his', 'her'), ('DAD', 'MOM'), ('Dads', 'Moms'), ('brother', 'sister'), ('UNCLE', 'AUNT'), ('GENTLEMAN', 'LADY'), ('prince', 'princess'), ('Son', 'Daughter'), ('Himself', 'Herself'), ('SCHOOLBOY', 'SCHOOLGIRL'), ('dudes', 'gals'), ('Grandfather', 'Grandmother'), ('Congressman', 'Congresswoman'), ('BROTHER', 'SISTER'), ('Nephew', 'Niece'), ('twin_brother', 'twin_sister'), ('GRANDFATHER', 'GRANDMOTHER'), ('Wives', 'Husbands'), ('grandsons', 'granddaughters'), ('Grandson', 'Granddaughter'), ('Males', 'Females'), ('Gelding', 'Mare'), ('schoolboy', 'schoolgirl'), ('HIS', 'HER'), ('He', 'She'), ('PROSTATE_CANCER', 'OVARIAN_CANCER'), ('SONS', 'DAUGHTERS'), ('Businessman', 'Businesswoman'), ('FATHERS', 'MOTHERS'), ('Fella', 'Granny'), ('uncle', 'aunt'), ('BOY', 'GIRL'), ('gelding', 'mare'), ('DUDES', 'GALS'), ('Prince', 'Princess'), ('FELLA', 'GRANNY'), ('kings', 'queens'), ('FATHERHOOD', 'MOTHERHOOD'), ('COLT', 'FILLY'), ('Boy', 'Girl'), ('himself', 'herself'), ('BUSINESSMAN', 'BUSINESSWOMAN'), ('grandson', 'granddaughter'), ('HE', 'SHE'), ('CATHOLIC_PRIEST', 'NUN'), ('Dudes', 'Gals'), ('spokesman', 'spokeswoman'), ('MALE', 'FEMALE'), ('Councilman', 'Councilwoman'), ('testosterone', 'estrogen'), ('Fathers', 'Mothers'), ('councilman', 'councilwoman'), ('Prostate_Cancer', 'Ovarian_Cancer'), ('businessman', 'businesswoman'), ('Schoolboy', 'Schoolgirl'), ('Man', 'Woman'), ('Boys', 'Girls'), ('SPOKESMAN', 'SPOKESWOMAN'), ('Uncle', 'Aunt'), ('fella', 'granny'), ('HIMSELF', 'HERSELF'), ('GRANDSONS', 'GRANDDAUGHTERS'), ('His', 'Her'), ('catholic_priest', 'nun'), ('Gentlemen', 'Ladies'), ('King', 'Queen'), ('congressman', 'congresswoman'), ('WIVES', 'HUSBANDS'), ('MONASTERY', 'CONVENT'), ('son', 'daughter'), ('Fatherhood', 'Motherhood'), ('dad', 'mom'), ('MAN', 'WOMAN'), ('gentlemen', 'ladies'), ('DADS', 'MOMS'), ('ex_girlfriend', 'ex_boyfriend'), ('Chairman', 'Chairwoman'), ('TESTOSTERONE', 'ESTROGEN'), ('Gentleman', 'Lady'), ('MEN', 'WOMEN'), ('EX_GIRLFRIEND', 'EX_BOYFRIEND'), ('Ex_Girlfriend', 'Ex_Boyfriend'), ('SON', 'DAUGHTER'), ('COUNCILMAN', 'COUNCILWOMAN'), ('GRANDPA', 'GRANDMA'), ('king', 'queen'), ('brothers', 'sisters'), ('males', 'females'), ('Brother', 'Sister'), ('Sons', 'Daughters'), ('KING', 'QUEEN'), ('Dad', 'Mom'), ('fatherhood', 'motherhood'), ('FRATERNITY', 'SORORITY'), ('Kings', 'Queens'), ('CONGRESSMAN', 'CONGRESSWOMAN'), ('Colt', 'Filly'), ('Men', 'Women'), ('nephew', 'niece'), ('colt', 'filly'), ('grandpa', 'grandma'), ('GRANDSON', 'GRANDDAUGHTER'), ('men', 'women'), ('KINGS', 'QUEENS'), ('BOYS', 'GIRLS'), ('wives', 'husbands'), ('fraternity', 'sorority'), ('BROTHERS', 'SISTERS'), ('MALES', 'FEMALES'), ('man', 'woman'), ('boys', 'girls'), ('TWIN_BROTHER', 'TWIN_SISTER'), ('Father', 'Mother'), ('chairman', 'chairwoman'), ('FATHER', 'MOTHER'), ('Grandpa', 'Grandma'), ('sons', 'daughters'), ('Spokesman', 'Spokeswoman'), ('he', 'she'), ('father', 'mother'), ('GELDING', 'MARE'), ('dads', 'moms'), ('gentleman', 'lady'), ('fathers', 'mothers'), ('NEPHEW', 'NIECE'), ('grandfather', 'grandmother'), ('Male', 'Female'), ('Brothers', 'Sisters'), ('CHAIRMAN', 'CHAIRWOMAN'), ('Twin_Brother', 'Twin_Sister'), ('prostate_cancer', 'ovarian_cancer'), ('boy', 'girl')}\n",
            "All CN:  (119127, 300)\n",
            "With debiasing:  fasttext gender_list_pronouns\n",
            "WEAT d =  0.18101071\n",
            "WEAT p =  0.347\n",
            "{('GENTLEMEN', 'LADIES'), ('Catholic_Priest', 'Nun'), ('Testosterone', 'Estrogen'), ('Monastery', 'Convent'), ('male', 'female'), ('Fraternity', 'Sorority'), ('Grandsons', 'Granddaughters'), ('PRINCE', 'PRINCESS'), ('monastery', 'convent'), ('his', 'her'), ('DAD', 'MOM'), ('Dads', 'Moms'), ('brother', 'sister'), ('UNCLE', 'AUNT'), ('GENTLEMAN', 'LADY'), ('prince', 'princess'), ('Son', 'Daughter'), ('Himself', 'Herself'), ('SCHOOLBOY', 'SCHOOLGIRL'), ('dudes', 'gals'), ('Grandfather', 'Grandmother'), ('Congressman', 'Congresswoman'), ('BROTHER', 'SISTER'), ('Nephew', 'Niece'), ('twin_brother', 'twin_sister'), ('GRANDFATHER', 'GRANDMOTHER'), ('Wives', 'Husbands'), ('grandsons', 'granddaughters'), ('Grandson', 'Granddaughter'), ('Males', 'Females'), ('Gelding', 'Mare'), ('schoolboy', 'schoolgirl'), ('HIS', 'HER'), ('He', 'She'), ('PROSTATE_CANCER', 'OVARIAN_CANCER'), ('SONS', 'DAUGHTERS'), ('Businessman', 'Businesswoman'), ('FATHERS', 'MOTHERS'), ('Fella', 'Granny'), ('uncle', 'aunt'), ('BOY', 'GIRL'), ('gelding', 'mare'), ('DUDES', 'GALS'), ('Prince', 'Princess'), ('FELLA', 'GRANNY'), ('kings', 'queens'), ('FATHERHOOD', 'MOTHERHOOD'), ('COLT', 'FILLY'), ('Boy', 'Girl'), ('himself', 'herself'), ('BUSINESSMAN', 'BUSINESSWOMAN'), ('grandson', 'granddaughter'), ('HE', 'SHE'), ('CATHOLIC_PRIEST', 'NUN'), ('Dudes', 'Gals'), ('spokesman', 'spokeswoman'), ('MALE', 'FEMALE'), ('Councilman', 'Councilwoman'), ('testosterone', 'estrogen'), ('Fathers', 'Mothers'), ('councilman', 'councilwoman'), ('Prostate_Cancer', 'Ovarian_Cancer'), ('businessman', 'businesswoman'), ('Schoolboy', 'Schoolgirl'), ('Man', 'Woman'), ('Boys', 'Girls'), ('SPOKESMAN', 'SPOKESWOMAN'), ('Uncle', 'Aunt'), ('fella', 'granny'), ('HIMSELF', 'HERSELF'), ('GRANDSONS', 'GRANDDAUGHTERS'), ('His', 'Her'), ('catholic_priest', 'nun'), ('Gentlemen', 'Ladies'), ('King', 'Queen'), ('congressman', 'congresswoman'), ('WIVES', 'HUSBANDS'), ('MONASTERY', 'CONVENT'), ('son', 'daughter'), ('Fatherhood', 'Motherhood'), ('dad', 'mom'), ('MAN', 'WOMAN'), ('gentlemen', 'ladies'), ('DADS', 'MOMS'), ('ex_girlfriend', 'ex_boyfriend'), ('Chairman', 'Chairwoman'), ('TESTOSTERONE', 'ESTROGEN'), ('Gentleman', 'Lady'), ('MEN', 'WOMEN'), ('EX_GIRLFRIEND', 'EX_BOYFRIEND'), ('Ex_Girlfriend', 'Ex_Boyfriend'), ('SON', 'DAUGHTER'), ('COUNCILMAN', 'COUNCILWOMAN'), ('GRANDPA', 'GRANDMA'), ('king', 'queen'), ('brothers', 'sisters'), ('males', 'females'), ('Brother', 'Sister'), ('Sons', 'Daughters'), ('KING', 'QUEEN'), ('Dad', 'Mom'), ('fatherhood', 'motherhood'), ('FRATERNITY', 'SORORITY'), ('Kings', 'Queens'), ('CONGRESSMAN', 'CONGRESSWOMAN'), ('Colt', 'Filly'), ('Men', 'Women'), ('nephew', 'niece'), ('colt', 'filly'), ('grandpa', 'grandma'), ('GRANDSON', 'GRANDDAUGHTER'), ('men', 'women'), ('KINGS', 'QUEENS'), ('BOYS', 'GIRLS'), ('wives', 'husbands'), ('fraternity', 'sorority'), ('BROTHERS', 'SISTERS'), ('MALES', 'FEMALES'), ('man', 'woman'), ('boys', 'girls'), ('TWIN_BROTHER', 'TWIN_SISTER'), ('Father', 'Mother'), ('chairman', 'chairwoman'), ('FATHER', 'MOTHER'), ('Grandpa', 'Grandma'), ('sons', 'daughters'), ('Spokesman', 'Spokeswoman'), ('he', 'she'), ('father', 'mother'), ('GELDING', 'MARE'), ('dads', 'moms'), ('gentleman', 'lady'), ('fathers', 'mothers'), ('NEPHEW', 'NIECE'), ('grandfather', 'grandmother'), ('Male', 'Female'), ('Brothers', 'Sisters'), ('CHAIRMAN', 'CHAIRWOMAN'), ('Twin_Brother', 'Twin_Sister'), ('prostate_cancer', 'ovarian_cancer'), ('boy', 'girl')}\n",
            "All CN:  (119127, 300)\n",
            "With debiasing:  fasttext gender_list_all\n",
            "WEAT d =  0.20226458\n",
            "WEAT p =  0.319\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Embedding                 Subspace  Effect Size p-value\n",
              "0      glove        without_debiasing    1.0896145   0.019\n",
              "1      glove     gender_list_extended  -0.60132134   0.863\n",
              "2      glove  gender_list_propernouns  -0.57953775   0.864\n",
              "3      glove     gender_list_pronouns   -0.5332187   0.845\n",
              "4      glove          gender_list_all   -0.5322254   0.821\n",
              "5   word2vec        without_debiasing   0.99810755   0.022\n",
              "6   word2vec     gender_list_extended   -1.1440278   0.995\n",
              "7   word2vec  gender_list_propernouns   -1.1701102   0.996\n",
              "8   word2vec     gender_list_pronouns   -1.0880654   0.991\n",
              "9   word2vec          gender_list_all   -1.0729237   0.988\n",
              "10  fasttext        without_debiasing    1.1896381   0.008\n",
              "11  fasttext     gender_list_extended   0.20830178   0.353\n",
              "12  fasttext  gender_list_propernouns   0.15312241   0.403\n",
              "13  fasttext     gender_list_pronouns   0.18101071   0.347\n",
              "14  fasttext          gender_list_all   0.20226458   0.319"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Embedding</th>\n",
              "      <th>Subspace</th>\n",
              "      <th>Effect Size</th>\n",
              "      <th>p-value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>glove</td>\n",
              "      <td>without_debiasing</td>\n",
              "      <td>1.0896145</td>\n",
              "      <td>0.019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>glove</td>\n",
              "      <td>gender_list_extended</td>\n",
              "      <td>-0.60132134</td>\n",
              "      <td>0.863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>glove</td>\n",
              "      <td>gender_list_propernouns</td>\n",
              "      <td>-0.57953775</td>\n",
              "      <td>0.864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>glove</td>\n",
              "      <td>gender_list_pronouns</td>\n",
              "      <td>-0.5332187</td>\n",
              "      <td>0.845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>glove</td>\n",
              "      <td>gender_list_all</td>\n",
              "      <td>-0.5322254</td>\n",
              "      <td>0.821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>word2vec</td>\n",
              "      <td>without_debiasing</td>\n",
              "      <td>0.99810755</td>\n",
              "      <td>0.022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>word2vec</td>\n",
              "      <td>gender_list_extended</td>\n",
              "      <td>-1.1440278</td>\n",
              "      <td>0.995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>word2vec</td>\n",
              "      <td>gender_list_propernouns</td>\n",
              "      <td>-1.1701102</td>\n",
              "      <td>0.996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>word2vec</td>\n",
              "      <td>gender_list_pronouns</td>\n",
              "      <td>-1.0880654</td>\n",
              "      <td>0.991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>word2vec</td>\n",
              "      <td>gender_list_all</td>\n",
              "      <td>-1.0729237</td>\n",
              "      <td>0.988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>fasttext</td>\n",
              "      <td>without_debiasing</td>\n",
              "      <td>1.1896381</td>\n",
              "      <td>0.008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>fasttext</td>\n",
              "      <td>gender_list_extended</td>\n",
              "      <td>0.20830178</td>\n",
              "      <td>0.353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>fasttext</td>\n",
              "      <td>gender_list_propernouns</td>\n",
              "      <td>0.15312241</td>\n",
              "      <td>0.403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>fasttext</td>\n",
              "      <td>gender_list_pronouns</td>\n",
              "      <td>0.18101071</td>\n",
              "      <td>0.347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>fasttext</td>\n",
              "      <td>gender_list_all</td>\n",
              "      <td>0.20226458</td>\n",
              "      <td>0.319</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "metadata": {
        "id": "6Xi-W-t1DZC4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}