{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEC with CN(ElMo, BERT).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsedoc/ConceptorDebias/blob/ACL-cleanup/EEC_with_CN(ElMo%2C_BERT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4wlukDN_-5XU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "t = np.transpose\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6kH_k0zp7wid",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "cd5694ce-f515-4fb9-d994-7c9055c5f95c"
      },
      "cell_type": "code",
      "source": [
        "##Install flair\n",
        "!pip install -q flair allennlp"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 81kB 5.0MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 5.6MB 6.3MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 798kB 22.5MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 122kB 28.8MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 245kB 27.9MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 552kB 24.2MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 1.5MB 16.2MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 51kB 20.5MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 51kB 20.3MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 512kB 24.3MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 4.5MB 8.1MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 133kB 31.3MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 1.0MB 17.9MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 61kB 22.5MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 2.3MB 13.3MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 143kB 29.8MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 51kB 11.7MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 51kB 19.7MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 102kB 26.6MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 7.5MB 5.2MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 92kB 27.7MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 204kB 27.7MB/s \n",
            "\u001b[?25h  Building wheel for flair (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for segtok (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for mpld3 (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for sqlitedict (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for overrides (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for parsimonious (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for numpydoc (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for jsonnet (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for jsondiff (setup.py) ... \u001b[?25ldone\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mfastai 1.0.46 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mawscli 1.16.125 has requirement botocore==1.12.115, but you'll have botocore 1.12.106 which is incompatible.\u001b[0m\n",
            "\u001b[31mallennlp 0.8.2 has requirement matplotlib==2.2.3, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WeGiuPg07zwF",
        "colab_type": "code",
        "outputId": "f6278c94-9da1-439f-85d8-866c4e6ec0d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "##Using flair to load emlo small embeddinngs, which is in 768 dimensions\n",
        "from flair.embeddings import BertEmbeddings, ELMoEmbeddings\n",
        "from flair.data import Sentence\n",
        "\n",
        "\n",
        "# init embedding\n",
        "elmo_embedding = ELMoEmbeddings('small') #BertEmbeddings('bert-large-uncased')\n",
        "bert_embedding = BertEmbeddings('bert-large-uncased')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 336/336 [00:00<00:00, 216314.07B/s]\n",
            "100%|██████████| 54402456/54402456 [00:04<00:00, 13202509.60B/s]\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 1167360.26B/s]\n",
            "100%|██████████| 1248501532/1248501532 [00:35<00:00, 35208210.58B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ytQwbLLuS20p",
        "colab_type": "code",
        "outputId": "30b9ad44-24de-4d75-8955-1c00d8bcf3b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "##Load EEC Data\n",
        "!wget http://www.saifmohammad.com/WebDocs/EEC/Equity-Evaluation-Corpus.zip\n",
        "!unzip Equity-Evaluation-Corpus.zip\n",
        "##Load Packages\n",
        "import numpy as np\n",
        "import scipy, requests, codecs, os, re, nltk, itertools, csv\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "import tensorflow as tf\n",
        "from scipy.stats import spearmanr\n",
        "import pandas as pd\n",
        "import functools as ft\n",
        "import os\n",
        "import io\n",
        "nltk.download('punkt')\n",
        "\n",
        "##Read EEC\n",
        "EEC = pd.read_csv('/content/Equity-Evaluation-Corpus.csv', header=0)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-15 02:18:29--  http://www.saifmohammad.com/WebDocs/EEC/Equity-Evaluation-Corpus.zip\n",
            "Resolving www.saifmohammad.com (www.saifmohammad.com)... 192.185.17.122\n",
            "Connecting to www.saifmohammad.com (www.saifmohammad.com)|192.185.17.122|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 102568 (100K) [application/zip]\n",
            "Saving to: ‘Equity-Evaluation-Corpus.zip’\n",
            "\n",
            "\r          Equity-Ev   0%[                    ]       0  --.-KB/s               \rEquity-Evaluation-C 100%[===================>] 100.16K   573KB/s    in 0.2s    \n",
            "\n",
            "2019-03-15 02:18:30 (573 KB/s) - ‘Equity-Evaluation-Corpus.zip’ saved [102568/102568]\n",
            "\n",
            "Archive:  Equity-Evaluation-Corpus.zip\n",
            "  inflating: Equity-Evaluation-Corpus.csv  \n",
            "  inflating: README.txt              \n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DBjgXTl96jln",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load BERT"
      ]
    },
    {
      "metadata": {
        "id": "asGSO5rlBHfF",
        "colab_type": "code",
        "outputId": "6ab74877-f1af-4d5e-8f01-9f3680b335eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1JyRVLZb7IdiMGGwPfXuA1f-CmCvxHDXx"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JyRVLZb7IdiMGGwPfXuA1f-CmCvxHDXx\n",
            "To: /content/bert_conceptors.tar.bz2\n",
            "656MB [00:04, 134MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0mROFJ1SGRot",
        "colab_type": "code",
        "outputId": "2db8afbe-a948-4412-b488-3b597dfb0d48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1003
        }
      },
      "cell_type": "code",
      "source": [
        "!tar xvjf /content/bert_conceptors.tar.bz2"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert/\n",
            "bert/bert_WEFAT_1_Male_attributes.pkl\n",
            "bert/big_bert_gender_list_propernouns.pkl\n",
            "bert/bert_W_5_African_American_names.pkl\n",
            "bert/bert_W_2_Weapons.pkl\n",
            "bert/bert_W_1_Pleasant.pkl\n",
            "bert/bert_W_5_European_American_names.pkl\n",
            "bert/bert_W_5_Unpleasant.pkl\n",
            "bert/bert_W_6_Male_names.pkl\n",
            "bert/big_bert_gender_list_all.pkl\n",
            "bert/bert_W_10_Pleasant.pkl\n",
            "bert/bert_W_1_Insects.pkl\n",
            "bert/bert_W_5_Unused_full_list_European_American_names.pkl\n",
            "bert/bert_W_10_Young_peoples_names.pkl\n",
            "bert/bert_W_6_Female_names.pkl\n",
            "bert/bert_W_9_Physical_disease.pkl\n",
            "bert/bert_W_7_Female_terms.pkl\n",
            "bert/big_bert_gender_list_pronouns.pkl\n",
            "bert/bert_W_3_Unused_full_list_European_American_names.pkl\n",
            "bert/bert_W_4_Unused_full_list_African_American_names.pkl\n",
            "bert/bert_W_8_Male_terms.pkl\n",
            "bert/bert_WEFAT_2_Androgynous_Names.pkl\n",
            "bert/bert_WEFAT_1_Careers.pkl\n",
            "bert/bert_W_9_Temporary.pkl\n",
            "bert/bert_W_2_Unpleasant.pkl\n",
            "bert/bert_WEFAT_1_Female_attributes.pkl\n",
            "bert/bert_W_2_Instruments.pkl\n",
            "bert/bert_W_7_Male_terms.pkl\n",
            "bert/bert_W_3_Unpleasant.pkl\n",
            "bert/bert_W_7_Math.pkl\n",
            "bert/bert_WEFAT_2_Male_attributes.pkl\n",
            "bert/bert_W_4_European_American_names.pkl\n",
            "bert/bert_W_1_Unpleasant.pkl\n",
            "bert/bert_W_5_Pleasant.pkl\n",
            "bert/bert_W_4_Unpleasant.pkl\n",
            "bert/bert_W_8_Arts.pkl\n",
            "bert/bert_W_4_African_American_names.pkl\n",
            "bert/bert_W_1_Flowers.pkl\n",
            "bert/bert_W_8_Female_terms.pkl\n",
            "bert/bert_W_6_Family.pkl\n",
            "bert/bert_W_3_European_American_names.pkl\n",
            "bert/bert_W_3_Pleasant.pkl\n",
            "bert/bert_WEFAT_2_Female_attributes.pkl\n",
            "bert/bert_W_3_Unused_full_list_African_American_names.pkl\n",
            "bert/bert_W_9_Permanent.pkl\n",
            "bert/bert_W_10_Old_peoples_names.pkl\n",
            "bert/bert_W_4_Unused_full_list_European_American_names.pkl\n",
            "bert/bert_W_9_Mental_disease.pkl\n",
            "bert/big_bert_gender_list_extended.pkl\n",
            "bert/bert_W_5_Unused_full_list_African_American_names.pkl\n",
            "bert/bert_W_8_Science.pkl\n",
            "bert/bert_W_2_Pleasant.pkl\n",
            "bert/bert_W_7_Arts.pkl\n",
            "bert/bert_W_6_Career.pkl\n",
            "bert/bert_W_4_Pleasant.pkl\n",
            "bert/bert_W_10_Unpleasant.pkl\n",
            "bert/bert_W_3_African_American_names.pkl\n",
            "bert/big_bert_race_list.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xONCr7Ei-FON",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load elmo race list embeddings"
      ]
    },
    {
      "metadata": {
        "id": "gZw8xNOrIdHl",
        "colab_type": "code",
        "outputId": "90b6b4dc-a78d-4c92-f84f-1a235e7a7c8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1WJBoEHOBkG4EW1MkB0y1QpggtV_J0i9w"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1WJBoEHOBkG4EW1MkB0y1QpggtV_J0i9w\n",
            "To: /content/elmo_race_list.pkl\n",
            "4.88MB [00:00, 43.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5enovlXI8Gfn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load pre-calculated CN processed ELMo and BERT embeddings"
      ]
    },
    {
      "metadata": {
        "id": "_Vkc8EKs_CaF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xUBsA7uWJEYL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = pickle.load(open('/content/elmo_race_list.pkl','rb'))\n",
        "#x.keys()\n",
        "#dict_keys(['GnegC', 'type_embedings', 'words'])\n",
        "negC = x['GnegC']\n",
        "negC_elmo = np.squeeze(np.asarray(negC))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iKHJutPIbWTV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = pickle.load(open('/content/bert/big_bert_race_list.pkl','rb'))\n",
        "#x.keys()\n",
        "#dict_keys(['GnegC', 'type_embedings', 'words'])\n",
        "negC = x['GnegC']\n",
        "negC_bert = np.squeeze(np.asarray(negC))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iDIh1lfL6zrd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load extra BERT for sentiment word embeddings"
      ]
    },
    {
      "metadata": {
        "id": "9qCiPSOoUdLr",
        "colab_type": "code",
        "outputId": "7a2b4676-18d8-431c-b3c6-000b7e2f84b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1dBiXlxSxfKq0-OhFQyL_CCij8pmThQZI"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dBiXlxSxfKq0-OhFQyL_CCij8pmThQZI\n",
            "To: /content/bert_EEC_sentiment.pkl\n",
            "\r  0% 0.00/65.8k [00:00<?, ?B/s]\r100% 65.8k/65.8k [00:00<00:00, 25.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uPAei1Z7bsBJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "senn = pickle.load(open('/content/bert_EEC_sentiment.pkl','rb'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UE0jvav_b6Nv",
        "colab_type": "code",
        "outputId": "926958ce-bb27-4d98-aeaa-36606c067794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "senn['words']"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fear', 'sadness', 'joy', 'anger']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "-2LrbWLOFlyM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Racial Debiasing"
      ]
    },
    {
      "metadata": {
        "id": "xH2Dk0jlFGDG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load brown corpus to get sentiment embeddings based on ELMo"
      ]
    },
    {
      "metadata": {
        "id": "6C0mIN5MftFN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD, randomized_svd\n",
        "from numpy.linalg import norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9sHTcgevTES5",
        "colab_type": "code",
        "outputId": "d2450cc1-10c2-45db-cde6-fc9f4376234f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "## Get brown_emb \n",
        "!gdown https://drive.google.com/uc?id=17TK2h3cz7amgm2mCY4QCYy1yh23ZFWDU\n",
        "!pip3 install -q pymagnitude\n",
        "from pymagnitude import *\n",
        "from pymagnitude import MagnitudeUtils\n",
        "import numpy as np\n",
        "import pickle\n",
        "with open('/content/elmo_embeddings_emma_brown.pkl', 'rb') as f:\n",
        "    elmo_data = pickle.load(f)\n",
        "elmo_data = elmo_data['brown_embs']\n",
        "\n",
        "## Get brown corpus\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "brown_corpus = brown.sents()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17TK2h3cz7amgm2mCY4QCYy1yh23ZFWDU\n",
            "To: /content/elmo_embeddings_emma_brown.pkl\n",
            "4.78GB [01:57, 40.6MB/s]\n",
            "\u001b[K    100% |████████████████████████████████| 5.4MB 8.0MB/s \n",
            "\u001b[?25h  Building wheel for pymagnitude (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7TPr6-HfTQiy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pick_embeddings(corpus, sent_embs, word_list):\n",
        "    X = []\n",
        "    #labels = []\n",
        "    #sents = []\n",
        "    for i, s in enumerate(corpus):\n",
        "       # print(s)\n",
        "        for j, w in enumerate(s):\n",
        "            if w in word_list:\n",
        "                X.append(sent_embs[i][j])\n",
        "                #labels.append(w)\n",
        "                #sents.append(s)\n",
        "    return X\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HPX2O5nJFPWK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run experiment-ELMO\n"
      ]
    },
    {
      "metadata": {
        "id": "8oDXuoVDZlLi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_raw(sen_emb, sent1, sent2):\n",
        "  \n",
        "  sentence1 = Sentence(sent1)\n",
        "  sentence2 = Sentence(sent2)\n",
        "  x = embedding.embed(sentence1) # get sentence embedding of sentence 1\n",
        "  se = torch.stack([token.embedding for token in sentence1])\n",
        "  sent1_emb = se.numpy()\n",
        "  x = embedding.embed(sentence2) # get sentence embedding of sentence 2\n",
        "  se = torch.stack([token.embedding for token in sentence2])\n",
        "  sent2_emb = se.numpy()\n",
        "  sent1_emb = np.average(sent1_emb, axis = 0) #average sentence1 embeddings\n",
        "  sent2_emb = np.average(sent2_emb, axis = 0) #average sentence2 embeddings\n",
        "  proj1 = (sent1_emb/norm(sent1_emb)).dot(sen_emb)/norm(sen_emb) #cosine similarity\n",
        "  proj2 = (sent2_emb/norm(sent2_emb)).dot(sen_emb)/norm(sen_emb)\n",
        "  #print(sent2_emb)\n",
        "  #print(proj1, proj2)\n",
        "  return proj1, proj2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hnBNClCHcT_A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_conceptor(sen_emb, sent1, sent2):\n",
        "  sentence1 = Sentence(sent1) \n",
        "  sentence2 = Sentence(sent2)\n",
        "  x = embedding.embed(sentence1) # get sentence embedding of sentence 1\n",
        "  se = torch.stack([token.embedding for token in sentence1])\n",
        "  sent1_emb = se.numpy()\n",
        "  x = embedding.embed(sentence2) # get sentence embedding of sentence 2\n",
        "  se = torch.stack([token.embedding for token in sentence2])\n",
        "  sent2_emb = se.numpy()\n",
        "  sent1_emb = np.average(sent1_emb, axis = 0) #average sentence1 embeddings\n",
        "  sent2_emb = np.average(sent2_emb, axis = 0) #average sentence2 embeddings\n",
        "  #print(sent1_emb.T.shape)\n",
        "  sent1_emb = negC.dot(sent1_emb.T).T  # CN sentence embeddings\n",
        "  #print(sent1_emb.shape)\n",
        "  sent2_emb = negC.dot(sent2_emb.T).T\n",
        "  sen_emb =  negC.dot(sen_emb.T).T\n",
        "  #print(sent2_emb)\n",
        "  proj1 = (sent1_emb/norm(sent1_emb)).dot(sen_emb)/norm(sen_emb) #cosine similarity\n",
        "  proj2 = (sent2_emb/norm(sent2_emb)).dot(sen_emb)/norm(sen_emb)\n",
        "  #print(proj1,proj2)\n",
        "  return proj1, proj2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iJFAY9VHTfD3",
        "colab_type": "code",
        "outputId": "8fc2e876-ce19-47d0-c513-5b1e3cdd76df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Get male sentences results\n",
        "embedding = elmo_embedding\n",
        "negC = negC_elmo\n",
        "p= []\n",
        "for sen in ['anger','fear','joy', 'sadness']:\n",
        "  aa = []\n",
        "  ea = []\n",
        "  aa_cn = []\n",
        "  ea_cn = []\n",
        "  sent_sent = []\n",
        "  sent_aa = []\n",
        "  sent_ea = []\n",
        "  sent_sent = EEC[EEC['Emotion']==sen] # get sentence of certain sentiment\n",
        "  sent_aa = sent_sent[(sent_sent['Race']=='African-American')] #get AA male sentences\n",
        "  sent_ea = sent_sent[(sent_sent['Race']=='European')] #get EA male sentences\n",
        "  sen_emb  = pick_embeddings(brown_corpus, elmo_data, str(sen)) # get certain sentiment embedding\n",
        "  sen_emb = np.average(sen_emb, axis = 0)\n",
        "  print(len(sent_ea))\n",
        "  for sen1, sen2 in zip(sent_aa['Sentence'], sent_ea['Sentence']):\n",
        "    \n",
        "    score1, score2 = run_raw(sen_emb, sen1, sen2) #projection score on raw data\n",
        "    score3, score4 = run_conceptor(sen_emb, sen1, sen2) # projection score on CN\n",
        "    aa.append(score1) # raw score for african american\n",
        "    ea.append(score2) # raw score for european american\n",
        "    aa_cn.append(score3) # CN score for african american\n",
        "    ea_cn.append(score4) # CN score for european american\n",
        "  #paired t-test for raw data\n",
        "  p1 = scipy.stats.ttest_rel(aa, ea)\n",
        "  #paired t-test for CN data\n",
        "  p2 = scipy.stats.ttest_rel(aa_cn, ea_cn)\n",
        "  p.append([p1.statistic, p1.pvalue, p2.statistic, p2.pvalue])\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "700\n",
            "700\n",
            "700\n",
            "700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Bicd-aY9i1Ya",
        "colab_type": "code",
        "outputId": "2fd97bff-7d1a-44db-d367-edbd26d612c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "print('RAW vs CN data using ELMO, racial debiasing')\n",
        "p = pd.DataFrame(p, index=['anger','fear','joy','sadness'], columns= ['raw_t_statistic', 'raw_p_value', 'cn_t_statistic', 'cn_p_value'])\n",
        "print(p)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RAW vs CN data using ELMO, racial debiasing\n",
            "         raw_t_statistic   raw_p_value  cn_t_statistic    cn_p_value\n",
            "anger          -6.439617  2.227122e-10      -11.236909  4.771182e-27\n",
            "fear           -4.731632  2.696043e-06       -9.460337  4.574014e-20\n",
            "joy            -5.928437  4.804858e-09       -3.817702  1.466544e-04\n",
            "sadness        -4.916006  1.101645e-06       -9.110037  8.550129e-19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bAoKAMixcgJA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Bert"
      ]
    },
    {
      "metadata": {
        "id": "9jrDAQk1chdU",
        "colab_type": "code",
        "outputId": "82b3977a-de35-4246-81a9-f5b7165e8977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Get male sentences results\n",
        "embedding = bert_embedding\n",
        "negC = negC_bert\n",
        "p= []\n",
        "i = 0\n",
        "for sen in ['fear','sadness','joy', 'anger']:\n",
        "  aa = []\n",
        "  ea = []\n",
        "  aa_cn = []\n",
        "  ea_cn = []\n",
        "  sent_sent = []\n",
        "  sent_aa = []\n",
        "  sent_ea = []\n",
        "  sent_sent = EEC[EEC['Emotion']==sen] # get sentence of certain sentiment\n",
        "  sent_aa = sent_sent[(sent_sent['Race']=='African-American')] #get AA male sentences\n",
        "  sent_ea = sent_sent[(sent_sent['Race']=='European')] #get EA male sentences\n",
        "  sen_emb = np.squeeze(np.asarray(senn['type_embedings'][i]))\n",
        "  i+=1\n",
        "  print(len(sent_ea))\n",
        "  for sen1, sen2 in zip(sent_aa['Sentence'], sent_ea['Sentence']):\n",
        "    \n",
        "    score1, score2 = run_raw(sen_emb, sen1, sen2) #projection score on raw data\n",
        "    score3, score4 = run_conceptor(sen_emb, sen1, sen2) # projection score on CN\n",
        "    aa.append(score1) # raw score for african american\n",
        "    ea.append(score2) # raw score for european american\n",
        "    aa_cn.append(score3) # CN score for african american\n",
        "    ea_cn.append(score4) # CN score for european american\n",
        "  #paired t-test for raw data\n",
        "  p1 = scipy.stats.ttest_rel(aa, ea)\n",
        "  #paired t-test for CN data\n",
        "  p2 = scipy.stats.ttest_rel(aa_cn, ea_cn)\n",
        "  p.append([p1.statistic, p1.pvalue, p2.statistic, p2.pvalue])\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "700\n",
            "700\n",
            "700\n",
            "700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JEoQOavtqNNA",
        "colab_type": "code",
        "outputId": "745cd69a-d756-4c59-9230-83706223519a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "print('RAW vs CN data using BERT, racial debiasing')\n",
        "p = pd.DataFrame(p, index=['fear','sadness','joy','anger'], columns= ['raw_t_statistic', 'raw_p_value', 'cn_t_statistic', 'cn_p_value'])\n",
        "print(p)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RAW vs CN data using BERT, racial debiasing\n",
            "         raw_t_statistic   raw_p_value  cn_t_statistic    cn_p_value\n",
            "fear            5.647380  2.370618e-08       22.624530  1.853297e-85\n",
            "sadness        -3.449800  5.946922e-04        3.267547  1.137887e-03\n",
            "joy             4.756097  2.398224e-06        9.040590  1.512630e-18\n",
            "anger           1.633738  1.027643e-01       13.350301  2.237348e-36\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}