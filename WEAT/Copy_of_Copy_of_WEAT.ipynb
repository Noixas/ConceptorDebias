{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of WEAT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsedoc/ConceptorDebias/blob/ACL-cleanup/WEAT/Copy_of_Copy_of_WEAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "bY4RMFQxrWyv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from itertools import combinations, filterfalse\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import pandas as pd\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l8Me4_OyFyKo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# WEAT Algorithm\n",
        "The Word Embeddings Association Test (WEAT), as proposed by Calikson et. al., is a statistical test analogous to the Implicit Association Test (IAT) which helps quantify human biases in textual data. WEAT uses the cosine similarity between word embeddings which is analogous to the reaction time when subjects are asked to pair two concepts they find similar in the IAT.  WEAT considers two sets of target words and two sets of attribute words of equal size. The null hypothesis is that there is no difference between the two sets of target words and the sets of attribute words in terms of their relative similarities measured as the cosine similarity between the embeddings. For example, consider the target sets as words representing *Career* and *Family* and let the two sets of attribute words be *Male* and *Female* in that order. The null hypothesis states that *Career* and *Family* are equally similar (mathematically, in terms of the mean cosine similarity between the word representations) to each of the words in the *Male* and *Female* word lists. \n",
        "\n",
        "REF: https://gist.github.com/SandyRogers/e5c2e938502a75dcae25216e4fae2da5\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "oQFPEITekNnV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test Statistic\n",
        "\n",
        "The WEAT test statistic measures the differential association of the two sets of target words with the attribute.\n",
        "\n",
        "To ground this, we cast WEAT in our formulation where $\\mathcal{X}$ and $\\mathcal{Y}$ are two sets of target\n",
        "words, (concretely, $\\mathcal{X}$ might be*Career* words and $\\mathcal{Y}$ *Family* words) and $\\mathcal{A}$, $\\mathcal{B}$ are two sets of attribute words ($\\mathcal{A}$ might be ''female'' names and $\\mathcal{B}$  ''male'' names) assumed to associate with the bias concept(s). WEAT is then\n",
        "\\begin{align*}\n",
        "s(\\mathcal{X}, &\\mathcal{Y}, \\mathcal{A}, \\mathcal{B}) \\\\ &= \\frac{1}{|\\mathcal{X}|}\\Bigg[\\sum_{x \\in \\mathcal{X}}{\\Big[\\sum_{a\\in \\mathcal{A}}{s(x,a)} - \\sum_{b\\in \\mathcal{B}}{s(x,b)}\\Big]} \\\\ &\\hbox{}  - \\sum_{y \\in \\mathcal{Y}}{\\Big[\\sum_{a\\in \\mathcal{A}}{s(y,a)} - \\sum_{b\\in \\mathcal{B}}{s(y,b)}\\Big]}\\Bigg],\n",
        "\\end{align*}\n",
        "where $s(x,y) = \\cos(\\hbox{vec}(x), \\hbox{vec}(y))$ and $\\hbox{vec}(x) \\in \\mathbb{R}^k$ is the $k$-dimensional word embedding for word $x$. We assume that there is no overlap between any of the sets $\\mathcal{X}$, $\\mathcal{Y}$, $\\mathcal{A}$, and $\\mathcal{B}$.\n",
        "\n",
        "Note that for this definition of WEAT, the cardinality of the sets must be equal, so $|\\mathcal{A}|=|\\mathcal{B}|$ and $|\\mathcal{X}|=|\\mathcal{Y}|$. Our  conceptor formulation given below relaxes this assumption."
      ]
    },
    {
      "metadata": {
        "id": "mf6_liysF8en",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def swAB(W, A, B):\n",
        "  \"\"\"Calculates differential cosine-similarity between word vectors in W, A and W, B\n",
        "     Arguments\n",
        "              W, A, B : n x d matrix of word embeddings stored row wise\n",
        "  \"\"\"\n",
        "  WA = cosine_similarity(W,A)\n",
        "  WB = cosine_similarity(W,B)\n",
        "  \n",
        "  #Take mean along columns\n",
        "  WAmean = np.mean(WA, axis = 1)\n",
        "  WBmean = np.mean(WB, axis = 1)\n",
        "  \n",
        "  return (WAmean - WBmean)\n",
        "  \n",
        "def test_statistic(X, Y, A, B):\n",
        "  \"\"\"Calculates test-statistic between the pair of association words and target words\n",
        "     Arguments\n",
        "              X, Y, A, B : n x d matrix of word embeddings stored row wise\n",
        "     Returns\n",
        "              Test Statistic\n",
        "  \"\"\"\n",
        "  return (sum(swAB(X, A, B)) - sum(swAB(Y, A, B)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m9ZO4--vpBh6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Effect Size\n",
        "\n",
        "The ''effect size'' is a normalized measure of how separated the two distributions are."
      ]
    },
    {
      "metadata": {
        "id": "EHPKwHLjo-m8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weat_effect_size(X, Y, A, B, embd):\n",
        "  \"\"\"Computes the effect size for the given list of association and target word pairs\n",
        "     Arguments\n",
        "              X, Y : List of association words\n",
        "              A, B : List of target words\n",
        "              embd : Dictonary of word-to-embedding for all words\n",
        "     Returns\n",
        "              Effect Size\n",
        "  \"\"\"\n",
        "  \n",
        "  Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
        "  Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
        "  Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
        "  Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
        "  \n",
        "  XuY = list(set(X).union(Y))\n",
        "  XuYmat = []\n",
        "  for w in XuY:\n",
        "    if w.lower() in embd:\n",
        "      XuYmat.append(embd[w.lower()])\n",
        "  XuYmat = np.array(XuYmat)\n",
        "\n",
        "  \n",
        "  d = (np.mean(swAB(Xmat,Amat,Bmat)) - np.mean(swAB(Ymat,Amat,Bmat)))/np.std(swAB(XuYmat, Amat, Bmat))\n",
        "  \n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d8foGwVSGI16",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## P-Value\n",
        "\n",
        "The one-sided P value measures the likelihood that a random permutation of the attribute words would produce at least the observed test statistic"
      ]
    },
    {
      "metadata": {
        "id": "ZDy-duFOFj71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def random_permutation(iterable, r=None):\n",
        "  \"\"\"Returns a random permutation for any iterable object\"\"\"\n",
        "  pool = tuple(iterable)\n",
        "  r = len(pool) if r is None else r\n",
        "  return tuple(random.sample(pool, r))\n",
        "\n",
        "def weat_p_value(X, Y, A, B, embd, sample = 1000):\n",
        "  \"\"\"Computes the one-sided P value for the given list of association and target word pairs\n",
        "     Arguments\n",
        "              X, Y : List of association words\n",
        "              A, B : List of target words\n",
        "              embd : Dictonary of word-to-embedding for all words\n",
        "              sample : Number of random permutations used.\n",
        "     Returns\n",
        "  \"\"\"\n",
        "  size_of_permutation = min(len(X), len(Y))\n",
        "  X_Y = X + Y\n",
        "  test_stats_over_permutation = []\n",
        "  \n",
        "  Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
        "  Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
        "  Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
        "  Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
        "  \n",
        "  if not sample:\n",
        "      permutations = combinations(X_Y, size_of_permutation)\n",
        "  else:\n",
        "      permutations = [random_permutation(X_Y, size_of_permutation) for s in range(sample)]\n",
        "      \n",
        "  for Xi in permutations:\n",
        "    Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
        "    Ximat = np.array([embd[w.lower()] for w in Xi if w.lower() in embd])\n",
        "    Yimat = np.array([embd[w.lower()] for w in Yi if w.lower() in embd])\n",
        "    test_stats_over_permutation.append(test_statistic(Ximat, Yimat, Amat, Bmat))\n",
        "    \n",
        "  unperturbed = test_statistic(Xmat, Ymat, Amat, Bmat)\n",
        "  \n",
        "  is_over = np.array([o > unperturbed for o in test_stats_over_permutation])\n",
        "  \n",
        "  return is_over.sum() / is_over.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NbRMmhwbGL98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test on sample input"
      ]
    },
    {
      "metadata": {
        "id": "upscdvtwFm0l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"List of association and target word pairs for the sample test (Instruments, Weapons) vs (Pleasant, Unpleasant)\"\"\"\n",
        "\n",
        "# Instruments\n",
        "X = [\"bagpipe\", \"cello\", \"guitar\", \"lute\", \"trombone\", \"banjo\", \"clarinet\", \"harmonica\", \"mandolin\", \"trumpet\", \"bassoon\", \"drum\", \"harp\", \"oboe\", \"tuba\", \"bell\", \"fiddle\", \"harpsichord\", \"piano\", \"viola\", \"bongo\",\n",
        "\"flute\", \"horn\", \"saxophone\", \"violin\"] \n",
        "# Weapons\n",
        "Y = [\"arrow\", \"club\", \"gun\", \"missile\", \"spear\", \"axe\", \"dagger\", \"harpoon\", \"pistol\", \"sword\", \"blade\", \"dynamite\", \"hatchet\", \"rifle\", \"tank\", \"bomb\", \"firearm\", \"knife\", \"shotgun\", \"teargas\", \"cannon\", \"grenade\",\n",
        "    \"mace\", \"slingshot\", \"whip\"] \n",
        "# Pleasant\n",
        "A = [\"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\",\n",
        "    \"happy\", \"laughter\", \"paradise\", \"vacation\"] \n",
        "# Unpleasant\n",
        "B = [\"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\",\n",
        "    \"vomit\", \"agony\", \"prison\"] \n",
        "\n",
        "\"\"\"Download the 'Glove' embeddings if not downloaded\"\"\"\n",
        "!if [ ! -f /content/gensim_glove.840B.300d.txt.bin ]; then gdown https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2; fi\n",
        "\n",
        "\"\"\"Load the embeddings to a gensim object\"\"\"\n",
        "resourceFile = ''\n",
        "glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
        "print('The glove embedding has been loaded!')\n",
        "\n",
        "\"\"\"Compute the effect-size and P value\"\"\"\n",
        "print('WEAT d = ', weat_effect_size(X, Y, A, B, glove))\n",
        "print('WEAT p = ', weat_p_value(X, Y, A, B, glove, 1000))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rfwYNFl2ckGy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load all vectors and compute the conceptor\n",
        "\n",
        "A conceptor matrix, $C$, is a regularized identity map (in our case, from the original word embeddings to their debiased versions) that minimizes\n",
        "\n",
        "\\begin{equation}\n",
        "\\|Z - CZ\\|_F^2 + \\alpha^{-2}\\|C\\|_{F}^2.\n",
        "\\end{equation}\n",
        "\n",
        "where $\\alpha^{-2}$ is a scalar parameter.\n",
        "Given that many readers will be unfamiliar with conceptors, we reintroduce matrix conceptors. \n",
        "\n",
        "$C$ has a closed form solution: \n",
        "\n",
        "\\begin{equation}\n",
        "C = \\frac{1}{k} Z Z^{\\top} (\\frac{1}{k} Z Z^{\\top}+\\alpha^{-2} I)^{-1}.\n",
        "\\end{equation}\n",
        "\n",
        "Intuitively, $C$ is a soft projection matrix on the linear subspace where the word embeddings $Z$ have the highest variance. Once $C$ has been learned, it can be 'negated' by subtracting it from the identity matrix and then applied to any word embeddings (e.g., those defined by the lists, $\\mathcal{X}$ and $\\mathcal{Y}$) to remove the biased subspace.\n",
        "\n",
        "Conceptors can represent laws of Boolean logic, such as NOT $\\neg$, AND $\\wedge$ and OR $\\vee$. For two conceptors $C$ and $B$, we define the following operations:\n",
        "\\begin{align*}\n",
        "\\neg C:=&  I-C,  \\\\\n",
        "C\\wedge B:=&  (C^{-1} + B^{-1} - I)^{-1} \\\\\n",
        "C \\vee B:=&\\neg(\\neg C \\wedge \\neg B) \n",
        "\\end{align*}\n",
        "\n",
        "Given that the conceptor, $C$, represents the subspace of maximum bias, we want to apply the negated conceptor, NOT $C$ to an embedding space remove its bias. We call NOT $C$ the *debiasing conceptor*. More generally, if we have $K$ conceptors, $C_i$ derived from $K$ different word lists, we call NOT $(C_1 \\vee ... \\vee C_K)$ a debiasing conceptor. "
      ]
    },
    {
      "metadata": {
        "id": "xEuq-Lt5ctV2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Clone our repository. The repository contains code for computing the conceptors. It also includes word lists needed representing different subspaces."
      ]
    },
    {
      "metadata": {
        "id": "vqaF4DyZxjLl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# our code for debiasing -- also includes word lists\n",
        "!rm -r ConceptorDebias\n",
        "!git clone https://github.com/jsedoc/ConceptorDebias\n",
        "!cd ConceptorDebias; git checkout ACL-cleanup\n",
        "\n",
        "sys.path.append('/content/ConceptorDebias')\n",
        "\n",
        "from Conceptors.conceptor_fxns import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_BoXwMINw0sA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Compute the conceptor matrix"
      ]
    },
    {
      "metadata": {
        "id": "HAG4oqwIc1Z3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def process_cn_matrix(subspace, alpha = 2):\n",
        "  \"\"\"Returns the conceptor negation matrix\n",
        "  Arguments\n",
        "           subspace : n x d matrix of word vectors from a oarticular subspace\n",
        "           alpha : Tunable parameter\n",
        "  \"\"\"\n",
        "  # Compute the conceptor matrix\n",
        "  C,_ = train_Conceptor(subspace, alpha)\n",
        "  \n",
        "  # Calculate the negation of the conceptor matrix\n",
        "  negC = NOT(C)\n",
        "  \n",
        "  return negC\n",
        "\n",
        "def apply_conceptor(x, C):\n",
        "  \"\"\"Returns the conceptored embeddings\n",
        "  Arguments\n",
        "           x : n x d matrix of all words to be conceptored\n",
        "           C : d x d conceptor matrix\n",
        "  \"\"\"\n",
        "  # Post-process the vocab matrix\n",
        "  newX = (C @ x).T\n",
        "  \n",
        "  return newX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GHaXxx5L3Ugb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load embeddings of all words from the ref. wordlist from a specific embedding"
      ]
    },
    {
      "metadata": {
        "id": "8WBI1VED3bpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_all_vectors(embd, wikiWordsPath):\n",
        "  \"\"\"Loads all word vectors for all words in the list of words as a matrix\n",
        "  Arguments\n",
        "           embd : Dictonary of word-to-embedding for all words\n",
        "           wikiWordsPath : URL to the path where all embeddings are stored\n",
        "  Returns\n",
        "          all_words_index : Dictonary of words to the row-number of the corresponding word in the matrix\n",
        "          all_words_mat : Matrix of word vectors stored row-wise\n",
        "  \"\"\"\n",
        "  all_words_index = {}\n",
        "  all_words_mat = []\n",
        "  with open(wikiWordsPath, \"r+\") as f_in:\n",
        "    ind = 0\n",
        "    for line in f_in:\n",
        "      word = line.split(' ')[0]\n",
        "      if word in embd:\n",
        "        all_words_index[word] = ind\n",
        "        all_words_mat.append(embd[word])\n",
        "        ind = ind+1\n",
        "        \n",
        "  return all_words_index, all_words_mat\n",
        "\n",
        "def load_subspace_vectors(embd, subspace_words):\n",
        "  \"\"\"Loads all word vectors for the particular subspace in the list of words as a matrix\n",
        "  Arguments\n",
        "           embd : Dictonary of word-to-embedding for all words\n",
        "           subspace_words : List of words representing a particular subspace\n",
        "  Returns\n",
        "          subspace_embd_mat : Matrix of word vectors stored row-wise\n",
        "  \"\"\"\n",
        "  subspace_embd_mat = []\n",
        "  ind = 0\n",
        "  for word in subspace_words:\n",
        "    if word in embd:\n",
        "      subspace_embd_mat.append(embd[word])\n",
        "      ind = ind+1\n",
        "      \n",
        "  return subspace_embd_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EzfTYgq0I9WW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load all word lists for the ref. subspace"
      ]
    },
    {
      "metadata": {
        "id": "D4kndsi_DwNr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# General word list\n",
        "!wget https://raw.githubusercontent.com/IlyaSemenov/wikipedia-word-frequency/master/results/enwiki-20150602-words-frequency.txt\n",
        "!git clone https://github.com/PrincetonML/SIF\n",
        "    \n",
        "# Gender word lists\n",
        "!git clone https://github.com/uclanlp/gn_glove\n",
        "!git clone https://github.com/uclanlp/corefBias\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/female.txt\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/male.txt\n",
        "\n",
        "from lists.load_word_lists import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "orBvfpqCFOfa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Load list of pronouns representing the 'Pronoun' subspace for gender debiasing\"\"\"\n",
        "gender_list_pronouns = WEATLists.W_7_Male_terms + WEATLists.W_7_Female_terms + WEATLists.W_8_Male_terms + WEATLists.W_8_Female_terms\n",
        "gender_list_pronouns = list(set(gender_list_pronouns))\n",
        "\n",
        "\"\"\"Load an extended list of words representing the gender subspace for gender debiasing\"\"\"\n",
        "gender_list_extended = male_vino_extra + female_vino_extra + male_gnGlove + female_gnGlove\n",
        "gender_list_extended = list(set(gender_list_extended))\n",
        "\n",
        "\"\"\"Load list of proper nouns representing the 'Proper Noun' subspace for gender debiasing\"\"\"\n",
        "gender_list_propernouns = male_cmu + female_cmu\n",
        "gender_list_propernouns = list(set(gender_list_propernouns))\n",
        "\n",
        "\"\"\"Load list of all representing the gender subspace for gender debiasing\"\"\"\n",
        "gender_list_all = gender_list_pronouns + gender_list_extended + gender_list_propernouns\n",
        "gender_list_all = list(set(gender_list_all))\n",
        "\n",
        "\"\"\"Load list of common black and white names for racial debiasing\"\"\"\n",
        "race_list = WEATLists.W_3_Unused_full_list_European_American_names + WEATLists.W_3_European_American_names + WEATLists.W_3_Unused_full_list_African_American_names + WEATLists.W_3_African_American_names + WEATLists.W_4_Unused_full_list_European_American_names + WEATLists.W_4_European_American_names + WEATLists.W_4_Unused_full_list_African_American_names + WEATLists.W_4_African_American_names + WEATLists.W_5_Unused_full_list_European_American_names + WEATLists.W_5_European_American_names + WEATLists.W_5_Unused_full_list_African_American_names + WEATLists.W_5_African_American_names \n",
        "race_list = list(set(race_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kEiBEVI2I2WR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load different embeddings"
      ]
    },
    {
      "metadata": {
        "id": "Nrlqu2xvJH_6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Glove**"
      ]
    },
    {
      "metadata": {
        "id": "PFNSjmY8I1Ut",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Download the 'Glove' embeddings if not downloaded\"\"\"\n",
        "!if [ ! -f /content/gensim_glove.840B.300d.txt.bin ]; then gdown https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2; fi\n",
        "\n",
        "\"\"\"Load the embeddings to a gensim object\"\"\"\n",
        "resourceFile = ''\n",
        "if 'glove' not in dir():\n",
        "  glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
        "  print('The glove embedding has been loaded!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aD-bmiCdGKYt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Sample output of the glove embeddings\"\"\"\n",
        "X = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "print(X)\n",
        "a = [glove[w] for w in X if w.lower() in glove]\n",
        "print(np.array(a).shape)\n",
        "glove['Brad']\n",
        "#glove['brad']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "84af1zflJKLQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Word2ve**c"
      ]
    },
    {
      "metadata": {
        "id": "OmJlU2IiJMMj",
        "colab_type": "code",
        "outputId": "1b302430-806a-436e-addc-1eae7d51ef1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Download the 'Word2Vec' embeddings if not downloaded\"\"\"\n",
        "!if test -e /content/GoogleNews-vectors-negative300.bin.gz || test -e /content/GoogleNews-vectors-negative300.bin; then echo 'file already downloaded'; else echo 'starting download'; gdown https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM; fi\n",
        "!if [ ! -f /content/GoogleNews-vectors-negative300.bin ]; then gunzip GoogleNews-vectors-negative300.bin.gz; fi\n",
        "\n",
        "\"\"\"Load the embeddings to a gensim object\"\"\"\n",
        "resourceFile = ''\n",
        "if 'word2vec' not in dir():\n",
        "  word2vec = KeyedVectors.load_word2vec_format(resourceFile + 'GoogleNews-vectors-negative300.bin', binary=True)\n",
        "  print('The word2vec embedding has been loaded!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file already downloaded\n",
            "The word2vec embedding has been loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FCiZfu13JPHl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Fasttex**t"
      ]
    },
    {
      "metadata": {
        "id": "A9zwQ2T_JRBo",
        "colab_type": "code",
        "outputId": "454342eb-e607-451e-e70d-97f1270a2ef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Download the 'Fasttext' embeddings if not downloaded\"\"\"\n",
        "!if [ ! -f /content/fasttext.bin ]; then gdown https://drive.google.com/uc?id=1Zl6a75Ybf8do9uupmrJWKQMnvqqme4fh; fi\n",
        "\n",
        "\"\"\"Load the embeddings to a gensim object\"\"\"\n",
        "resourceFile = ''\n",
        "if 'fasttext' not in dir():\n",
        "  fasttext = KeyedVectors.load_word2vec_format(resourceFile + 'fasttext.bin', binary=True)\n",
        "  print('The fasttext embedding has been loaded!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The fasttext embedding has been loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7hL2zgX6JRhG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**ELMo**"
      ]
    },
    {
      "metadata": {
        "id": "cXvK1wuvJTVP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Download the 'ELMo' embeddings if not downloaded\"\"\"\n",
        "!if [ ! -f /content/elmo_embeddings_emma_brown.pkl ]; then gdown https://drive.google.com/uc?id=17TK2h3cz7amgm2mCY4QCYy1yh23ZFWDU; fi\n",
        "\n",
        "\"\"\"Load the embeddings to a dictonary\"\"\"\n",
        "data = pickle.load(open(\"elmo_embeddings_emma_brown.pkl\", \"rb\"))\n",
        "\n",
        "def pick_embeddings(corpus, sent_embs):\n",
        "    X = []\n",
        "    labels = {}\n",
        "    sents = []\n",
        "    ind = 0\n",
        "    for i, s in enumerate(corpus):\n",
        "        for j, w in enumerate(s):\n",
        "            X.append(sent_embs[i][j])\n",
        "            if w.lower() in labels:\n",
        "              labels[w.lower()].append(ind)\n",
        "            else:\n",
        "              labels[w.lower()] = [ind]\n",
        "            sents.append(s)\n",
        "            ind = ind + 1\n",
        "    return (X, labels, sents)\n",
        "  \n",
        "def get_word_list(path):\n",
        "    word_list = []\n",
        "    with open(path, \"r+\") as f_in:\n",
        "      for line in f_in:\n",
        "        word = line.split(' ')[0]\n",
        "        word_list.append(word.lower())\n",
        "\n",
        "    return word_list\n",
        "\n",
        "def load_subspace_vectors_contextual(all_mat, all_index, subspace_list):\n",
        "    subspace_mat = []\n",
        "    for w in subspace_list:\n",
        "      if w.lower() in all_index:\n",
        "        for i in all_index[w.lower()]:\n",
        "          #print(type(i))\n",
        "          subspace_mat.append(all_mat[i])\n",
        "    #subspace_mat = [all_mat[i,:] for i in all_index[w.lower()] for w in subspace_list if w.lower() in all_index]\n",
        "    print(\"Subspace: \", np.array(subspace_mat).shape)\n",
        "    return subspace_mat\n",
        "  \n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "nltk.download('brown')\n",
        "\n",
        "brown_corpus = brown.sents()\n",
        "elmo = data['brown_embs']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5TkER5fbdsFO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**BERT**"
      ]
    },
    {
      "metadata": {
        "id": "hDL0wCtSwoOx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_bert(all_dict, subspace):\n",
        "  \"\"\"Loads all embeddings in a matrix and a dictonary of words to row numbers\"\"\"\n",
        "  all_mat = all_dict['big_bert_' + subspace + '.pkl']['type_embedings']\n",
        "  words = []\n",
        "  for name in all_dict:\n",
        "    all_mat = np.concatenate((all_mat, all_dict[name]['type_embedings']))\n",
        "    words += all_dict[name]['words']\n",
        "  \n",
        "  words = [w.lower() for w in words]\n",
        "  all_words_index = {}\n",
        "  for i,a in enumerate(words):\n",
        "    all_words_index[a] = i\n",
        "    \n",
        "  return all_words_index, all_mat\n",
        "\n",
        "def load_bert_conceptor(all_dict, subspace):\n",
        "  \"\"\"Loads the required BERT conceptor matrix\"\"\"\n",
        "  if subspace == 'gender_list_pronouns':\n",
        "    cn = all_dict['big_bert_gender_list_pronouns.pkl']['GnegC']\n",
        "  elif subspace == 'gender_list_propernouns':\n",
        "    cn = all_dict['big_bert_gender_list_propernouns.pkl']['GnegC']\n",
        "  elif subspace == 'gender_list_extended':\n",
        "    cn = all_dict['big_bert_gender_list_extended.pkl']['GnegC']\n",
        "  elif subspace == 'gender_list_all':\n",
        "    cn = all_dict['big_bert_gender_list_all.pkl']['GnegC']\n",
        "  elif subspace == 'race_list':\n",
        "    cn = all_dict['big_bert_race_list.pkl']['GnegC']\n",
        "  \n",
        "  return cn\n",
        "\n",
        "\"\"\"Load all bert embeddings in a dictonary\"\"\"\n",
        "all_dict = {}\n",
        "for filename in os.listdir('/home/saketk/bert'):\n",
        "  all_dict[filename] = pickle.load(open(filename, \"rb\"))\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wkVyOfPKuzpp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Custom embeddings (From text file)**"
      ]
    },
    {
      "metadata": {
        "id": "bD4pT7DDu5si",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Download your custom embeddings (text file)\"\"\"\n",
        "\n",
        "\"\"\"Convert to word2vec format (if in GloVe format)\"\"\"\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "input_file = 'glove.txt'\n",
        "output_file = 'word2vec.txt'\n",
        "glove2word2vec(input_file, output_file)\n",
        "\n",
        "\"\"\"Load embeddings as gensim object\"\"\"\n",
        "custom = KeyedVectors.load_word2vec_format(output_file, binary=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KIvNmLw-vPWN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# WEAT on Conceptored embeddings"
      ]
    },
    {
      "metadata": {
        "id": "IPP2UaVDy2r_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Initialize variables"
      ]
    },
    {
      "metadata": {
        "id": "1ouWsWnl0csn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "resourceFile = ''\n",
        "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "\"\"\"Set the embedding to be used\"\"\"\n",
        "embd = 'glove'\n",
        "\n",
        "\"\"\"Set the subspace to be tested on\"\"\"\n",
        "subspace = 'gender_list_all' \n",
        "\n",
        "\"\"\"Load association and target word pairs\"\"\"\n",
        "X = WEATLists.W_8_Science\n",
        "Y = WEATLists.W_8_Arts\n",
        "A = WEATLists.W_8_Male_terms\n",
        "B = WEATLists.W_8_Female_terms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPCAcond1_b2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the vectors as a matrix"
      ]
    },
    {
      "metadata": {
        "id": "X3-Nqa2J2FoF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "curr_embd = eval(embd)\n",
        "  \n",
        "\"\"\"Load all embeddings in a matrix of all words in the wordlist\"\"\"\n",
        "if embd == 'elmo':\n",
        "  all_words_mat, all_words_index, _ = pick_embeddings(brown_corpus, curr_embd)\n",
        "if embd == 'bert':\n",
        "  all_words_index, all_words_mat = load_bert(all_dict, subspace)\n",
        "else:\n",
        "  all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T8d4ERsozAVK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Compute the conceptor"
      ]
    },
    {
      "metadata": {
        "id": "dfO_XltO3hp5",
        "colab_type": "code",
        "outputId": "fe253d89-5f73-4bda-9b0b-7b46ab1dc169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Load the vectors for the words representing the subspace as a matrix and compute the respetive conceptor matrix\"\"\"\n",
        "if subspace != 'without_conceptor':\n",
        "  subspace_words_list = eval(subspace)\n",
        "  if subspace == 'gender_list_and':\n",
        "    if embd == 'elmo':\n",
        "      subspace_words_mat1 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_pronouns)\n",
        "      cn1 = process_cn_matrix(np.array(subspace_words_mat1).T, alpha = 8)\n",
        "\n",
        "      subspace_words_mat2 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_extended)\n",
        "      cn2 = process_cn_matrix(np.array(subspace_words_mat2).T, alpha = 3)\n",
        "\n",
        "      subspace_words_mat3 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_propernouns)\n",
        "      cn3 = process_cn_matrix(np.array(subspace_words_mat3).T, alpha = 10)\n",
        "\n",
        "      cn = AND(cn1, AND(cn2, cn3))\n",
        "    elif embd == 'bert':\n",
        "      cn1 = load_bert_conceptor(all_dict, gender_list_pronouns)\n",
        "      \n",
        "      cn2 = load_bert_conceptor(all_dict, gender_list_extended)\n",
        "      \n",
        "      cn3 = load_bert_conceptor(all_dict, gender_list_propernouns)\n",
        "      \n",
        "      cn = AND(cn1, AND(cn2, cn3))\n",
        "    else:\n",
        "      subspace_words_mat1 = load_subspace_vectors(curr_embd, gender_list_pronouns)\n",
        "      cn1 = process_cn_matrix(np.array(subspace_words_mat1).T)\n",
        "\n",
        "      subspace_words_mat2 = load_subspace_vectors(curr_embd, gender_list_extended)\n",
        "      cn2 = process_cn_matrix(np.array(subspace_words_mat2).T)\n",
        "\n",
        "      subspace_words_mat3 = load_subspace_vectors(curr_embd, gender_list_propernouns)\n",
        "      cn3 = process_cn_matrix(np.array(subspace_words_mat3).T)\n",
        "\n",
        "      cn = AND(cn1, AND(cn2, cn3))\n",
        "  else: \n",
        "    if embd == 'elmo':\n",
        "      subspace_words_mat = load_subspace_vectors_contextual(all_words_mat, all_words_index, subspace_words_list)\n",
        "      cn = process_cn_matrix(np.array(subspace_words_mat).T, alpha = 6)\n",
        "    elif embd == 'bert':\n",
        "      cn = load_bert_conceptor(all_dict, subspace)\n",
        "    else:\n",
        "      subspace_words_mat = load_subspace_vectors(curr_embd, subspace_words_list)\n",
        "      cn = process_cn_matrix(np.array(subspace_words_mat).T)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting...\n",
            "(300, 7270)\n",
            "R calculated\n",
            "C calculated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xj-aXqB2zOpI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Compute conceptored embeddings"
      ]
    },
    {
      "metadata": {
        "id": "XoQGSnAH4aZ-",
        "colab_type": "code",
        "outputId": "1797a958-4bd3-4925-80db-365175f81c40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Conceptor all embeddings\"\"\"\n",
        "all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "\n",
        "\"\"\"Store all conceptored words in a dictonary\"\"\"\n",
        "all_words = {}\n",
        "for word, index in all_words_index.items():\n",
        "  if embd == 'elmo':\n",
        "    all_words[word] = np.mean([all_words_cn[i,:] for i in index], axis = 0)\n",
        "  else:\n",
        "    all_words[word] = all_words_cn[index,:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128607, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cVmZBwykJmgn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Calculate WEAT scores"
      ]
    },
    {
      "metadata": {
        "id": "PPukMLU2ZosX",
        "colab_type": "code",
        "outputId": "3fe737ce-7f92-4dae-dee9-8470b9c01bc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "d = weat_effect_size(X, Y, A, B, all_words)\n",
        "p = weat_p_value(X, Y, A, B, all_words, 1000)\n",
        "\n",
        "print('WEAT d = ', d)\n",
        "print('WEAT p = ', p)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WEAT d =  0.6839112075804858\n",
            "WEAT p =  0.091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gbg0wsR_Ykym",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hard Debiasing"
      ]
    },
    {
      "metadata": {
        "id": "bEzUV_AOtRAd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mu et. al. Hard Debiasing"
      ]
    },
    {
      "metadata": {
        "id": "a0ZvHFuFtV_v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "def hard_debias(all_words, subspace):\n",
        "   \"\"\"Project off the first principal component (of the subspace) from all word vectors\n",
        "  Arguments\n",
        "           all_words : Matrix of word vectors of all words stored row-wise\n",
        "           subspace : Matrix of words representing a particular subspace stored row-wise\n",
        "  Returns\n",
        "          ret : Matrix of debiased word vectors stored row-wise\n",
        "  \"\"\"\n",
        "  all_words = np.array(all_words)\n",
        "  subspace = np.array(subspace)\n",
        "  \n",
        "  # Compute the first principal component of the subspace matrix\n",
        "  pca = PCA(n_components = 1)\n",
        "  pca.fit(subspace)\n",
        "  pc1 = np.array(pca.components_)\n",
        "  \n",
        "  # Project off the first PC from all word vectors\n",
        "  temp = (pc1.T @ (pc1 @ all_words.T)).T\n",
        "  ret = all_words - temp\n",
        "  \n",
        "  return ret\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "938VVWqdFxeE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Initialize variables"
      ]
    },
    {
      "metadata": {
        "id": "p2a2HaGBF4N4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "resourceFile = ''\n",
        "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "\"\"\"Set the embedding to be used\"\"\"\n",
        "embd = 'glove'\n",
        "\n",
        "\"\"\"Set the subspace to be tested on\"\"\"\n",
        "subspace = 'gender_list_all' \n",
        "\n",
        "\"\"\"Load association and target word pairs\"\"\"\n",
        "X = WEATLists.W_8_Science\n",
        "Y = WEATLists.W_8_Arts\n",
        "A = WEATLists.W_8_Male_terms\n",
        "B = WEATLists.W_8_Female_terms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KVR4ydFvF4Xe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the vectors as a matrix"
      ]
    },
    {
      "metadata": {
        "id": "6hfyet0eF4g9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "curr_embd = eval(embd)\n",
        "  \n",
        "\"\"\"Load all embeddings in a matrix of all words in the wordlist\"\"\"\n",
        "if embd == 'elmo':\n",
        "  all_words_mat, all_words_index, _ = pick_embeddings(brown_corpus, curr_embd)\n",
        "if embd == 'bert':\n",
        "  all_words_index, all_words_mat = load_bert(all_dict, subspace)\n",
        "else:\n",
        "  all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QnmMCGW7F4pw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Debias the embeddings"
      ]
    },
    {
      "metadata": {
        "id": "yzSiXFUlF47C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Load the vectors for the words representing the subspace as a matrix and compute the respetive conceptor matrix\"\"\"\n",
        "if subspace != 'without_conceptor' and subspace != 'gender_list_and':\n",
        "  subspace_words_list = eval(subspace)\n",
        "  \n",
        "if subspace != 'without_debiasing':\n",
        "  if embd == 'elmo' or embd == 'bert':\n",
        "    subspace_words_mat = load_subspace_vectors_contextual(all_words_mat, all_words_index, subspace_words_list)\n",
        "    all_words_cn = hard_debias(all_words_mat, subspace_words_mat)\n",
        "  else:\n",
        "    subspace_words_mat = load_subspace_vectors(curr_embd, subspace_words_list)\n",
        "    all_words_cn = hard_debias(all_words_mat, subspace_words_mat)\n",
        "else:\n",
        "  all_words_cn = all_words_mat\n",
        "\n",
        "all_words_cn = np.array(all_words_cn)\n",
        "\n",
        "#Store all conceptored words in a dictonary\n",
        "all_words = {}\n",
        "for word, index in all_words_index.items():\n",
        "  if embd == 'elmo' or embd == 'bert':\n",
        "    all_words[word] = np.mean([all_words_cn[i,:] for i in index], axis = 0)\n",
        "  else:\n",
        "    all_words[word] = all_words_cn[index,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Ary7T5NGI9F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Calculate WEAT scores"
      ]
    },
    {
      "metadata": {
        "id": "cthQht44GJPQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "d = weat_effect_size(X, Y, A, B, all_words)\n",
        "p = weat_p_value(X, Y, A, B, all_words, 1000)\n",
        "\n",
        "print('WEAT d = ', d)\n",
        "print('WEAT p = ', p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iP9IM02Ln-qp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bolukbasi hard debiasing"
      ]
    },
    {
      "metadata": {
        "id": "N-5CfftKoGCt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Helper methods to debias word embeddings as proposed by the original authors\"\"\"\n",
        "\n",
        "def doPCA(pairs, mat, index, num_components = 5):\n",
        "    matrix = []\n",
        "    for a, b in pairs:\n",
        "        center = (mat[index[a.lower()]] + mat[index[b.lower()]])/2\n",
        "        matrix.append(mat[index[a.lower()]] - center)\n",
        "        matrix.append(mat[index[b.lower()]] - center)\n",
        "    matrix = np.array(matrix)\n",
        "    pca = PCA(n_components = num_components)\n",
        "    pca.fit(matrix)\n",
        "    # bar(range(num_components), pca.explained_variance_ratio_)\n",
        "    return pca\n",
        "\n",
        "def drop(u, v):\n",
        "    return u - v * u.dot(v) / v.dot(v)\n",
        "  \n",
        "def normalize(all_words_mat):\n",
        "    all_words_mat /= np.linalg.norm(all_words_mat, axis=1)[:, np.newaxis]\n",
        "    return all_words_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GWOwX9wHoB-7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def debias(all_words_mat, all_words_index, gender_specific_words, definitional, equalize):\n",
        "    \"\"\"Debiases the word vectors as proposed by the original authors\n",
        "    Arguments\n",
        "             all_words_mat : Matrix of word vectors of all words stored row-wise\n",
        "             all_words_index : Dictonary of words to row number in the matrix\n",
        "             gender_specific_words : List of words defining the subspace\n",
        "             definitional : List of definitional words\n",
        "             equalize : List of tuples defined as the set of equalize pairs (downloaded)\n",
        "    Returns\n",
        "            all_words_mat : Matrix of debiased word vectors stored row-wise\n",
        "    \"\"\"\n",
        "    gender_direction = doPCA(definitional, all_words_mat, all_words_index).components_[0]\n",
        "    specific_set = set(gender_specific_words)\n",
        "    for w in list(all_words_index.keys()):\n",
        "        if w not in specific_set:\n",
        "            all_words_mat[all_words_index[w.lower()]] = drop(all_words_mat[all_words_index[w.lower()]], gender_direction)\n",
        "    all_words_mat = normalize(all_words_mat)\n",
        "    candidates = {x for e1, e2 in equalize for x in [(e1.lower(), e2.lower()),\n",
        "                                                     (e1.title(), e2.title()),\n",
        "                                                     (e1.upper(), e2.upper())]}\n",
        "    print(candidates)\n",
        "    for (a, b) in candidates:\n",
        "        if (a.lower() in all_words_index and b.lower() in all_words_index):\n",
        "            y = drop((all_words_mat[all_words_index[a.lower()]] + all_words_mat[all_words_index[b.lower()]]) / 2, gender_direction)\n",
        "            z = np.sqrt(1 - np.linalg.norm(y)**2)\n",
        "            if (all_words_mat[all_words_index[a.lower()]] - all_words_mat[all_words_index[b.lower()]]).dot(gender_direction) < 0:\n",
        "                z = -z\n",
        "            all_words_mat[all_words_index[a.lower()]] = z * gender_direction + y\n",
        "            all_words_mat[all_words_index[b.lower()]] = -z * gender_direction + y\n",
        "    all_words_mat = normalize(all_words_mat)\n",
        "    return all_words_mat\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5J-1XQaGKVgx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Initialize Variables"
      ]
    },
    {
      "metadata": {
        "id": "Bj3J-omPKVoS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "resourceFile = ''\n",
        "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "!git clone https://github.com/tolga-b/debiaswe.git\n",
        "\n",
        "\"\"\"Load definitional and equalize lists\"\"\"\n",
        "%cd debiaswe_tutorial/debiaswe/\n",
        "# Lets load some gender related word lists to help us with debiasing\n",
        "with open('./data/definitional_pairs.json', \"r\") as f:\n",
        "    defs = json.load(f) #gender definitional words\n",
        "\n",
        "defs_list = []\n",
        "for pair in defs:\n",
        "  defs_list.append(pair[0])\n",
        "  defs_list.append(pair[1])\n",
        "\n",
        "with open('./data/equalize_pairs.json', \"r\") as f:\n",
        "    equalize_pairs = json.load(f) \n",
        "\n",
        "%cd ../../\n",
        "!ls\n",
        "\n",
        "\"\"\"Set the embedding to be used\"\"\"\n",
        "embd = 'glove'\n",
        "\n",
        "\"\"\"Set the subspace to be tested on\"\"\"\n",
        "subspace = 'gender_list_all' \n",
        "\n",
        "\"\"\"Load association and target word pairs\"\"\"\n",
        "X = WEATLists.W_8_Science\n",
        "Y = WEATLists.W_8_Arts\n",
        "A = WEATLists.W_8_Male_terms\n",
        "B = WEATLists.W_8_Female_terms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GK9EIJ5wKVvv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the vectors as a matrix"
      ]
    },
    {
      "metadata": {
        "id": "AwttPHYxKV3C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "curr_embd = eval(embd)\n",
        "  \n",
        "\"\"\"Load all embeddings in a matrix of all words in the wordlist\"\"\"\n",
        "all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PRIXvmLbKV9K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Debias the embeddings"
      ]
    },
    {
      "metadata": {
        "id": "GXqwn2aOKWEL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Load the vectors for the words representing the subspace as a matrix and compute the respetive conceptor matrix\"\"\"\n",
        "if subspace != 'without_conceptor' and subspace != 'gender_list_and':\n",
        "  subspace_words_list = eval(subspace)\n",
        "  \n",
        "if subspace != 'without_debiasing':\n",
        "  all_words_cn = debias(all_words_mat, all_words_index, subspace_words_list, defs, equalize_pairs)\n",
        "else:\n",
        "  all_words_cn = all_words_mat\n",
        "\n",
        "all_words_cn = np.array(all_words_cn)\n",
        "\n",
        "#Store all conceptored words in a dictonary\n",
        "all_words = {}\n",
        "for word, index in all_words_index.items():\n",
        "  all_words[word] = all_words_cn[index,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TvmetCG_Ke83",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Calculate WEAT scores"
      ]
    },
    {
      "metadata": {
        "id": "77y-UB_-KfE6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "d = weat_effect_size(X, Y, A, B, all_words)\n",
        "p = weat_p_value(X, Y, A, B, all_words, 1000)\n",
        "\n",
        "print('WEAT d = ', d)\n",
        "print('WEAT p = ', p)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}