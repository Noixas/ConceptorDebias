{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of WEAT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsedoc/ConceptorDebias/blob/ACL-cleanup/WEAT/Copy_of_Copy_of_WEAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "bY4RMFQxrWyv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from itertools import combinations, filterfalse\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import pandas as pd\n",
        "import random\n",
        "import sys\n",
        "import pickle\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l8Me4_OyFyKo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# WEAT Algorithm\n",
        "The Word Embeddings Association Test (WEAT), as proposed by Calikson et. al., is a statistical test analogous to the Implicit Association Test (IAT) which helps quantify human biases in textual data. WEAT uses the cosine similarity between word embeddings which is analogous to the reaction time when subjects are asked to pair two concepts they find similar in the IAT.  WEAT considers two sets of target words and two sets of attribute words of equal size. The null hypothesis is that there is no difference between the two sets of target words and the sets of attribute words in terms of their relative similarities measured as the cosine similarity between the embeddings. For example, consider the target sets as words representing *Career* and *Family* and let the two sets of attribute words be *Male* and *Female* in that order. The null hypothesis states that *Career* and *Family* are equally similar (mathematically, in terms of the mean cosine similarity between the word representations) to each of the words in the *Male* and *Female* word lists. \n",
        "\n",
        "REF: https://gist.github.com/SandyRogers/e5c2e938502a75dcae25216e4fae2da5\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "oQFPEITekNnV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test Statistic\n",
        "\n",
        "The WEAT test statistic measures the differential association of the two sets of target words with the attribute.\n",
        "\n",
        "To ground this, we cast WEAT in our formulation where $\\mathcal{X}$ and $\\mathcal{Y}$ are two sets of target\n",
        "words, (concretely, $\\mathcal{X}$ might be*Career* words and $\\mathcal{Y}$ *Family* words) and $\\mathcal{A}$, $\\mathcal{B}$ are two sets of attribute words ($\\mathcal{A}$ might be ''female'' names and $\\mathcal{B}$  ''male'' names) assumed to associate with the bias concept(s). WEAT is then~\\footnote{We assume that there is no overlap between any of the sets $\\mathcal{X}$, $\\mathcal{Y}$, $\\mathcal{A}$, and $\\mathcal{B}$.} \n",
        "\\begin{align*}\n",
        "s(\\mathcal{X}, &\\mathcal{Y}, \\mathcal{A}, \\mathcal{B}) \\\\ &= \\frac{1}{|\\mathcal{X}|}\\Bigg[\\sum_{x \\in \\mathcal{X}}{\\Big[\\sum_{a\\in \\mathcal{A}}{s(x,a)} - \\sum_{b\\in \\mathcal{B}}{s(x,b)}\\Big]} \\\\ &\\hbox{}  - \\sum_{y \\in \\mathcal{Y}}{\\Big[\\sum_{a\\in \\mathcal{A}}{s(y,a)} - \\sum_{b\\in \\mathcal{B}}{s(y,b)}\\Big]}\\Bigg],\n",
        "\\end{align*}\n",
        "where $s(x,y) = \\cos(\\hbox{vec}(x), \\hbox{vec}(y))$ and $\\hbox{vec}(x) \\in \\mathbb{R}^k$ is the $k$-dimensional word embedding for word $x$. \n",
        "Note that for this definition of WEAT, the cardinality of the sets must be equal, so $|\\mathcal{A}|=|\\mathcal{B}|$ and $|\\mathcal{X}|=|\\mathcal{Y}|$. Our  conceptor formulation given below relaxes this assumption."
      ]
    },
    {
      "metadata": {
        "id": "mf6_liysF8en",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def swAB(W, A, B):\n",
        "  \"\"\"Calculates differential cosine-similarity between word vectors in W, A and W, B\n",
        "     Arguments\n",
        "              W, A, B : n x d matrix of word embeddings stored row wise\n",
        "  \"\"\"\n",
        "  WA = cosine_similarity(W,A)\n",
        "  WB = cosine_similarity(W,B)\n",
        "  \n",
        "  #Take mean along columns\n",
        "  WAmean = np.mean(WA, axis = 1)\n",
        "  WBmean = np.mean(WB, axis = 1)\n",
        "  \n",
        "  return (WAmean - WBmean)\n",
        "  \n",
        "def test_statistic(X, Y, A, B):\n",
        "  \"\"\"Calculates test-statistic between the pair of association words and target words\n",
        "     Arguments\n",
        "              X, Y, A, B : n x d matrix of word embeddings stored row wise\n",
        "     Returns\n",
        "              Test Statistic\n",
        "  \"\"\"\n",
        "  return (sum(swAB(X, A, B)) - sum(swAB(Y, A, B)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m9ZO4--vpBh6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Effect Size\n",
        "\n",
        "The ''effect size'' is a normalized measure of how separated the two distributions are."
      ]
    },
    {
      "metadata": {
        "id": "EHPKwHLjo-m8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def weat_effect_size(X, Y, A, B, embd):\n",
        "  \"\"\"Computes the effect size for the given list of association and target word pairs\n",
        "     Arguments\n",
        "              X, Y : List of association words\n",
        "              A, B : List of target words\n",
        "              embd : Dictonary of word-to-embedding for all words\n",
        "     Returns\n",
        "              Effect Size\n",
        "  \"\"\"\n",
        "  \n",
        "  Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
        "  Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
        "  Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
        "  Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
        "  \n",
        "  XuY = list(set(X).union(Y))\n",
        "  XuYmat = []\n",
        "  for w in XuY:\n",
        "    if w.lower() in embd:\n",
        "      XuYmat.append(embd[w.lower()])\n",
        "  XuYmat = np.array(XuYmat)\n",
        "\n",
        "  \n",
        "  d = (np.mean(swAB(Xmat,Amat,Bmat)) - np.mean(swAB(Ymat,Amat,Bmat)))/np.std(swAB(XuYmat, Amat, Bmat))\n",
        "  \n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d8foGwVSGI16",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## P-Value\n",
        "\n",
        "The one-sided P value measures the likelihood that a random permutation of the attribute words would produce at least the observed test statistic"
      ]
    },
    {
      "metadata": {
        "id": "ZDy-duFOFj71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def random_permutation(iterable, r=None):\n",
        "  \"\"\"Returns a random permutation for any iterable object\"\"\"\n",
        "  pool = tuple(iterable)\n",
        "  r = len(pool) if r is None else r\n",
        "  return tuple(random.sample(pool, r))\n",
        "\n",
        "def weat_p_value(X, Y, A, B, embd, sample = 1000):\n",
        "  \"\"\"Computes the one-sided P value for the given list of association and target word pairs\n",
        "     Arguments\n",
        "              X, Y : List of association words\n",
        "              A, B : List of target words\n",
        "              embd : Dictonary of word-to-embedding for all words\n",
        "              sample : Number of random permutations used.\n",
        "     Returns\n",
        "  \"\"\"\n",
        "  size_of_permutation = min(len(X), len(Y))\n",
        "  X_Y = X + Y\n",
        "  test_stats_over_permutation = []\n",
        "  \n",
        "  Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
        "  Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
        "  Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
        "  Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
        "  \n",
        "  if not sample:\n",
        "      permutations = combinations(X_Y, size_of_permutation)\n",
        "  else:\n",
        "      permutations = [random_permutation(X_Y, size_of_permutation) for s in range(sample)]\n",
        "      \n",
        "  for Xi in permutations:\n",
        "    Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
        "    Ximat = np.array([embd[w.lower()] for w in Xi if w.lower() in embd])\n",
        "    Yimat = np.array([embd[w.lower()] for w in Yi if w.lower() in embd])\n",
        "    test_stats_over_permutation.append(test_statistic(Ximat, Yimat, Amat, Bmat))\n",
        "    \n",
        "  unperturbed = test_statistic(Xmat, Ymat, Amat, Bmat)\n",
        "  \n",
        "  is_over = np.array([o > unperturbed for o in test_stats_over_permutation])\n",
        "  \n",
        "  return is_over.sum() / is_over.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NbRMmhwbGL98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test on sample input"
      ]
    },
    {
      "metadata": {
        "id": "upscdvtwFm0l",
        "colab_type": "code",
        "outputId": "7703722a-fe9e-45cf-9930-82cb25c81920",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"List of association and target word pairs for the sample test (Instruments, Weapons) vs (Pleasant, Unpleasant)\"\"\"\n",
        "\n",
        "# Instruments\n",
        "X = [\"bagpipe\", \"cello\", \"guitar\", \"lute\", \"trombone\", \"banjo\", \"clarinet\", \"harmonica\", \"mandolin\", \"trumpet\", \"bassoon\", \"drum\", \"harp\", \"oboe\", \"tuba\", \"bell\", \"fiddle\", \"harpsichord\", \"piano\", \"viola\", \"bongo\",\n",
        "\"flute\", \"horn\", \"saxophone\", \"violin\"] \n",
        "# Weapons\n",
        "Y = [\"arrow\", \"club\", \"gun\", \"missile\", \"spear\", \"axe\", \"dagger\", \"harpoon\", \"pistol\", \"sword\", \"blade\", \"dynamite\", \"hatchet\", \"rifle\", \"tank\", \"bomb\", \"firearm\", \"knife\", \"shotgun\", \"teargas\", \"cannon\", \"grenade\",\n",
        "    \"mace\", \"slingshot\", \"whip\"] \n",
        "# Pleasant\n",
        "A = [\"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\",\n",
        "    \"happy\", \"laughter\", \"paradise\", \"vacation\"] \n",
        "# Unpleasant\n",
        "B = [\"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\",\n",
        "    \"vomit\", \"agony\", \"prison\"] \n",
        "\n",
        "\"\"\"Download the 'Glove' embeddings if not downloaded\"\"\"\n",
        "!if [ ! -f /content/gensim_glove.840B.300d.txt.bin ]; then gdown https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2; fi\n",
        "\n",
        "\"\"\"Load the embeddings to a gensim object\"\"\"\n",
        "resourceFile = ''\n",
        "glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
        "print('The glove embedding has been loaded!')\n",
        "\n",
        "\"\"\"Compute the effect-size and P value\"\"\"\n",
        "print('WEAT d = ', weat_effect_size(X, Y, A, B, glove))\n",
        "print('WEAT p = ', weat_p_value(X, Y, A, B, glove, 1000))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The glove embedding has been loaded!\n",
            "WEAT d =  1.5495627\n",
            "WEAT p =  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rfwYNFl2ckGy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load all vectors and compute the conceptor"
      ]
    },
    {
      "metadata": {
        "id": "xEuq-Lt5ctV2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Clone our repository. The repository contains code for computing the conceptors. It also includes word lists needed representing different subspaces."
      ]
    },
    {
      "metadata": {
        "id": "vqaF4DyZxjLl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# our code for debiasing -- also includes word lists\n",
        "!rm -r ConceptorDebias\n",
        "!git clone https://github.com/jsedoc/ConceptorDebias\n",
        "!cd ConceptorDebias; git checkout ACL-cleanup\n",
        "\n",
        "sys.path.append('/content/ConceptorDebias')\n",
        "\n",
        "from Conceptors.conceptor_fxns import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_BoXwMINw0sA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Compute the conceptor matrix"
      ]
    },
    {
      "metadata": {
        "id": "HAG4oqwIc1Z3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Compute the conceptor matrix\n",
        "def process_cn_matrix(subspace, alpha = 2):\n",
        "  \n",
        "  C = train_conceptor(subspace, alpha)\n",
        "  \n",
        "  #Calculate the negation of the conceptor matrix\n",
        "  negC = NOT(C)\n",
        "  \n",
        "  return negC\n",
        "\n",
        "def apply_conceptor(x, C):\n",
        "  #Post-process the vocab matrix\n",
        "  newX = (C @ x).T\n",
        "  print(newX.shape)\n",
        "  return newX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GHaXxx5L3Ugb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load embeddings of all words from the ref. wordlist from a specific embedding"
      ]
    },
    {
      "metadata": {
        "id": "8WBI1VED3bpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Arguments - embd: The word embeddings in the form of a dict || wikiWordsPath: List of words to be considered\n",
        "def load_all_vectors(embd, wikiWordsPath):\n",
        "  all_words_index = {}\n",
        "  all_words_mat = []\n",
        "  with open(wikiWordsPath, \"r+\") as f_in:\n",
        "    ind = 0\n",
        "    for line in f_in:\n",
        "      word = line.split(' ')[0]\n",
        "      if word in embd:\n",
        "        all_words_index[word] = ind\n",
        "        all_words_mat.append(embd[word])\n",
        "        ind = ind+1\n",
        "        \n",
        "  return all_words_index, all_words_mat\n",
        "\n",
        "def load_subspace_vectors(embd, subspace_words):\n",
        "  subspace_embd_mat = []\n",
        "  ind = 0\n",
        "  for word in subspace_words:\n",
        "    if word in embd:\n",
        "      subspace_embd_mat.append(embd[word])\n",
        "      ind = ind+1\n",
        "      \n",
        "  return subspace_embd_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EzfTYgq0I9WW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load all word lists for the ref. subspace"
      ]
    },
    {
      "metadata": {
        "id": "D4kndsi_DwNr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# General word list\n",
        "!wget https://raw.githubusercontent.com/IlyaSemenov/wikipedia-word-frequency/master/results/enwiki-20150602-words-frequency.txt\n",
        "!git clone https://github.com/PrincetonML/SIF\n",
        "    \n",
        "# Gender word lists\n",
        "!git clone https://github.com/uclanlp/gn_glove\n",
        "!git clone https://github.com/uclanlp/corefBias\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/female.txt\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/male.txt\n",
        "\n",
        "from lists.load_word_lists import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "orBvfpqCFOfa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gender_list_pronouns = WEATLists.W_7_Male_terms + WEATLists.W_7_Female_terms + WEATLists.W_8_Male_terms + WEATLists.W_8_Female_terms\n",
        "gender_list_pronouns = list(set(gender_list_pronouns))\n",
        "\n",
        "gender_list_extended = male_vino_extra + female_vino_extra + male_gnGlove + female_gnGlove\n",
        "gender_list_extended = list(set(gender_list_extended))\n",
        "\n",
        "gender_list_propernouns = male_cmu + female_cmu\n",
        "gender_list_propernouns = list(set(gender_list_propernouns))\n",
        "\n",
        "gender_list_all = gender_list_pronouns + gender_list_extended + gender_list_propernouns\n",
        "gender_list_all = list(set(gender_list_all))\n",
        "\n",
        "race_list = WEATLists.W_3_Unused_full_list_European_American_names + WEATLists.W_3_European_American_names + WEATLists.W_3_Unused_full_list_African_American_names + WEATLists.W_3_African_American_names + WEATLists.W_4_Unused_full_list_European_American_names + WEATLists.W_4_European_American_names + WEATLists.W_4_Unused_full_list_African_American_names + WEATLists.W_4_African_American_names + WEATLists.W_5_Unused_full_list_European_American_names + WEATLists.W_5_European_American_names + WEATLists.W_5_Unused_full_list_African_American_names + WEATLists.W_5_African_American_names \n",
        "race_list = list(set(race_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kEiBEVI2I2WR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load different embeddings"
      ]
    },
    {
      "metadata": {
        "id": "Nrlqu2xvJH_6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Glove"
      ]
    },
    {
      "metadata": {
        "id": "PFNSjmY8I1Ut",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Load word embeddings\n",
        "#download gensim formatted Full Glove embeddings\n",
        "!if [ ! -f /content/gensim_glove.840B.300d.txt.bin ]; then gdown https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2; fi\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = ''\n",
        "\n",
        "if 'glove' not in dir():\n",
        "  glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
        "  print('The glove embedding has been loaded!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aD-bmiCdGKYt",
        "colab_type": "code",
        "outputId": "ee106ebb-6daf-46fe-d09f-890c17d0f8d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1156
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Sample output of the glove embeddings\"\"\"\n",
        "X = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "print(X)\n",
        "a = [glove[w] for w in X if w.lower() in glove]\n",
        "print(np.array(a).shape)\n",
        "glove['Brad']\n",
        "#glove['brad']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Brad', 'Brendan', 'Geoffrey', 'Greg', 'Brett', 'Jay', 'Matthew', 'Neil', 'Todd', 'Allison', 'Anne', 'Carrie', 'Emily', 'Jill', 'Laurie', 'Kristen', 'Meredith', 'Sarah']\n",
            "(18, 300)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.5609e-01, -1.5396e-01, -5.0346e-01,  2.5138e-01,  3.3952e-01,\n",
              "        2.6105e-01,  3.0257e-01, -1.1273e-01,  4.6391e-02,  2.1197e+00,\n",
              "        1.9856e-01, -3.1840e-02, -4.4469e-01, -8.6130e-03, -3.6665e-01,\n",
              "       -7.8715e-02, -1.8301e-01, -3.5935e-01, -2.9033e-01,  3.2755e-01,\n",
              "       -1.1633e-01, -5.3637e-02, -3.1589e-01, -9.0069e-02,  5.4889e-02,\n",
              "        1.0793e-01, -7.6549e-02,  3.2448e-01, -3.7830e-01, -7.1353e-03,\n",
              "       -9.2837e-02, -3.4702e-01, -4.4372e-02, -3.6582e-02,  6.6447e-02,\n",
              "        4.3170e-01,  1.4958e-01,  4.1407e-01, -7.3515e-04, -5.4028e-01,\n",
              "        3.4552e-01, -5.6781e-02, -1.9483e-01,  3.7953e-01,  4.6593e-02,\n",
              "        4.1634e-02, -5.3288e-01,  1.4703e-01,  2.6140e-02, -1.6343e-03,\n",
              "        3.7304e-01, -1.7348e-01,  2.5698e-02, -2.5498e-01, -1.9006e-01,\n",
              "       -5.8404e-01, -6.0172e-03,  4.6960e-02,  1.4949e-02,  4.7884e-01,\n",
              "        1.8709e-01,  1.1012e-01, -2.9987e-01,  3.7356e-02, -4.1855e-02,\n",
              "        1.2758e-02,  1.5549e-01, -7.8430e-02,  3.5795e-01, -2.1139e-02,\n",
              "        2.1553e-01, -1.7305e-01,  2.9180e-01,  3.4817e-01, -8.2386e-01,\n",
              "        5.0522e-02, -4.4454e-02, -1.3776e-01,  1.6891e-01, -2.1201e-01,\n",
              "        3.8606e-01,  2.7904e-02, -2.6739e-02, -1.1496e-01,  2.7703e-01,\n",
              "       -1.9411e-01,  1.8388e-01, -3.9542e-01,  1.9627e-02,  3.7480e-01,\n",
              "        2.8426e-01, -2.4735e-01,  2.1075e-01,  5.5669e-01,  1.5538e-01,\n",
              "       -3.4910e-01, -4.2342e-01, -2.9979e-01, -9.9152e-02,  4.6956e-02,\n",
              "        2.3423e-01,  4.2039e-01,  5.7573e-02,  1.3133e-01,  3.0233e-01,\n",
              "        1.8890e+00, -6.2675e-02,  3.3504e-01,  1.6672e-02,  2.1146e-01,\n",
              "        3.0714e-01, -5.6883e-02,  1.7517e-01,  4.0338e-01,  1.4808e-01,\n",
              "        6.4289e-02,  1.9331e-01, -3.7482e-01, -1.7096e-02,  6.1206e-01,\n",
              "       -6.6393e-02,  3.5268e-01,  3.2031e-01,  7.6724e-02,  4.0608e-01,\n",
              "        2.4221e-01, -5.1967e-01,  5.2104e-01,  3.0191e-01,  4.5360e-01,\n",
              "        3.1720e-01,  7.0415e-01, -8.9437e-03,  4.1537e-01, -3.0064e-01,\n",
              "       -1.2406e-01,  3.2650e-01, -2.3340e-01,  5.8629e-02, -1.4551e-01,\n",
              "       -1.1687e+00, -2.0016e-01, -1.7595e-01,  5.1038e-01,  1.3941e-01,\n",
              "       -1.3492e-01, -1.6592e-01, -6.4183e-01, -7.4643e-02,  1.2845e-01,\n",
              "        1.3640e-01, -5.3371e-01,  2.3772e-01,  3.1996e-02,  6.7226e-02,\n",
              "        9.8275e-02,  3.5349e-01,  3.0168e-02, -3.3902e-01,  2.3055e-01,\n",
              "        2.0858e-01,  1.3370e-01, -4.0881e-01, -5.6269e-01, -2.6002e-01,\n",
              "        4.5334e-01,  1.1423e-01, -2.2129e-01,  3.9015e-02, -1.7528e-01,\n",
              "        1.3944e-01, -4.0646e-01,  1.5931e-01,  5.8924e-02,  1.3164e-01,\n",
              "       -7.0105e-02, -1.7987e-01, -2.0772e-01,  4.0103e-01,  1.5782e-01,\n",
              "        3.2983e-02, -3.7376e-01, -3.6251e-01,  2.0702e-01, -2.3175e-01,\n",
              "        2.8544e-01,  2.1287e-01,  1.2014e-01, -9.5387e-02, -1.1966e-01,\n",
              "       -2.1259e-01,  2.1491e-01, -2.2424e-01,  2.0714e-01, -3.5469e-01,\n",
              "       -1.8725e-01, -1.3854e-02,  1.3169e-01,  2.9420e-02, -4.6341e-01,\n",
              "       -3.8674e-01, -4.1141e-01, -9.1650e-02, -4.1818e-01, -6.5491e-01,\n",
              "        5.0581e-02, -1.2775e-02,  1.0144e-01, -3.5621e-01,  4.9632e-01,\n",
              "       -1.0433e-01,  9.9404e-02,  4.8235e-01,  2.3474e-01, -4.7720e-01,\n",
              "        7.9219e-01, -1.1258e-02,  3.5949e-03,  2.7460e-02, -1.7296e-01,\n",
              "       -2.2248e-01,  2.2229e-01,  1.3333e-01,  4.0048e-01,  4.2337e-01,\n",
              "       -5.1213e-01, -8.2212e-02,  8.7370e-02,  1.5837e-02, -1.3151e-02,\n",
              "       -5.1821e-03,  1.0908e-01,  2.6508e-01, -4.1608e-01, -1.6802e-01,\n",
              "       -1.3486e-01,  1.7403e-01,  2.8325e-01,  3.2487e-01,  1.3839e-01,\n",
              "        4.6552e-01, -3.6659e-01,  2.4266e-01,  1.3700e-01,  4.0531e-01,\n",
              "        4.8374e-01, -2.8112e-02, -3.2147e-01,  1.3126e-01,  2.2298e-01,\n",
              "        3.6765e-02, -1.8370e-01, -1.4024e-01, -7.2459e-02, -1.9426e-01,\n",
              "       -4.2977e-01, -1.6731e-01,  1.7215e-01, -1.0831e-01,  2.5391e-01,\n",
              "        4.9705e-01, -8.7651e-02, -7.4383e-02, -8.5615e-02, -3.8821e-02,\n",
              "        3.2831e-01, -1.7713e-01, -1.5463e-01,  5.2676e-01,  1.0048e-01,\n",
              "       -4.9239e-02, -2.3351e-02,  1.5625e-01, -3.1273e-01, -4.2893e-01,\n",
              "        7.2558e-02,  1.1981e-01, -5.4989e-02,  1.1979e-01,  8.6490e-02,\n",
              "       -3.9215e-01, -2.1741e-01, -3.4306e-01,  3.9574e-01, -3.3915e-01,\n",
              "       -4.3442e-01, -4.4865e-01, -1.4487e-01,  4.9300e-02,  1.6221e-01,\n",
              "        4.6767e-01,  3.1584e-01,  2.5845e-01,  3.2365e-01,  1.0205e-01,\n",
              "        4.6215e-01, -1.1424e-01, -1.3349e-01, -3.5560e-01,  7.2325e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "84af1zflJKLQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Word2vec"
      ]
    },
    {
      "metadata": {
        "id": "OmJlU2IiJMMj",
        "colab_type": "code",
        "outputId": "1b302430-806a-436e-addc-1eae7d51ef1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "#load gensim formatted Full Word2vec embeddings\n",
        "!if test -e /content/GoogleNews-vectors-negative300.bin.gz || test -e /content/GoogleNews-vectors-negative300.bin; then echo 'file already downloaded'; else echo 'starting download'; gdown https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM; fi\n",
        "!if [ ! -f /content/GoogleNews-vectors-negative300.bin ]; then gunzip GoogleNews-vectors-negative300.bin.gz; fi\n",
        "  \n",
        "import gensim\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = ''\n",
        "\n",
        "if 'word2vec' not in dir():\n",
        "  word2vec = KeyedVectors.load_word2vec_format(resourceFile + 'GoogleNews-vectors-negative300.bin', binary=True)\n",
        "  print('The word2vec embedding has been loaded!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file already downloaded\n",
            "The word2vec embedding has been loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FCiZfu13JPHl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fasttext"
      ]
    },
    {
      "metadata": {
        "id": "A9zwQ2T_JRBo",
        "colab_type": "code",
        "outputId": "454342eb-e607-451e-e70d-97f1270a2ef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!if [ ! -f /content/fasttext.bin ]; then gdown https://drive.google.com/uc?id=1Zl6a75Ybf8do9uupmrJWKQMnvqqme4fh; fi\n",
        "\n",
        "import gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors  \n",
        "\n",
        "resourceFile = ''\n",
        "if 'fasttext' not in dir():\n",
        "  fasttext = KeyedVectors.load_word2vec_format(resourceFile + 'fasttext.bin', binary=True)\n",
        "  print('The fasttext embedding has been loaded!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The fasttext embedding has been loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7hL2zgX6JRhG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Elmo"
      ]
    },
    {
      "metadata": {
        "id": "cXvK1wuvJTVP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!if [ ! -f /content/elmo_embeddings_emma_brown.pkl ]; then gdown https://drive.google.com/uc?id=17TK2h3cz7amgm2mCY4QCYy1yh23ZFWDU; fi\n",
        "\n",
        "data = pickle.load(open(\"elmo_embeddings_emma_brown.pkl\", \"rb\"))\n",
        "\n",
        "def pick_embeddings(corpus, sent_embs):\n",
        "    X = []\n",
        "    labels = {}\n",
        "    sents = []\n",
        "    ind = 0\n",
        "    for i, s in enumerate(corpus):\n",
        "        for j, w in enumerate(s):\n",
        "            X.append(sent_embs[i][j])\n",
        "            if w.lower() in labels:\n",
        "              labels[w.lower()].append(ind)\n",
        "            else:\n",
        "              labels[w.lower()] = [ind]\n",
        "            sents.append(s)\n",
        "            ind = ind + 1\n",
        "    return (X, labels, sents)\n",
        "  \n",
        "def get_word_list(path):\n",
        "    word_list = []\n",
        "    with open(path, \"r+\") as f_in:\n",
        "      for line in f_in:\n",
        "        word = line.split(' ')[0]\n",
        "        word_list.append(word.lower())\n",
        "\n",
        "    return word_list\n",
        "\n",
        "def load_subspace_vectors_contextual(all_mat, all_index, subspace_list):\n",
        "    subspace_mat = []\n",
        "    for w in subspace_list:\n",
        "      if w.lower() in all_index:\n",
        "        for i in all_index[w.lower()]:\n",
        "          #print(type(i))\n",
        "          subspace_mat.append(all_mat[i])\n",
        "    #subspace_mat = [all_mat[i,:] for i in all_index[w.lower()] for w in subspace_list if w.lower() in all_index]\n",
        "    print(\"Subspace: \", np.array(subspace_mat).shape)\n",
        "    return subspace_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SZy2oUe0nlxz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "#nltk.download('brown')\n",
        "\n",
        "brown_corpus = brown.sents()\n",
        "elmo = data['brown_embs']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gTsLdzKKrnn6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(np.array(elmo_brown_mat).shape)\n",
        "print(len(list(elmo_brown_index.keys())))\n",
        "print(len(wiki_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_DRlLRadJT1A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bert"
      ]
    },
    {
      "metadata": {
        "id": "KIvNmLw-vPWN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# WEAT on Conceptored embeddings"
      ]
    },
    {
      "metadata": {
        "id": "IPP2UaVDy2r_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Initialize variables"
      ]
    },
    {
      "metadata": {
        "id": "1ouWsWnl0csn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "resourceFile = ''\n",
        "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "\"\"\"Set the embedding to be used\"\"\"\n",
        "embd = 'glove'\n",
        "\n",
        "\"\"\"Set the subspace to be tested on\"\"\"\n",
        "subspace = 'gender_list_all' \n",
        "\n",
        "\"\"\"Load association and target word pairs\"\"\"\n",
        "X = WEATLists.W_8_Science\n",
        "Y = WEATLists.W_8_Arts\n",
        "A = WEATLists.W_8_Male_terms\n",
        "B = WEATLists.W_8_Female_terms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPCAcond1_b2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load the vectors as a matrix"
      ]
    },
    {
      "metadata": {
        "id": "X3-Nqa2J2FoF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "curr_embd = eval(embd)\n",
        "  \n",
        "\"\"\"Load all embeddings in a matrix of all words in the wordlist\"\"\"\n",
        "if embd == 'elmo':\n",
        "  all_words_mat, all_words_index, _ = pick_embeddings(brown_corpus, curr_embd)\n",
        "else:\n",
        "  all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T8d4ERsozAVK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Compute the conceptor"
      ]
    },
    {
      "metadata": {
        "id": "dfO_XltO3hp5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if subspace != 'without_conceptor':\n",
        "  subspace_words_list = eval(subspace)\n",
        "  if subspace == 'gender_list_and':\n",
        "    if embd == 'elmo':\n",
        "      subspace_words_mat1 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_pronouns)\n",
        "      cn1 = process_cn_matrix(np.array(subspace_words_mat1).T, alpha = 8)\n",
        "\n",
        "      subspace_words_mat2 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_extended)\n",
        "      cn2 = process_cn_matrix(np.array(subspace_words_mat2).T, alpha = 3)\n",
        "\n",
        "      subspace_words_mat3 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_propernouns)\n",
        "      cn3 = process_cn_matrix(np.array(subspace_words_mat3).T, alpha = 10)\n",
        "\n",
        "      cn = AND(cn1, AND(cn2, cn3))\n",
        "    else:\n",
        "      subspace_words_mat1 = load_subspace_vectors(curr_embd, gender_list_pronouns)\n",
        "      cn1 = process_cn_matrix(np.array(subspace_words_mat1).T)\n",
        "\n",
        "      subspace_words_mat2 = load_subspace_vectors(curr_embd, gender_list_extended)\n",
        "      cn2 = process_cn_matrix(np.array(subspace_words_mat2).T)\n",
        "\n",
        "      subspace_words_mat3 = load_subspace_vectors(curr_embd, gender_list_propernouns)\n",
        "      cn3 = process_cn_matrix(np.array(subspace_words_mat3).T)\n",
        "\n",
        "      cn = AND(cn1, AND(cn2, cn3))\n",
        "  else: \n",
        "    if embd == 'elmo':\n",
        "      subspace_words_mat = load_subspace_vectors_contextual(all_words_mat, all_words_index, subspace_words_list)\n",
        "      cn = process_cn_matrix(np.array(subspace_words_mat).T, alpha = 6)\n",
        "    else:\n",
        "      subspace_words_mat = load_subspace_vectors(curr_embd, subspace_words_list)\n",
        "      cn = process_cn_matrix(np.array(subspace_words_mat).T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xj-aXqB2zOpI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Compute conceptored embeddings"
      ]
    },
    {
      "metadata": {
        "id": "XoQGSnAH4aZ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "\n",
        "#Store all conceptored words in a dictonary\n",
        "all_words = {}\n",
        "for word, index in all_words_index.items():\n",
        "  #print(word, index)\n",
        "  if embd == 'elmo':\n",
        "    all_words[word] = np.mean([all_words_cn[i,:] for i in index], axis = 0)\n",
        "  else:\n",
        "    all_words[word] = all_words_cn[index,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cVmZBwykJmgn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Calculate WEAT scores"
      ]
    },
    {
      "metadata": {
        "id": "PPukMLU2ZosX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "d = weat_effect_size(X, Y, A, B, all_words)\n",
        "p = weat_p_value(X, Y, A, B, all_words, 1000)\n",
        "\n",
        "print('WEAT d = ', d)\n",
        "print('WEAT p = ', p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2I9dGN5fGQw-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# WEAT on BERT embeddings"
      ]
    },
    {
      "metadata": {
        "id": "hDL0wCtSwoOx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#Load all bert embeddings in a dictonary\n",
        "all = {}\n",
        "for filename in os.listdir('/home/saketk/bert'):\n",
        "  all[filename] = pickle.load(open(filename, \"rb\"))\n",
        "\n",
        "#Load all embeddings in a matrix and a dictonary of words to row numbers\n",
        "res = all['big_bert_gender_list_extended.pkl']['type_embedings']\n",
        "w = []\n",
        "for name in all:\n",
        "  res = np.concatenate((res, all[name]['type_embedings']))\n",
        "  w += all[name]['words']\n",
        "print(res.shape)\n",
        "print(len(set(w)))\n",
        "w = [aaa.lower() for aaa in w]\n",
        "\n",
        "#Load all BERT conceptors\n",
        "cn_pronouns = all['big_bert_gender_list_pronouns.pkl']['GnegC']\n",
        "cn_propernouns = all['big_bert_gender_list_propernouns.pkl']['GnegC']\n",
        "cn_extended = all['big_bert_gender_list_extended.pkl']['GnegC']\n",
        "cn_all = all['big_bert_gender_list_all.pkl']['GnegC']\n",
        "cn_race = all['big_bert_race_list.pkl']['GnegC']\n",
        "\n",
        "print(np.array(cn_all).shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JZ5hwHPey84G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Test Debiasing on BERT Embeddings using WEAT"
      ]
    },
    {
      "metadata": {
        "id": "cWeJf7sDkSCc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from ConceptorDebias.conceptor_fxns import AND\n",
        "import pandas as pd\n",
        "\n",
        "resourceFile = ''\n",
        "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "all_embd = ['bert']\n",
        "all_subspace = ['without_conceptor','gender_list_pronouns', 'gender_list_extended','gender_list_propernouns', 'gender_list_all', 'gender_list_and']\n",
        "# all_subspace = ['without_conceptor', 'race_list']\n",
        "\n",
        "all_words_mat = res\n",
        "all_words_index = {}\n",
        "print(len(set(w)))\n",
        "for i,a in enumerate(w):\n",
        "  all_words_index[a] = i\n",
        "# all_words_index = {wr:i for i,wr in enumerate(w)}\n",
        "print(\"All_words_mat: \", all_words_mat.shape)\n",
        "print(\"Index: \", len(all_words_index.keys()))\n",
        "career = WEATLists.W_8_Science\n",
        "family = WEATLists.W_8_Arts\n",
        "male = WEATLists.W_8_Male_terms\n",
        "female = WEATLists.W_8_Female_terms\n",
        "\n",
        "# white = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "# black = WEATLists.W_5_Unused_full_list_African_American_names\n",
        "# pleasant = WEATLists.W_5_Pleasant\n",
        "# unpleasant = WEATLists.W_5_Unpleasant\n",
        "embd = 'bert'\n",
        "results = []\n",
        "for subspace in all_subspace:\n",
        "    \n",
        "  if subspace != 'without_conceptor' and subspace != 'gender_list_and':\n",
        "    subspace_words_list = eval(subspace)\n",
        "\n",
        "  if subspace != 'without_conceptor':\n",
        "    #CN all word embeddings using the respective subspace\n",
        "    if subspace == 'gender_list_and':\n",
        "      cn = AND(cn_pronouns, AND(cn_extended, cn_propernouns))\n",
        "      all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "      print(\"All mat CN: \", np.array(all_words_cn).shape)\n",
        "    else:\n",
        "      #Load all embeddings of the subspace as a matrix\n",
        "      if subspace == 'gender_list_pronouns':\n",
        "        cn = cn_pronouns\n",
        "      elif subspace == 'gender_list_propernouns':\n",
        "        cn = cn_propernouns\n",
        "      elif subspace == 'gender_list_extended':\n",
        "        cn = cn_extended\n",
        "      elif subspace == 'gender_list_all':\n",
        "        cn = cn_all\n",
        "        \n",
        "      print(\"CN shape: \", np.array(cn).shape)\n",
        "      all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "      print(\"Subspace mat: \", np.array(subspace_words_mat).shape)\n",
        "      \n",
        "  else:\n",
        "    all_words_cn = all_words_mat\n",
        "  all_words_cn = np.array(all_words_cn)\n",
        "  print(\"All CN: \", all_words_cn.shape)\n",
        "  #Store all conceptored words in a dictonary\n",
        "  all_words = {}\n",
        "  for word, index in all_words_index.items():\n",
        "    all_words[word] = all_words_cn[index,:]\n",
        "  print(\"D: \", len(all_words.keys()))\n",
        "  if subspace == 'without_conceptor':\n",
        "    #WITHOUT CONCEPTOR\n",
        "    d_cn = weat_effect_size(career, family, male, female, all_words)\n",
        "    p_cn = weat_p_value(career, family, male, female, all_words, 1000)\n",
        "    print(\"Without conceptor\")\n",
        "    print('WEAT d = ', d_cn)\n",
        "    print('WEAT p = ', p_cn)\n",
        "  else:\n",
        "    #WITH CONCEPTOR\n",
        "    d_cn = weat_effect_size(career, family, male, female, all_words)\n",
        "    p_cn = weat_p_value(career, family, male, female, all_words, 1000)\n",
        "    print(\"With conceptor: \", embd, subspace)\n",
        "    print('WEAT d = ', d_cn)\n",
        "    print('WEAT p = ', p_cn)\n",
        "\n",
        "  row = [embd, subspace, d_cn, p_cn]\n",
        "  results.append(row)\n",
        "\n",
        "  \n",
        "  \n",
        "pd.DataFrame(np.array(results), columns = ['Embedding', 'Subspace', 'Effect Size', 'p-value'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gbg0wsR_Ykym",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hard Debiasing"
      ]
    },
    {
      "metadata": {
        "id": "bEzUV_AOtRAd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mu et. al. Hard Debiasing"
      ]
    },
    {
      "metadata": {
        "id": "a0ZvHFuFtV_v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#Project off first principal component\n",
        "def hard_debias(all_words, subspace):\n",
        "  all_words = np.array(all_words)\n",
        "  subspace = np.array(subspace)\n",
        "  print(all_words.shape, \" \", subspace.shape)\n",
        "  pca = PCA(n_components = 1)\n",
        "  pca.fit(subspace)\n",
        "  pc1 = np.array(pca.components_)\n",
        "  \n",
        "  temp = (pc1.T @ (pc1 @ all_words.T)).T\n",
        "  ret = all_words - temp\n",
        "  \n",
        "  return ret\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9t0IHhGhznxG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Test Hard Debiasing using WEAT"
      ]
    },
    {
      "metadata": {
        "id": "ESv2ZU7r1PV4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "resourceFile = ''\n",
        "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "all_embd = ['glove', 'word2vec', 'fasttext','elmo']\n",
        "all_subspace = ['without_debiasing', 'gender_list_extended','gender_list_propernouns','gender_list_pronouns', 'gender_list_all']\n",
        "# all_subspace = ['without_debiasing', 'race_list']\n",
        "\n",
        "# science = WEATLists.W_8_Science\n",
        "# arts = WEATLists.W_8_Arts\n",
        "# male = WEATLists.W_8_Male_terms\n",
        "# female = WEATLists.W_8_Female_terms\n",
        "\n",
        "white = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "black = WEATLists.W_5_Unused_full_list_African_American_names\n",
        "pleasant = WEATLists.W_5_Pleasant\n",
        "unpleasant = WEATLists.W_5_Unpleasant\n",
        "\n",
        "#print(career, family, male, female)\n",
        "results = []\n",
        "\n",
        "for embd in all_embd:\n",
        "  #Initialize the embeddings to be used\n",
        "  curr_embd = eval(embd)\n",
        "  \n",
        "  #Load all embeddings in a matrix of all words in the wordlist\n",
        "  if embd == 'elmo' or embd == 'bert':\n",
        "    all_words_mat, all_words_index, _ = pick_embeddings(brown_corpus, curr_embd)\n",
        "    print(\"All mat: \", np.array(all_words_mat).shape)\n",
        "    print(\"Number of words: \", len(list(all_words_index.keys())))\n",
        "  else:\n",
        "    all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)\n",
        "  \n",
        "  for subspace in all_subspace:\n",
        "    \n",
        "    if subspace != 'without_debiasing' and subspace != 'gender_list_and':\n",
        "      subspace_words_list = eval(subspace)\n",
        "\n",
        "    \n",
        "    if subspace != 'without_debiasing':\n",
        "      #Load all embeddings of the subspace as a matrix\n",
        "      if embd == 'elmo' or embd == 'bert':\n",
        "        subspace_words_mat = load_subspace_vectors_contextual(all_words_mat, all_words_index, subspace_words_list)\n",
        "        all_words_cn = hard_debias(all_words_mat, subspace_words_mat)\n",
        "        print(\"Subspace mat: \", np.array(subspace_words_mat).shape)\n",
        "      else:\n",
        "        subspace_words_mat = load_subspace_vectors(curr_embd, subspace_words_list)\n",
        "        all_words_cn = hard_debias(all_words_mat, subspace_words_mat)\n",
        "    else:\n",
        "      all_words_cn = all_words_mat\n",
        "    all_words_cn = np.array(all_words_cn)\n",
        "    print(\"All CN: \", all_words_cn.shape)\n",
        "    #Store all conceptored words in a dictonary\n",
        "    all_words = {}\n",
        "    for word, index in all_words_index.items():\n",
        "      #print(word, index)\n",
        "      if embd == 'elmo' or embd == 'bert':\n",
        "        all_words[word] = np.mean([all_words_cn[i,:] for i in index], axis = 0)\n",
        "      else:\n",
        "        all_words[word] = all_words_cn[index,:]\n",
        "    \n",
        "    if subspace == 'without_debiasing':\n",
        "      #WITHOUT CONCEPTOR\n",
        "      d_cn = weat_effect_size(white, black, pleasant, unpleasant, all_words)\n",
        "      p_cn = weat_p_value(white, black, pleasant, unpleasant, all_words, 1000)\n",
        "      print(\"Without debiasing\")\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    else:\n",
        "      #WITH CONCEPTOR\n",
        "      d_cn = weat_effect_size(white, black, pleasant, unpleasant, all_words)\n",
        "      p_cn = weat_p_value(white, black, pleasant, unpleasant, all_words, 1000)\n",
        "      print(\"With debiasing: \", embd, subspace)\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    \n",
        "    row = [embd, subspace, d_cn, p_cn]\n",
        "    results.append(row)\n",
        "    \n",
        "pd.DataFrame(np.array(results), columns = ['Embedding', 'Subspace', 'Effect Size', 'p-value'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iP9IM02Ln-qp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bolukbasi hard debiasing"
      ]
    },
    {
      "metadata": {
        "id": "N-5CfftKoGCt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def doPCA(pairs, mat, index, num_components = 5):\n",
        "    matrix = []\n",
        "    for a, b in pairs:\n",
        "        center = (mat[index[a.lower()]] + mat[index[b.lower()]])/2\n",
        "        matrix.append(mat[index[a.lower()]] - center)\n",
        "        matrix.append(mat[index[b.lower()]] - center)\n",
        "    matrix = np.array(matrix)\n",
        "    pca = PCA(n_components = num_components)\n",
        "    pca.fit(matrix)\n",
        "    # bar(range(num_components), pca.explained_variance_ratio_)\n",
        "    return pca\n",
        "\n",
        "def drop(u, v):\n",
        "    return u - v * u.dot(v) / v.dot(v)\n",
        "  \n",
        "def normalize(all_words_mat):\n",
        "    all_words_mat /= np.linalg.norm(all_words_mat, axis=1)[:, np.newaxis]\n",
        "    return all_words_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GWOwX9wHoB-7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def debias(all_words_mat, all_words_index, gender_specific_words, definitional, equalize):\n",
        "    gender_direction = doPCA(definitional, all_words_mat, all_words_index).components_[0]\n",
        "    specific_set = set(gender_specific_words)\n",
        "    for w in list(all_words_index.keys()):\n",
        "        if w not in specific_set:\n",
        "            all_words_mat[all_words_index[w.lower()]] = drop(all_words_mat[all_words_index[w.lower()]], gender_direction)\n",
        "    all_words_mat = normalize(all_words_mat)\n",
        "    candidates = {x for e1, e2 in equalize for x in [(e1.lower(), e2.lower()),\n",
        "                                                     (e1.title(), e2.title()),\n",
        "                                                     (e1.upper(), e2.upper())]}\n",
        "    print(candidates)\n",
        "    for (a, b) in candidates:\n",
        "        if (a.lower() in all_words_index and b.lower() in all_words_index):\n",
        "            y = drop((all_words_mat[all_words_index[a.lower()]] + all_words_mat[all_words_index[b.lower()]]) / 2, gender_direction)\n",
        "            z = np.sqrt(1 - np.linalg.norm(y)**2)\n",
        "            if (all_words_mat[all_words_index[a.lower()]] - all_words_mat[all_words_index[b.lower()]]).dot(gender_direction) < 0:\n",
        "                z = -z\n",
        "            all_words_mat[all_words_index[a.lower()]] = z * gender_direction + y\n",
        "            all_words_mat[all_words_index[b.lower()]] = -z * gender_direction + y\n",
        "    all_words_mat = normalize(all_words_mat)\n",
        "    return all_words_mat\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t9h4VxZfqluU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "resourceFile = ''\n",
        "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "%cd debiaswe_tutorial/debiaswe/\n",
        "# Lets load some gender related word lists to help us with debiasing\n",
        "with open('./data/definitional_pairs.json', \"r\") as f:\n",
        "    defs = json.load(f) #gender definitional words\n",
        "print(\"definitional\", defs)\n",
        "defs_list = []\n",
        "for pair in defs:\n",
        "  defs_list.append(pair[0])\n",
        "  defs_list.append(pair[1])\n",
        "\n",
        "with open('./data/equalize_pairs.json', \"r\") as f:\n",
        "    equalize_pairs = json.load(f) \n",
        "print(\"Equalize pairs\", equalize_pairs)\n",
        "\n",
        "%cd ../../\n",
        "!ls\n",
        "\n",
        "all_embd = ['glove', 'word2vec', 'fasttext']\n",
        "all_subspace = ['without_debiasing', 'gender_list_extended','gender_list_propernouns','gender_list_pronouns', 'gender_list_all']\n",
        "#all_subspace = ['without_debiasing', 'race_list']\n",
        "\n",
        "math = WEATLists.W_7_Math\n",
        "arts = WEATLists.W_7_Arts\n",
        "male = WEATLists.W_7_Male_terms\n",
        "female = WEATLists.W_7_Female_terms\n",
        "\n",
        "# white = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "# black = WEATLists.W_5_Unused_full_list_African_American_names\n",
        "# pleasant = WEATLists.W_5_Pleasant\n",
        "# unpleasant = WEATLists.W_5_Unpleasant\n",
        "\n",
        "#print(career, family, male, female)\n",
        "results = []\n",
        "\n",
        "for embd in all_embd:\n",
        "  #Initialize the embeddings to be used\n",
        "  curr_embd = eval(embd)\n",
        "  \n",
        "  #Load all embeddings in a matrix of all words in the wordlist\n",
        "  if embd == 'elmo' or embd == 'bert':\n",
        "    all_words_mat, all_words_index, _ = pick_embeddings(brown_corpus, curr_embd)\n",
        "    print(\"All mat: \", np.array(all_words_mat).shape)\n",
        "    print(\"Number of words: \", len(list(all_words_index.keys())))\n",
        "  else:\n",
        "    all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)\n",
        "  \n",
        "  for subspace in all_subspace:\n",
        "    \n",
        "    if subspace != 'without_debiasing' and subspace != 'gender_list_and':\n",
        "      subspace_words_list = eval(subspace)\n",
        "\n",
        "    \n",
        "    if subspace != 'without_debiasing':\n",
        "      #Load all embeddings of the subspace as a matrix\n",
        "      if embd == 'elmo' or embd == 'bert':\n",
        "        #subspace_words_mat = load_subspace_vectors_contextual(all_words_mat, all_words_index, subspace_words_list)\n",
        "        all_words_cn = debias(all_words_mat, all_words_index, subspace_words_list, defs, equalize_pairs)\n",
        "        print(\"Subspace mat: \", np.array(subspace_words_mat).shape)\n",
        "      else:\n",
        "        #subspace_words_mat = load_subspace_vectors(curr_embd, subspace_words_list)\n",
        "        all_words_cn = debias(all_words_mat, all_words_index, subspace_words_list, defs, equalize_pairs)\n",
        "    else:\n",
        "      all_words_cn = all_words_mat\n",
        "    all_words_cn = np.array(all_words_cn)\n",
        "    print(\"All CN: \", all_words_cn.shape)\n",
        "    #Store all conceptored words in a dictonary\n",
        "    all_words = {}\n",
        "    for word, index in all_words_index.items():\n",
        "      #print(word, index)\n",
        "      if embd == 'elmo' or embd == 'bert':\n",
        "        all_words[word] = np.mean([all_words_cn[i,:] for i in index], axis = 0)\n",
        "      else:\n",
        "        all_words[word] = all_words_cn[index,:]\n",
        "    \n",
        "    if subspace == 'without_debiasing':\n",
        "      #WITHOUT CONCEPTOR\n",
        "      d_cn = weat_effect_size(math, arts, male, female, all_words)\n",
        "      p_cn = weat_p_value(math, arts, male, female, all_words, 1000)\n",
        "      print(\"Without debiasing\")\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    else:\n",
        "      #WITH CONCEPTOR\n",
        "      d_cn = weat_effect_size(math, arts, male, female, all_words)\n",
        "      p_cn = weat_p_value(math, arts, male, female, all_words, 1000)\n",
        "      print(\"With debiasing: \", embd, subspace)\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    \n",
        "    row = [embd, subspace, d_cn, p_cn]\n",
        "    results.append(row)\n",
        "    \n",
        "pd.DataFrame(np.array(results), columns = ['Embedding', 'Subspace', 'Effect Size', 'p-value'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Xi-W-t1DZC4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}