{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "winobias_debias_glove_only",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsedoc/ConceptorDebias/blob/master/winobias_debias_glove_only.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZznnJSKcfK15",
        "colab_type": "text"
      },
      "source": [
        "# Set up environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpX6n-YeacmL",
        "colab_type": "code",
        "outputId": "0462be09-4353-4868-9286-1c10e0c3c67f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.14.0rc1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.14.0rc0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.7)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.16.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r__9Mlj_sf8W",
        "colab_type": "code",
        "outputId": "be0a364b-c77b-4ee5-83d5-85be9df03fd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlSq3K_KHe5D",
        "colab_type": "code",
        "outputId": "61649b26-e8e8-4d64-bb48-f5cd01b5b003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/SebastianJia/e2e-coref"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'e2e-coref'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 214 (delta 8), reused 0 (delta 0), pack-reused 199\u001b[K\n",
            "Receiving objects: 100% (214/214), 88.09 KiB | 3.83 MiB/s, done.\n",
            "Resolving deltas: 100% (110/110), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Km7POnrH3nY",
        "colab_type": "code",
        "outputId": "4b5311a8-750e-49c7-ea1b-065acf08cc58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
            "\u001b[K     |████████████████████████████████| 345.2MB 58kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.13.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.16.4)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow-gpu)\n",
            "  Using cached https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu) (3.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n",
            "Collecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu)\n",
            "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
            "\u001b[31mERROR: tensorflow 1.14.0rc1 has requirement tensorflow-estimator<1.15.0rc0,>=1.14.0rc0, but you'll have tensorflow-estimator 1.13.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 1.14.0rc0\n",
            "    Uninstalling tensorflow-estimator-1.14.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0rc0\n",
            "Successfully installed mock-3.0.5 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zcMihsBIjwh",
        "colab_type": "code",
        "outputId": "65923bae-532c-410e-b8d8-8d93fff04067",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install tensorflow-hub"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.16.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-hub) (3.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4-RObrIIn48",
        "colab_type": "code",
        "outputId": "016d4a83-335b-488b-cc54-e6a1c6bdbcbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "!pip install h5py\n",
        "!pip install pyhocon\n",
        "!pip install scipy"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.16.4)\n",
            "Collecting pyhocon\n",
            "  Using cached https://files.pythonhosted.org/packages/3f/35/34e16968df0b8b65d3696d80b8add0aaffd4f0461c1ef3c0f066fdc747e8/pyhocon-0.3.51.tar.gz\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from pyhocon) (2.4.0)\n",
            "Building wheels for collected packages: pyhocon\n",
            "  Building wheel for pyhocon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/75/73/ee5c7152a5225cedaa52eb7fff3b24cef70e317f9a73c6deec\n",
            "Successfully built pyhocon\n",
            "Installing collected packages: pyhocon\n",
            "Successfully installed pyhocon-0.3.51\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.16.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbls_5pYIzUH",
        "colab_type": "code",
        "outputId": "1f255d39-7472-475f-dd80-e484f54beed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# get e2e-coref tgz file\n",
        "!gdown https://drive.google.com/uc?id=1fkifqZzdzsOEo0DXMzCFjiNXqsKG_cHi"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fkifqZzdzsOEo0DXMzCFjiNXqsKG_cHi\n",
            "To: /content/e2e-coref.tgz\n",
            "605MB [00:03, 173MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbZ1I4YPPJeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# put and extract tgz file in e2e-coref folder\n",
        "!mv /content/e2e-coref.tgz /content/e2e-coref/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGOWMAAEsD2a",
        "colab_type": "code",
        "outputId": "2c0af25f-12c2-41a6-b874-ff2e2fcafe28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/e2e-coref"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/e2e-coref\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7thwCDcJpju",
        "colab_type": "code",
        "outputId": "22bd6cf6-8605-405c-8cd5-54e07a3e07d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!tar -xzvf /content/e2e-coref/e2e-coref.tgz"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove_50_300_2.txt\n",
            "logs/\n",
            "logs/final/\n",
            "logs/final/model.max.ckpt.index\n",
            "logs/final/model.max.ckpt.data-00000-of-00001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaDrtrpUtO0u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a2254db6-f304-4a28-93d7-2d5f4e34add5"
      },
      "source": [
        "!tar -xzvf /content/ontonotes-release-5.0_LDC2013T19.tgz"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar (child): /content/ontonotes-release-5.0_LDC2013T19.tgz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHx9HJMQ7YnB",
        "colab_type": "code",
        "outputId": "018c3bad-d538-44b5-adf5-395ef9f2026f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "source": [
        "# Get glove embeddings\n",
        "!wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "!unzip glove.840B.300d.zip\n",
        "!rm glove.840B.300d.zip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-15 00:46:04--  https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip [following]\n",
            "--2019-06-15 00:46:04--  http://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2176768927 (2.0G) [application/zip]\n",
            "Saving to: ‘glove.840B.300d.zip’\n",
            "\n",
            "glove.840B.300d.zip 100%[===================>]   2.03G  64.1MB/s    in 39s     \n",
            "\n",
            "2019-06-15 00:46:43 (53.4 MB/s) - ‘glove.840B.300d.zip’ saved [2176768927/2176768927]\n",
            "\n",
            "Archive:  glove.840B.300d.zip\n",
            "  inflating: glove.840B.300d.txt     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qeq92V6J2HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up configuration environment for evaluation\n",
        "!bash ./setup_all.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvXNWiFOVghH",
        "colab_type": "code",
        "outputId": "aa3a1e5c-e169-4dcf-b615-45bf99a7c41f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Get a required file\n",
        "!wget https://raw.githubusercontent.com/luheng/lsgn/master/embeddings/char_vocab.english.txt"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-15 00:47:53--  https://raw.githubusercontent.com/luheng/lsgn/master/embeddings/char_vocab.english.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 266 [text/plain]\n",
            "Saving to: ‘char_vocab.english.txt’\n",
            "\n",
            "\rchar_vocab.english.   0%[                    ]       0  --.-KB/s               \rchar_vocab.english. 100%[===================>]     266  --.-KB/s    in 0s      \n",
            "\n",
            "2019-06-15 00:47:53 (48.3 MB/s) - ‘char_vocab.english.txt’ saved [266/266]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJ4w_pLzvh5m",
        "colab_type": "code",
        "outputId": "d4565674-fadd-4113-e683-2162ae31e67d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Get wino-bias dataset\n",
        "%cd /content/\n",
        "!git clone https://github.com/SebastianJia/corefBias.git"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'corefBias'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 471 (delta 3), reused 0 (delta 0), pack-reused 457\u001b[K\n",
            "Receiving objects: 100% (471/471), 84.18 MiB | 40.36 MiB/s, done.\n",
            "Resolving deltas: 100% (273/273), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E5PMpr90Rih",
        "colab_type": "code",
        "outputId": "61a9429e-b829-471e-f5b0-d67d8d202b9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/e2e-coref/\n",
        "# !wget https://raw.githubusercontent.com/SebastianJia/e2e-coref/master/minimize.py"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/e2e-coref\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvnBzSRrgDRw",
        "colab_type": "text"
      },
      "source": [
        "# Prepate test.english.jsonlines, the file to evaluate. \n",
        "There are 4 files to evaluate: test_type1_pro_stereotype.v4_auto_conll,   test_type1_anti_stereotype.v4_auto_conll,  test_type2_pro_stereotype.v4_auto_conll, \n",
        " test_type1_anti_stereotype.v4_auto_conll. \\\\\n",
        " \\\\\n",
        " Change the file name in 1st line and 2nd line to evaluate different files \\\\\n",
        "- move dataset to e2e-coref folder\n",
        "- rename it as dev.english.v4_auto_conll\n",
        "- run minimize.py to convert the file to jsonlines format. After running this line, there will be a IOError but it doesn't matter. Just skip it.\n",
        "- rename the file as test.english.jsonlines \n",
        "(The reason to rename it first as dev.english.v4_auto_conll is for simplicity, to run minimize.py directly without changing anything)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwRyMvpNwcJA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "1c34f4f1-4620-4992-e4f8-3c9f6070f664"
      },
      "source": [
        "!cp /content/corefBias/WinoBias/wino/data/conll_format/test_type1_pro_stereotype.v4_auto_conll /content/e2e-coref/\n",
        "!mv /content/e2e-coref/test_type1_pro_stereotype.v4_auto_conll /content/e2e-coref/dev.english.v4_auto_conll \n",
        "!python minimize.py dev.english.v4_auto_conll \n",
        "!cp /content/e2e-coref/dev.english.jsonlines /content/e2e-coref/test.english.jsonlines "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Minimizing dev.english.v4_auto_conll\n",
            "Wrote 396 documents to dev.english.jsonlines\n",
            "Minimizing train.english.v4_auto_conll\n",
            "Traceback (most recent call last):\n",
            "  File \"minimize.py\", line 197, in <module>\n",
            "    minimize_language(\"english\", labels, stats)\n",
            "  File \"minimize.py\", line 191, in minimize_language\n",
            "    minimize_partition(\"train\", language, \"v4_auto_conll\", labels, stats)\n",
            "  File \"minimize.py\", line 177, in minimize_partition\n",
            "    with open(input_path, \"r\") as input_file:\n",
            "IOError: [Errno 2] No such file or directory: 'train.english.v4_auto_conll'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81qmKoIt6qOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/e2e-coref/dev.english.jsonlines /content/e2e-coref/test.english.jsonlines "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0mYI9bFflS2",
        "colab_type": "code",
        "outputId": "e6f538e4-bd6f-4972-d10e-606be91f1a03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1163
        }
      },
      "source": [
        "#Cache elmo embeddings for sentences in test jsonfile\n",
        "!python cache_elmo.py /content/e2e-coref/test.english.jsonlines"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-06-15 00:48:14.412521: W tensorflow/core/graph/graph_constructor.cc:1272] Importing a graph with a lower producer version 26 into an existing graph with producer version 27. Shape inference will have run different parts of the graph with different producer versions.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0615 00:48:14.718728 139625619593088 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "2019-06-15 00:48:15.731265: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2019-06-15 00:48:15.955557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-06-15 00:48:15.956307: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55c813c2d600 executing computations on platform CUDA. Devices:\n",
            "2019-06-15 00:48:15.956343: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-06-15 00:48:15.977187: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-15 00:48:15.977466: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55c813c2cf20 executing computations on platform Host. Devices:\n",
            "2019-06-15 00:48:15.977498: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-06-15 00:48:15.977877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-06-15 00:48:15.977907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-06-15 00:48:15.979511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-06-15 00:48:15.979541: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-06-15 00:48:15.979552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-06-15 00:48:15.979776: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-06-15 00:48:15.979850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2019-06-15 00:48:16.666825: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "Cached 1 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 11 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 21 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 31 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 41 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 51 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 61 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 71 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 81 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 91 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 101 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 111 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 121 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 131 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 141 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 151 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 161 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 171 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 181 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 191 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 201 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 211 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 221 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 231 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 241 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 251 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 261 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 271 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 281 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 291 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 301 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 311 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 321 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 331 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 341 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 351 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 361 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 371 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 381 documents in /content/e2e-coref/test.english.jsonlines\n",
            "Cached 391 documents in /content/e2e-coref/test.english.jsonlines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIa4bvlrh5zM",
        "colab_type": "text"
      },
      "source": [
        "# Run prediction using original coref system and raw embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2JGUUu65kgm",
        "colab_type": "code",
        "outputId": "9626929c-6763-416d-b7ec-411f0c2915c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 22889
        }
      },
      "source": [
        "!python predict.py final /content/e2e-coref/test.english.jsonlines /content/e2e-coref/output.jsonlines"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Setting CUDA_VISIBLE_DEVICES to: \n",
            "Running experiment: final\n",
            "max_top_antecedents = 50\n",
            "max_training_sentences = 50\n",
            "top_span_ratio = 0.4\n",
            "filter_widths = [\n",
            "  3\n",
            "  4\n",
            "  5\n",
            "]\n",
            "filter_size = 50\n",
            "char_embedding_size = 8\n",
            "char_vocab_path = \"char_vocab.english.txt\"\n",
            "context_embeddings {\n",
            "  path = \"glove.840B.300d.txt\"\n",
            "  size = 300\n",
            "}\n",
            "head_embeddings {\n",
            "  path = \"glove_50_300_2.txt\"\n",
            "  size = 300\n",
            "}\n",
            "contextualization_size = 200\n",
            "contextualization_layers = 3\n",
            "ffnn_size = 150\n",
            "ffnn_depth = 2\n",
            "feature_size = 20\n",
            "max_span_width = 30\n",
            "use_metadata = true\n",
            "use_features = true\n",
            "model_heads = true\n",
            "coref_depth = 2\n",
            "lm_layers = 3\n",
            "lm_size = 1024\n",
            "coarse_to_fine = true\n",
            "max_gradient_norm = 5.0\n",
            "lstm_dropout_rate = 0.4\n",
            "lexical_dropout_rate = 0.5\n",
            "dropout_rate = 0.2\n",
            "optimizer = \"adam\"\n",
            "learning_rate = 0.001\n",
            "decay_rate = 0.999\n",
            "decay_frequency = 100\n",
            "train_path = \"train.english.jsonlines\"\n",
            "eval_path = \"test.english.jsonlines\"\n",
            "conll_eval_path = \"test.english.v4_gold_conll\"\n",
            "lm_path = \"elmo_cache.hdf5\"\n",
            "genres = [\n",
            "  \"bc\"\n",
            "  \"bn\"\n",
            "  \"mz\"\n",
            "  \"nw\"\n",
            "  \"pt\"\n",
            "  \"tc\"\n",
            "  \"wb\"\n",
            "]\n",
            "eval_frequency = 5000\n",
            "report_frequency = 100\n",
            "log_root = \"logs\"\n",
            "cluster {\n",
            "  addresses {\n",
            "    ps = [\n",
            "      \"localhost:2222\"\n",
            "    ]\n",
            "    worker = [\n",
            "      \"localhost:2223\"\n",
            "      \"localhost:2224\"\n",
            "    ]\n",
            "  }\n",
            "  gpus = [\n",
            "    0\n",
            "    1\n",
            "  ]\n",
            "}\n",
            "log_dir = \"logs/final\"\n",
            "Loading word embeddings from glove.840B.300d.txt...\n",
            "Done loading word embeddings.\n",
            "Loading word embeddings from glove_50_300_2.txt...\n",
            "Done loading word embeddings.\n",
            "WARNING:tensorflow:From /content/e2e-coref/coref_model.py:204: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0615 00:51:24.912312 139934195804032 deprecation.py:323] From /content/e2e-coref/coref_model.py:204: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "W0615 00:51:24.929908 139934195804032 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/e2e-coref/coref_model.py:273: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0615 00:51:25.400492 139934195804032 deprecation.py:506] From /content/e2e-coref/coref_model.py:273: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /content/e2e-coref/coref_model.py:480: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "W0615 00:51:25.471838 139934195804032 deprecation.py:323] From /content/e2e-coref/coref_model.py:480: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0615 00:51:25.472239 139934195804032 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "W0615 00:51:25.516377 139934195804032 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0615 00:51:35.162898 139934195804032 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "2019-06-15 00:51:36.453725: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2019-06-15 00:51:36.567852: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2019-06-15 00:51:36.568764: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:161] retrieving CUDA diagnostic information for host: b6f83ce654cb\n",
            "2019-06-15 00:51:36.569519: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:168] hostname: b6f83ce654cb\n",
            "2019-06-15 00:51:36.569588: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:192] libcuda reported version is: 410.79.0\n",
            "2019-06-15 00:51:36.569631: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:196] kernel reported version is: 410.79.0\n",
            "2019-06-15 00:51:36.569658: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:303] kernel version seems to match DSO: 410.79.0\n",
            "2019-06-15 00:51:36.626986: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-06-15 00:51:36.630408: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55d6ceed0f20 executing computations on platform Host. Devices:\n",
            "2019-06-15 00:51:36.630484: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "Restoring from logs/final/model.max.ckpt\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W0615 00:51:37.652128 139934195804032 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "Decoded 1 examples.\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 20, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 19, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 20, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 19, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 19, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 10, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 19, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 10, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 22, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "Decoded 101 examples.\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 10, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 10, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 10, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 21, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 19, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 20, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 21, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 20, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 20, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 22, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 19, 1024, 3)\n",
            "Decoded 201 examples.\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 19, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 19, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 10, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 10, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 19, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 19, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 20, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 19, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 20, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 21, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 19, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "Decoded 301 examples.\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 21, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 19, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 20, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 21, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 10, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 10, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 19, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 19, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 20, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 11, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 12, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 19, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 10, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 17, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 20, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 16, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 18, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 19, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 15, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 14, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 10, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 10, 1024, 3)\n",
            "(300,)\n",
            "(300,)\n",
            "(1, 13, 1024, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPFT5Atx-Uc1",
        "colab_type": "code",
        "outputId": "cec4b559-d91f-4531-b70a-0742db6e05ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "i = 0\n",
        "acc = 0\n",
        "y_pred = []\n",
        "with open('output.jsonlines') as output:\n",
        "  for line in output:\n",
        "    i +=1\n",
        "    result = eval(line)\n",
        "#     print(result['predicted_clusters']), print(result['clusters'][0])\n",
        "    is_in = 1\n",
        "    if result['predicted_clusters']!=[]:\n",
        "      for item in result['predicted_clusters'][0]: # I didn't use result['predicted_clusters']==result['clusters'] directly because sometimes the order of indices in predicted clusters and true clusters are different but it's actually correct.\n",
        "        if item not in result['clusters'][0]:\n",
        "          is_in = 0\n",
        "      if is_in == 1 :\n",
        "        acc = acc + 1\n",
        "        y_pred.append(1)\n",
        "      else:\n",
        "        y_pred.append(0)\n",
        "    else:\n",
        "      y_pred.append(0)\n",
        "\n",
        "y_truth = np.ones(i)\n",
        "\n",
        "from sklearn.metrics import f1_score # f1 score\n",
        "print(f1_score(y_truth, y_pred))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7981790591805766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJjhVX3xjLi-",
        "colab_type": "text"
      },
      "source": [
        "# Run experiment of debiasing GloVe only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec-vehIqp0zp",
        "colab_type": "code",
        "outputId": "848fa633-993c-4763-ae2a-1c8d20359ebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "## Load gender name lists, which will be used to calculate conceptor\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/male.txt\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/female.txt\n",
        "word_list_gender = []\n",
        "f = open('/content/e2e-coref/male.txt', 'r+')\n",
        "i = 0\n",
        "word_list= []\n",
        "for line in f:\n",
        "  i+=1\n",
        "  if i < 7:\n",
        "    continue\n",
        "  if i>2906:\n",
        "    break\n",
        "  token = line.split()\n",
        "  word_list_gender.append(token[0])\n",
        "f.close()\n",
        "print(len(word_list_gender))\n",
        "f = open('/content/e2e-coref/female.txt', 'r+')\n",
        "i = 0\n",
        "for line in f:\n",
        "  i+=1\n",
        "  if i < 7:\n",
        "    continue\n",
        "  if i>2906:\n",
        "    break\n",
        "  token = line.split()\n",
        "  word_list_gender.append(token[0])\n",
        "f.close()\n",
        "print(len(word_list_gender))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2900\n",
            "5800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh0QOvDc4Eiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load embeddings for each word in each sentence\n",
        "import numpy as np\n",
        "def load_cn_embedding(path):\n",
        "    raw_emb = []\n",
        "    print(\"Loading word embeddings from {}...\".format(path))\n",
        "    default_embedding = np.zeros(300)\n",
        "    if len(path) > 0:\n",
        "      vocab_size = None\n",
        "      with open(path) as f:\n",
        "        for i, line in enumerate(f.readlines()):\n",
        "          word_end = line.find(\" \")\n",
        "          word = line[:word_end]\n",
        "          embedding = np.fromstring(line[word_end + 1:], np.float32, sep=\" \")\n",
        "          assert len(embedding) == 300\n",
        "          if word in word_list_gender:\n",
        "            raw_emb.append(embedding)\n",
        "      print(\"Done loading word embeddings.\")\n",
        "    return np.array(raw_emb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AZd_eAJ4Inx",
        "colab_type": "code",
        "outputId": "b0c65fa1-e47d-4af9-851c-6e781c70f0bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# store embeddings in raw_embedding\n",
        "raw_embedding = load_cn_embedding('/content/e2e-coref/glove.840B.300d.txt')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading word embeddings from /content/e2e-coref/glove.840B.300d.txt...\n",
            "Done loading word embeddings.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALY6MIRsfPwd",
        "colab_type": "code",
        "outputId": "3628417d-08c3-43b1-a1d4-4f2fe4f5336a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Get conceptor fxns\n",
        "!wget https://raw.githubusercontent.com/jsedoc/ConceptorDebias/master/Conceptors/conceptor_fxns.py\n",
        "from conceptor_fxns import post_process_cn_matrix"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-06-15 01:05:05--  https://raw.githubusercontent.com/jsedoc/ConceptorDebias/master/Conceptors/conceptor_fxns.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2300 (2.2K) [text/plain]\n",
            "Saving to: ‘conceptor_fxns.py.1’\n",
            "\n",
            "\rconceptor_fxns.py.1   0%[                    ]       0  --.-KB/s               \rconceptor_fxns.py.1 100%[===================>]   2.25K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-06-15 01:05:05 (48.6 MB/s) - ‘conceptor_fxns.py.1’ saved [2300/2300]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voIEmolWfS5J",
        "colab_type": "code",
        "outputId": "5f065ec4-4acf-4e36-bb15-ac75cb8ea0e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "negC_glove,_,_ = post_process_cn_matrix(raw_embedding.T, 5) #alpha = 5\n",
        "negC_glove = negC_glove.T"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting...\n",
            "(300, 5133)\n",
            "R calculated\n",
            "C calculated\n",
            "negC calculated\n",
            "(5133, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrSPvIZeZjpM",
        "colab_type": "code",
        "outputId": "bb01989e-2c5b-4294-9fcb-c3a248c2da5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/e2e-coref"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/e2e-coref\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdRLrFzmvOXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the code in utils.py. Some libraries have some trouble to be loaded in colab, so I just copy the code here and remove those libraries. No big change and didn't impact the result.\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import errno\n",
        "import codecs\n",
        "import collections\n",
        "import json\n",
        "import math\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pyhocon\n",
        "\n",
        "\n",
        "def initialize_from_env():\n",
        "  if \"GPU\" in os.environ:\n",
        "    set_gpus(int(os.environ[\"GPU\"]))\n",
        "  else:\n",
        "    set_gpus()\n",
        "\n",
        "  name = 'final'\n",
        "  print(\"Running experiment: {}\".format(name))\n",
        "\n",
        "  config = pyhocon.ConfigFactory.parse_file(\"experiments.conf\")[name]\n",
        "  config[\"log_dir\"] = mkdirs(os.path.join(config[\"log_root\"], name))\n",
        "\n",
        "  print(pyhocon.HOCONConverter.convert(config, \"hocon\"))\n",
        "  return config\n",
        "\n",
        "def copy_checkpoint(source, target):\n",
        "  for ext in (\".index\", \".data-00000-of-00001\"):\n",
        "    shutil.copyfile(source + ext, target + ext)\n",
        "\n",
        "def make_summary(value_dict):\n",
        "  return tf.Summary(value=[tf.Summary.Value(tag=k, simple_value=v) for k,v in value_dict.items()])\n",
        "\n",
        "def flatten(l):\n",
        "  return [item for sublist in l for item in sublist]\n",
        "\n",
        "def set_gpus(*gpus):\n",
        "  os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(str(g) for g in gpus)\n",
        "  print(\"Setting CUDA_VISIBLE_DEVICES to: {}\".format(os.environ[\"CUDA_VISIBLE_DEVICES\"]))\n",
        "\n",
        "def mkdirs(path):\n",
        "  try:\n",
        "    os.makedirs(path)\n",
        "  except OSError as exception:\n",
        "    if exception.errno != errno.EEXIST:\n",
        "      raise\n",
        "  return path\n",
        "\n",
        "def load_char_dict(char_vocab_path):\n",
        "  vocab = [u\"<unk>\"]\n",
        "  with codecs.open(char_vocab_path, encoding=\"utf-8\") as f:\n",
        "    vocab.extend(l.strip() for l in f.readlines())\n",
        "  char_dict = collections.defaultdict(int)\n",
        "  char_dict.update({c:i for i, c in enumerate(vocab)})\n",
        "  return char_dict\n",
        "\n",
        "def maybe_divide(x, y):\n",
        "  return 0 if y == 0 else x / float(y)\n",
        "\n",
        "def projection(inputs, output_size, initializer=None):\n",
        "  return ffnn(inputs, 0, -1, output_size, dropout=None, output_weights_initializer=initializer)\n",
        "\n",
        "def highway(inputs, num_layers, dropout):\n",
        "  for i in range(num_layers):\n",
        "    with tf.variable_scope(\"highway_{}\".format(i)):\n",
        "      j, f = tf.split(projection(inputs, 2 * shape(inputs, -1)), 2, -1)\n",
        "      f = tf.sigmoid(f)\n",
        "      j = tf.nn.relu(j)\n",
        "      if dropout is not None:\n",
        "        j = tf.nn.dropout(j, dropout)\n",
        "      inputs = f * j + (1 - f) * inputs\n",
        "  return inputs\n",
        "\n",
        "def shape(x, dim):\n",
        "  return x.get_shape()[dim].value or tf.shape(x)[dim]\n",
        "\n",
        "def ffnn(inputs, num_hidden_layers, hidden_size, output_size, dropout, output_weights_initializer=None):\n",
        "  if len(inputs.get_shape()) > 3:\n",
        "    raise ValueError(\"FFNN with rank {} not supported\".format(len(inputs.get_shape())))\n",
        "\n",
        "  if len(inputs.get_shape()) == 3:\n",
        "    batch_size = shape(inputs, 0)\n",
        "    seqlen = shape(inputs, 1)\n",
        "    emb_size = shape(inputs, 2)\n",
        "    current_inputs = tf.reshape(inputs, [batch_size * seqlen, emb_size])\n",
        "  else:\n",
        "    current_inputs = inputs\n",
        "\n",
        "  for i in range(num_hidden_layers):\n",
        "    hidden_weights = tf.get_variable(\"hidden_weights_{}\".format(i), [shape(current_inputs, 1), hidden_size])\n",
        "    hidden_bias = tf.get_variable(\"hidden_bias_{}\".format(i), [hidden_size])\n",
        "    current_outputs = tf.nn.relu(tf.nn.xw_plus_b(current_inputs, hidden_weights, hidden_bias))\n",
        "\n",
        "    if dropout is not None:\n",
        "      current_outputs = tf.nn.dropout(current_outputs, dropout)\n",
        "    current_inputs = current_outputs\n",
        "\n",
        "  output_weights = tf.get_variable(\"output_weights\", [shape(current_inputs, 1), output_size], initializer=output_weights_initializer)\n",
        "  output_bias = tf.get_variable(\"output_bias\", [output_size])\n",
        "  outputs = tf.nn.xw_plus_b(current_inputs, output_weights, output_bias)\n",
        "\n",
        "  if len(inputs.get_shape()) == 3:\n",
        "    outputs = tf.reshape(outputs, [batch_size, seqlen, output_size])\n",
        "  return outputs\n",
        "\n",
        "def cnn(inputs, filter_sizes, num_filters):\n",
        "  num_words = shape(inputs, 0)\n",
        "  num_chars = shape(inputs, 1)\n",
        "  input_size = shape(inputs, 2)\n",
        "  outputs = []\n",
        "  for i, filter_size in enumerate(filter_sizes):\n",
        "    with tf.variable_scope(\"conv_{}\".format(i)):\n",
        "      w = tf.get_variable(\"w\", [filter_size, input_size, num_filters])\n",
        "      b = tf.get_variable(\"b\", [num_filters])\n",
        "    conv = tf.nn.conv1d(inputs, w, stride=1, padding=\"VALID\") # [num_words, num_chars - filter_size, num_filters]\n",
        "    h = tf.nn.relu(tf.nn.bias_add(conv, b)) # [num_words, num_chars - filter_size, num_filters]\n",
        "    pooled = tf.reduce_max(h, 1) # [num_words, num_filters]\n",
        "    outputs.append(pooled)\n",
        "  return tf.concat(outputs, 1) # [num_words, num_filters * len(filter_sizes)]\n",
        "\n",
        "def batch_gather(emb, indices):\n",
        "  batch_size = shape(emb, 0)\n",
        "  seqlen = shape(emb, 1)\n",
        "  if len(emb.get_shape()) > 2:\n",
        "    emb_size = shape(emb, 2)\n",
        "  else:\n",
        "    emb_size = 1\n",
        "  flattened_emb = tf.reshape(emb, [batch_size * seqlen, emb_size])  # [batch_size * seqlen, emb]\n",
        "  offset = tf.expand_dims(tf.range(batch_size) * seqlen, 1)  # [batch_size, 1]\n",
        "  gathered = tf.gather(flattened_emb, indices + offset) # [batch_size, num_indices, emb]\n",
        "  if len(emb.get_shape()) == 2:\n",
        "    gathered = tf.squeeze(gathered, 2) # [batch_size, num_indices]\n",
        "  return gathered\n",
        "\n",
        "class RetrievalEvaluator(object):\n",
        "  def __init__(self):\n",
        "    self._num_correct = 0\n",
        "    self._num_gold = 0\n",
        "    self._num_predicted = 0\n",
        "\n",
        "  def update(self, gold_set, predicted_set):\n",
        "    self._num_correct += len(gold_set & predicted_set)\n",
        "    self._num_gold += len(gold_set)\n",
        "    self._num_predicted += len(predicted_set)\n",
        "\n",
        "  def recall(self):\n",
        "    return maybe_divide(self._num_correct, self._num_gold)\n",
        "\n",
        "  def precision(self):\n",
        "    return maybe_divide(self._num_correct, self._num_predicted)\n",
        "\n",
        "  def metrics(self):\n",
        "    recall = self.recall()\n",
        "    precision = self.precision()\n",
        "    f1 = maybe_divide(2 * recall * precision, precision + recall)\n",
        "    return recall, precision, f1\n",
        "\n",
        "class EmbeddingDictionary(object):\n",
        "  def __init__(self, info, normalize=True, maybe_cache=None):\n",
        "    self._size = info[\"size\"]\n",
        "    self._normalize = normalize\n",
        "    self._path = info[\"path\"]\n",
        "    if maybe_cache is not None and maybe_cache._path == self._path:\n",
        "      assert self._size == maybe_cache._size\n",
        "      self._embeddings = maybe_cache._embeddings\n",
        "    else:\n",
        "      self._embeddings = self.load_embedding_dict(self._path)\n",
        "\n",
        "  @property\n",
        "  def size(self):\n",
        "    return self._size\n",
        "\n",
        "  def load_embedding_dict(self, path):\n",
        "    print(\"Loading word embeddings from {}...\".format(path))\n",
        "    default_embedding = np.zeros(self.size)\n",
        "    embedding_dict = collections.defaultdict(lambda:default_embedding)\n",
        "    if len(path) > 0:\n",
        "      vocab_size = None\n",
        "      with open(path) as f:\n",
        "        for i, line in enumerate(f.readlines()):\n",
        "          word_end = line.find(\" \")\n",
        "          word = line[:word_end]\n",
        "          embedding = np.fromstring(line[word_end + 1:], np.float32, sep=\" \")\n",
        "          assert len(embedding) == self.size\n",
        "          embedding_dict[word] = embedding\n",
        "      if vocab_size is not None:\n",
        "        assert vocab_size == len(embedding_dict)\n",
        "      print(\"Done loading word embeddings.\")\n",
        "    return embedding_dict\n",
        "\n",
        "  def __getitem__(self, key):\n",
        "    embedding = self._embeddings[key]\n",
        "    if self._normalize:\n",
        "      embedding = self.normalize(embedding)\n",
        "    return embedding\n",
        "\n",
        "  def normalize(self, v):\n",
        "    norm = np.linalg.norm(v)\n",
        "    if norm > 0:\n",
        "      return v / norm\n",
        "    else:\n",
        "      return v\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpUzCrUNyMYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNNCell is under different modules as tensorflow updates to 1.13 so I manually changed it to the corret module.\n",
        "class CustomLSTMCell(tf.nn.rnn_cell.RNNCell):\n",
        "  def __init__(self, num_units, batch_size, dropout):\n",
        "    self._num_units = num_units\n",
        "    self._dropout = dropout\n",
        "    self._dropout_mask = tf.nn.dropout(tf.ones([batch_size, self.output_size]), dropout)\n",
        "    self._initializer = self._block_orthonormal_initializer([self.output_size] * 3)\n",
        "    initial_cell_state = tf.get_variable(\"lstm_initial_cell_state\", [1, self.output_size])\n",
        "    initial_hidden_state = tf.get_variable(\"lstm_initial_hidden_state\", [1, self.output_size])\n",
        "    self._initial_state = tf.contrib.rnn.LSTMStateTuple(initial_cell_state, initial_hidden_state)\n",
        "\n",
        "  @property\n",
        "  def state_size(self):\n",
        "    return tf.contrib.rnn.LSTMStateTuple(self.output_size, self.output_size)\n",
        "\n",
        "  @property\n",
        "  def output_size(self):\n",
        "    return self._num_units\n",
        "\n",
        "  @property\n",
        "  def initial_state(self):\n",
        "    return self._initial_state\n",
        "\n",
        "  def __call__(self, inputs, state, scope=None):\n",
        "    \"\"\"Long short-term memory cell (LSTM).\"\"\"\n",
        "    with tf.variable_scope(scope or type(self).__name__):  # \"CustomLSTMCell\"\n",
        "      c, h = state\n",
        "      h *= self._dropout_mask\n",
        "      concat = projection(tf.concat([inputs, h], 1), 3 * self.output_size, initializer=self._initializer)\n",
        "      i, j, o = tf.split(concat, num_or_size_splits=3, axis=1)\n",
        "      i = tf.sigmoid(i)\n",
        "      new_c = (1 - i) * c  + i * tf.tanh(j)\n",
        "      new_h = tf.tanh(new_c) * tf.sigmoid(o)\n",
        "      new_state = tf.contrib.rnn.LSTMStateTuple(new_c, new_h)\n",
        "      return new_h, new_state\n",
        "\n",
        "  def _orthonormal_initializer(self, scale=1.0):\n",
        "    def _initializer(shape, dtype=tf.float32, partition_info=None):\n",
        "      M1 = np.random.randn(shape[0], shape[0]).astype(np.float32)\n",
        "      M2 = np.random.randn(shape[1], shape[1]).astype(np.float32)\n",
        "      Q1, R1 = np.linalg.qr(M1)\n",
        "      Q2, R2 = np.linalg.qr(M2)\n",
        "      Q1 = Q1 * np.sign(np.diag(R1))\n",
        "      Q2 = Q2 * np.sign(np.diag(R2))\n",
        "      n_min = min(shape[0], shape[1])\n",
        "      params = np.dot(Q1[:, :n_min], Q2[:n_min, :]) * scale\n",
        "      return params\n",
        "    return _initializer\n",
        "\n",
        "  def _block_orthonormal_initializer(self, output_sizes):\n",
        "    def _initializer(shape, dtype=np.float32, partition_info=None):\n",
        "      assert len(shape) == 2\n",
        "      assert sum(output_sizes) == shape[1]\n",
        "      initializer = self._orthonormal_initializer()\n",
        "      params = np.concatenate([initializer([shape[0], o], dtype, partition_info) for o in output_sizes], 1)\n",
        "      return params\n",
        "    return _initializer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D4OduvFpdK9",
        "colab_type": "code",
        "outputId": "3972d850-45cf-4fcb-9626-e2003f8ea252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# This is the code in coref_model.py. Added conceptor negation of GloVe in tensorize_example function\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import operator\n",
        "import random\n",
        "import math\n",
        "import json\n",
        "import threading\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import h5py\n",
        "# import util_func\n",
        "import coref_ops\n",
        "import conll\n",
        "import metrics\n",
        "\n",
        "class CorefModel(object):\n",
        "  def __init__(self, config):\n",
        "    self.indicator = 0\n",
        "    self.word_emb = []\n",
        "    self.config = config\n",
        "    self.context_embeddings =EmbeddingDictionary(config[\"context_embeddings\"])\n",
        "    self.head_embeddings = EmbeddingDictionary(config[\"head_embeddings\"], maybe_cache=self.context_embeddings)\n",
        "    self.char_embedding_size = config[\"char_embedding_size\"]\n",
        "    self.char_dict = load_char_dict(config[\"char_vocab_path\"])\n",
        "    self.max_span_width = config[\"max_span_width\"]\n",
        "    self.genres = { g:i for i,g in enumerate(config[\"genres\"]) }\n",
        "    if config[\"lm_path\"]:\n",
        "      self.lm_file = h5py.File(self.config[\"lm_path\"], \"r\")\n",
        "    else:\n",
        "      self.lm_file = None\n",
        "    self.lm_layers = self.config[\"lm_layers\"]\n",
        "    self.lm_size = self.config[\"lm_size\"]\n",
        "    self.eval_data = None # Load eval data lazily.\n",
        "\n",
        "    input_props = []\n",
        "    input_props.append((tf.string, [None, None])) # Tokens.\n",
        "    input_props.append((tf.float32, [None, None, self.context_embeddings.size])) # Context embeddings.\n",
        "    input_props.append((tf.float32, [None, None, self.head_embeddings.size])) # Head embeddings.\n",
        "    input_props.append((tf.float32, [None, None, self.lm_size, self.lm_layers])) # LM embeddings.\n",
        "    input_props.append((tf.int32, [None, None, None])) # Character indices.\n",
        "    input_props.append((tf.int32, [None])) # Text lengths.\n",
        "    input_props.append((tf.int32, [None])) # Speaker IDs.\n",
        "    input_props.append((tf.int32, [])) # Genre.\n",
        "    input_props.append((tf.bool, [])) # Is training.\n",
        "    input_props.append((tf.int32, [None])) # Gold starts.\n",
        "    input_props.append((tf.int32, [None])) # Gold ends.\n",
        "    input_props.append((tf.int32, [None])) # Cluster ids.\n",
        "\n",
        "    self.queue_input_tensors = [tf.placeholder(dtype, shape) for dtype, shape in input_props]\n",
        "    dtypes, shapes = zip(*input_props)\n",
        "    queue = tf.PaddingFIFOQueue(capacity=10, dtypes=dtypes, shapes=shapes)\n",
        "    self.enqueue_op = queue.enqueue(self.queue_input_tensors)\n",
        "    self.input_tensors = queue.dequeue()\n",
        "\n",
        "    self.predictions, self.loss = self.get_predictions_and_loss(*self.input_tensors)\n",
        "    self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "    self.reset_global_step = tf.assign(self.global_step, 0)\n",
        "    learning_rate = tf.train.exponential_decay(self.config[\"learning_rate\"], self.global_step,\n",
        "                                               self.config[\"decay_frequency\"], self.config[\"decay_rate\"], staircase=True)\n",
        "    trainable_params = tf.trainable_variables()\n",
        "    gradients = tf.gradients(self.loss, trainable_params)\n",
        "    gradients, _ = tf.clip_by_global_norm(gradients, self.config[\"max_gradient_norm\"])\n",
        "    optimizers = {\n",
        "      \"adam\" : tf.train.AdamOptimizer,\n",
        "      \"sgd\" : tf.train.GradientDescentOptimizer\n",
        "    }\n",
        "    optimizer = optimizers[self.config[\"optimizer\"]](learning_rate)\n",
        "    self.train_op = optimizer.apply_gradients(zip(gradients, trainable_params), global_step=self.global_step)\n",
        "\n",
        "  def start_enqueue_thread(self, session):\n",
        "    with open(self.config[\"train_path\"]) as f:\n",
        "      train_examples = [json.loads(jsonline) for jsonline in f.readlines()]\n",
        "    def _enqueue_loop():\n",
        "      while True:\n",
        "        random.shuffle(train_examples)\n",
        "        for example in train_examples:\n",
        "          tensorized_example = self.tensorize_example(example, is_training=True)\n",
        "          feed_dict = dict(zip(self.queue_input_tensors, tensorized_example))\n",
        "          session.run(self.enqueue_op, feed_dict=feed_dict)\n",
        "    enqueue_thread = threading.Thread(target=_enqueue_loop)\n",
        "    enqueue_thread.daemon = True\n",
        "    enqueue_thread.start()\n",
        "\n",
        "  def restore(self, session):\n",
        "    # Don't try to restore unused variables from the TF-Hub ELMo module.\n",
        "    vars_to_restore = [v for v in tf.global_variables() if \"module/\" not in v.name]\n",
        "    saver = tf.train.Saver(vars_to_restore)\n",
        "    checkpoint_path = os.path.join(self.config[\"log_dir\"], \"model.max.ckpt\")\n",
        "    print(\"Restoring from {}\".format(checkpoint_path))\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    saver.restore(session, checkpoint_path)\n",
        "\n",
        "  def load_lm_embeddings(self, doc_key):\n",
        "    if self.lm_file is None:\n",
        "      return np.zeros([0, 0, self.lm_size, self.lm_layers])\n",
        "    file_key = doc_key.replace(\"/\", \":\")\n",
        "    group = self.lm_file[file_key]\n",
        "    num_sentences = len(list(group.keys()))\n",
        "    sentences = [group[str(i)][...] for i in range(num_sentences)]\n",
        "    lm_emb = np.zeros([num_sentences, max(s.shape[0] for s in sentences), self.lm_size, self.lm_layers])\n",
        "    for i, s in enumerate(sentences):\n",
        "      lm_emb[i, :s.shape[0], :, :] = s\n",
        "#       print(s)\n",
        "    return lm_emb\n",
        "\n",
        "  def tensorize_mentions(self, mentions):\n",
        "    if len(mentions) > 0:\n",
        "      starts, ends = zip(*mentions)\n",
        "    else:\n",
        "      starts, ends = [], []\n",
        "    return np.array(starts), np.array(ends)\n",
        "\n",
        "  def tensorize_span_labels(self, tuples, label_dict):\n",
        "    if len(tuples) > 0:\n",
        "      starts, ends, labels = zip(*tuples)\n",
        "    else:\n",
        "      starts, ends, labels = [], [], []\n",
        "    return np.array(starts), np.array(ends), np.array([label_dict[c] for c in labels])\n",
        "\n",
        "  def tensorize_example(self, example, is_training):\n",
        "    clusters = example[\"clusters\"]\n",
        "\n",
        "    gold_mentions = sorted(tuple(m) for m in flatten(clusters))\n",
        "    gold_mention_map = {m:i for i,m in enumerate(gold_mentions)}\n",
        "    cluster_ids = np.zeros(len(gold_mentions))\n",
        "    for cluster_id, cluster in enumerate(clusters):\n",
        "      for mention in cluster:\n",
        "        cluster_ids[gold_mention_map[tuple(mention)]] = cluster_id + 1\n",
        "\n",
        "    sentences = example[\"sentences\"]\n",
        "    num_words = sum(len(s) for s in sentences)\n",
        "    speakers = flatten(example[\"speakers\"])\n",
        "\n",
        "    assert num_words == len(speakers)\n",
        "\n",
        "    max_sentence_length = max(len(s) for s in sentences)\n",
        "    max_word_length = max(max(max(len(w) for w in s) for s in sentences), max(self.config[\"filter_widths\"]))\n",
        "    text_len = np.array([len(s) for s in sentences])\n",
        "    tokens = [[\"\"] * max_sentence_length for _ in sentences]\n",
        "    context_word_emb = np.zeros([len(sentences), max_sentence_length, self.context_embeddings.size])\n",
        "    head_word_emb = np.zeros([len(sentences), max_sentence_length, self.head_embeddings.size])\n",
        "    char_index = np.zeros([len(sentences), max_sentence_length, max_word_length])\n",
        "    for i, sentence in enumerate(sentences):\n",
        "      for j, word in enumerate(sentence):\n",
        "        tokens[i][j] = word\n",
        "        context_word_emb[i, j] = negC_glove.dot(self.context_embeddings[word].T).T # conceptor negation\n",
        "        head_word_emb[i, j] = negC_glove.dot(self.head_embeddings[word].T).T # conceptor negation\n",
        "        char_index[i, j, :len(word)] = [self.char_dict[c] for c in word]\n",
        "    tokens = np.array(tokens)\n",
        "    speaker_dict = { s:i for i,s in enumerate(set(speakers)) }\n",
        "    speaker_ids = np.array([speaker_dict[s] for s in speakers])\n",
        "    doc_key = example[\"doc_key\"]\n",
        "    genre = self.genres[doc_key[:2]]\n",
        "\n",
        "    gold_starts, gold_ends = self.tensorize_mentions(gold_mentions)\n",
        "\n",
        "    lm_emb = self.load_lm_embeddings(doc_key)\n",
        "    example_tensors = (tokens, context_word_emb, head_word_emb, lm_emb, char_index, text_len, speaker_ids, genre, is_training, gold_starts, gold_ends, cluster_ids)\n",
        "\n",
        "    if is_training and len(sentences) > self.config[\"max_training_sentences\"]:\n",
        "      return self.truncate_example(*example_tensors)\n",
        "    else:\n",
        "      return example_tensors\n",
        "\n",
        "  def truncate_example(self, tokens, context_word_emb, head_word_emb, lm_emb, char_index, text_len, speaker_ids, genre, is_training, gold_starts, gold_ends, cluster_ids):\n",
        "    max_training_sentences = self.config[\"max_training_sentences\"]\n",
        "    num_sentences = context_word_emb.shape[0]\n",
        "    assert num_sentences > max_training_sentences\n",
        "\n",
        "    sentence_offset = random.randint(0, num_sentences - max_training_sentences)\n",
        "    word_offset = text_len[:sentence_offset].sum()\n",
        "    num_words = text_len[sentence_offset:sentence_offset + max_training_sentences].sum()\n",
        "    tokens = tokens[sentence_offset:sentence_offset + max_training_sentences, :]\n",
        "    context_word_emb = context_word_emb[sentence_offset:sentence_offset + max_training_sentences, :, :]\n",
        "    head_word_emb = head_word_emb[sentence_offset:sentence_offset + max_training_sentences, :, :]\n",
        "    lm_emb = lm_emb[sentence_offset:sentence_offset + max_training_sentences, :, :, :]\n",
        "    char_index = char_index[sentence_offset:sentence_offset + max_training_sentences, :, :]\n",
        "    text_len = text_len[sentence_offset:sentence_offset + max_training_sentences]\n",
        "\n",
        "    speaker_ids = speaker_ids[word_offset: word_offset + num_words]\n",
        "    gold_spans = np.logical_and(gold_ends >= word_offset, gold_starts < word_offset + num_words)\n",
        "    gold_starts = gold_starts[gold_spans] - word_offset\n",
        "    gold_ends = gold_ends[gold_spans] - word_offset\n",
        "    cluster_ids = cluster_ids[gold_spans]\n",
        "\n",
        "    return tokens, context_word_emb, head_word_emb, lm_emb, char_index, text_len, speaker_ids, genre, is_training, gold_starts, gold_ends, cluster_ids\n",
        "\n",
        "  def get_candidate_labels(self, candidate_starts, candidate_ends, labeled_starts, labeled_ends, labels):\n",
        "    same_start = tf.equal(tf.expand_dims(labeled_starts, 1), tf.expand_dims(candidate_starts, 0)) # [num_labeled, num_candidates]\n",
        "    same_end = tf.equal(tf.expand_dims(labeled_ends, 1), tf.expand_dims(candidate_ends, 0)) # [num_labeled, num_candidates]\n",
        "    same_span = tf.logical_and(same_start, same_end) # [num_labeled, num_candidates]\n",
        "    candidate_labels = tf.matmul(tf.expand_dims(labels, 0), tf.to_int32(same_span)) # [1, num_candidates]\n",
        "    candidate_labels = tf.squeeze(candidate_labels, 0) # [num_candidates]\n",
        "    return candidate_labels\n",
        "\n",
        "  def get_dropout(self, dropout_rate, is_training):\n",
        "    return 1 - (tf.to_float(is_training) * dropout_rate)\n",
        "\n",
        "  def coarse_to_fine_pruning(self, top_span_emb, top_span_mention_scores, c):\n",
        "    k = shape(top_span_emb, 0)\n",
        "    top_span_range = tf.range(k) # [k]\n",
        "    antecedent_offsets = tf.expand_dims(top_span_range, 1) - tf.expand_dims(top_span_range, 0) # [k, k]\n",
        "    antecedents_mask = antecedent_offsets >= 1 # [k, k]\n",
        "    fast_antecedent_scores = tf.expand_dims(top_span_mention_scores, 1) + tf.expand_dims(top_span_mention_scores, 0) # [k, k]\n",
        "    fast_antecedent_scores += tf.log(tf.to_float(antecedents_mask)) # [k, k]\n",
        "    fast_antecedent_scores += self.get_fast_antecedent_scores(top_span_emb) # [k, k]\n",
        "\n",
        "    _, top_antecedents = tf.nn.top_k(fast_antecedent_scores, c, sorted=False) # [k, c]\n",
        "    top_antecedents_mask = batch_gather(antecedents_mask, top_antecedents) # [k, c]\n",
        "    top_fast_antecedent_scores = batch_gather(fast_antecedent_scores, top_antecedents) # [k, c]\n",
        "    top_antecedent_offsets = batch_gather(antecedent_offsets, top_antecedents) # [k, c]\n",
        "    return top_antecedents, top_antecedents_mask, top_fast_antecedent_scores, top_antecedent_offsets\n",
        "\n",
        "  def distance_pruning(self, top_span_emb, top_span_mention_scores, c):\n",
        "    k = shape(top_span_emb, 0)\n",
        "    top_antecedent_offsets = tf.tile(tf.expand_dims(tf.range(c) + 1, 0), [k, 1]) # [k, c]\n",
        "    raw_top_antecedents = tf.expand_dims(tf.range(k), 1) - top_antecedent_offsets # [k, c]\n",
        "    top_antecedents_mask = raw_top_antecedents >= 0 # [k, c]\n",
        "    top_antecedents = tf.maximum(raw_top_antecedents, 0) # [k, c]\n",
        "\n",
        "    top_fast_antecedent_scores = tf.expand_dims(top_span_mention_scores, 1) + tf.gather(top_span_mention_scores, top_antecedents) # [k, c]\n",
        "    top_fast_antecedent_scores += tf.log(tf.to_float(top_antecedents_mask)) # [k, c]\n",
        "    return top_antecedents, top_antecedents_mask, top_fast_antecedent_scores, top_antecedent_offsets\n",
        "\n",
        "  def get_predictions_and_loss(self, tokens, context_word_emb, head_word_emb, lm_emb, char_index, text_len, speaker_ids, genre, is_training, gold_starts, gold_ends, cluster_ids):\n",
        "    self.dropout = self.get_dropout(self.config[\"dropout_rate\"], is_training)\n",
        "    self.lexical_dropout = self.get_dropout(self.config[\"lexical_dropout_rate\"], is_training)\n",
        "    self.lstm_dropout = self.get_dropout(self.config[\"lstm_dropout_rate\"], is_training)\n",
        "\n",
        "    num_sentences = tf.shape(context_word_emb)[0]\n",
        "    max_sentence_length = tf.shape(context_word_emb)[1]\n",
        "\n",
        "    context_emb_list = [context_word_emb]\n",
        "    head_emb_list = [head_word_emb]\n",
        "\n",
        "    if self.config[\"char_embedding_size\"] > 0:\n",
        "      char_emb = tf.gather(tf.get_variable(\"char_embeddings\", [len(self.char_dict), self.config[\"char_embedding_size\"]]), char_index) # [num_sentences, max_sentence_length, max_word_length, emb]\n",
        "      flattened_char_emb = tf.reshape(char_emb, [num_sentences * max_sentence_length, shape(char_emb, 2), shape(char_emb, 3)]) # [num_sentences * max_sentence_length, max_word_length, emb]\n",
        "      flattened_aggregated_char_emb = cnn(flattened_char_emb, self.config[\"filter_widths\"], self.config[\"filter_size\"]) # [num_sentences * max_sentence_length, emb]\n",
        "      aggregated_char_emb = tf.reshape(flattened_aggregated_char_emb, [num_sentences, max_sentence_length, shape(flattened_aggregated_char_emb, 1)]) # [num_sentences, max_sentence_length, emb]\n",
        "      context_emb_list.append(aggregated_char_emb)\n",
        "      head_emb_list.append(aggregated_char_emb)\n",
        "\n",
        "    if not self.lm_file:\n",
        "      self.indicator = 1\n",
        "      elmo_module = hub.Module(\"https://tfhub.dev/google/elmo/2\")\n",
        "      lm_embeddings = elmo_module(\n",
        "          inputs={\"tokens\": tokens, \"sequence_len\": text_len},\n",
        "          signature=\"tokens\", as_dict=True)\n",
        "      word_emb = lm_embeddings[\"word_emb\"]  # [num_sentences, max_sentence_length, 512]\n",
        "      self.word_emb = word_emb\n",
        "      lm_emb = tf.stack([tf.concat([word_emb, word_emb], -1),\n",
        "                         lm_embeddings[\"lstm_outputs1\"],\n",
        "                         lm_embeddings[\"lstm_outputs2\"]], -1)  # [num_sentences, max_sentence_length, 1024, 3]\n",
        "    lm_emb_size = shape(lm_emb, 2)\n",
        "    lm_num_layers = shape(lm_emb, 3)\n",
        "    with tf.variable_scope(\"lm_aggregation\"):\n",
        "      self.lm_weights = tf.nn.softmax(tf.get_variable(\"lm_scores\", [lm_num_layers], initializer=tf.constant_initializer(0.0)))\n",
        "      self.lm_scaling = tf.get_variable(\"lm_scaling\", [], initializer=tf.constant_initializer(1.0))\n",
        "    flattened_lm_emb = tf.reshape(lm_emb, [num_sentences * max_sentence_length * lm_emb_size, lm_num_layers])\n",
        "    flattened_aggregated_lm_emb = tf.matmul(flattened_lm_emb, tf.expand_dims(self.lm_weights, 1)) # [num_sentences * max_sentence_length * emb, 1]\n",
        "    aggregated_lm_emb = tf.reshape(flattened_aggregated_lm_emb, [num_sentences, max_sentence_length, lm_emb_size])\n",
        "    aggregated_lm_emb *= self.lm_scaling\n",
        "    context_emb_list.append(aggregated_lm_emb)\n",
        "\n",
        "    context_emb = tf.concat(context_emb_list, 2) # [num_sentences, max_sentence_length, emb]\n",
        "    head_emb = tf.concat(head_emb_list, 2) # [num_sentences, max_sentence_length, emb]\n",
        "    context_emb = tf.nn.dropout(context_emb, self.lexical_dropout) # [num_sentences, max_sentence_length, emb]\n",
        "    head_emb = tf.nn.dropout(head_emb, self.lexical_dropout) # [num_sentences, max_sentence_length, emb]\n",
        "\n",
        "    text_len_mask = tf.sequence_mask(text_len, maxlen=max_sentence_length) # [num_sentence, max_sentence_length]\n",
        "\n",
        "    context_outputs = self.lstm_contextualize(context_emb, text_len, text_len_mask) # [num_words, emb]\n",
        "    num_words = shape(context_outputs, 0)\n",
        "\n",
        "    genre_emb = tf.gather(tf.get_variable(\"genre_embeddings\", [len(self.genres), self.config[\"feature_size\"]]), genre) # [emb]\n",
        "\n",
        "    sentence_indices = tf.tile(tf.expand_dims(tf.range(num_sentences), 1), [1, max_sentence_length]) # [num_sentences, max_sentence_length]\n",
        "    flattened_sentence_indices = self.flatten_emb_by_sentence(sentence_indices, text_len_mask) # [num_words]\n",
        "    flattened_head_emb = self.flatten_emb_by_sentence(head_emb, text_len_mask) # [num_words]\n",
        "\n",
        "    candidate_starts = tf.tile(tf.expand_dims(tf.range(num_words), 1), [1, self.max_span_width]) # [num_words, max_span_width]\n",
        "    candidate_ends = candidate_starts + tf.expand_dims(tf.range(self.max_span_width), 0) # [num_words, max_span_width]\n",
        "    candidate_start_sentence_indices = tf.gather(flattened_sentence_indices, candidate_starts) # [num_words, max_span_width]\n",
        "    candidate_end_sentence_indices = tf.gather(flattened_sentence_indices, tf.minimum(candidate_ends, num_words - 1)) # [num_words, max_span_width]\n",
        "    candidate_mask = tf.logical_and(candidate_ends < num_words, tf.equal(candidate_start_sentence_indices, candidate_end_sentence_indices)) # [num_words, max_span_width]\n",
        "    flattened_candidate_mask = tf.reshape(candidate_mask, [-1]) # [num_words * max_span_width]\n",
        "    candidate_starts = tf.boolean_mask(tf.reshape(candidate_starts, [-1]), flattened_candidate_mask) # [num_candidates]\n",
        "    candidate_ends = tf.boolean_mask(tf.reshape(candidate_ends, [-1]), flattened_candidate_mask) # [num_candidates]\n",
        "    candidate_sentence_indices = tf.boolean_mask(tf.reshape(candidate_start_sentence_indices, [-1]), flattened_candidate_mask) # [num_candidates]\n",
        "\n",
        "    candidate_cluster_ids = self.get_candidate_labels(candidate_starts, candidate_ends, gold_starts, gold_ends, cluster_ids) # [num_candidates]\n",
        "\n",
        "    candidate_span_emb = self.get_span_emb(flattened_head_emb, context_outputs, candidate_starts, candidate_ends) # [num_candidates, emb]\n",
        "    candidate_mention_scores =  self.get_mention_scores(candidate_span_emb) # [k, 1]\n",
        "    candidate_mention_scores = tf.squeeze(candidate_mention_scores, 1) # [k]\n",
        "\n",
        "    k = tf.to_int32(tf.floor(tf.to_float(tf.shape(context_outputs)[0]) * self.config[\"top_span_ratio\"]))\n",
        "    top_span_indices = coref_ops.extract_spans(tf.expand_dims(candidate_mention_scores, 0),\n",
        "                                               tf.expand_dims(candidate_starts, 0),\n",
        "                                               tf.expand_dims(candidate_ends, 0),\n",
        "                                               tf.expand_dims(k, 0),\n",
        "                                               shape(context_outputs, 0),\n",
        "                                               True) # [1, k]\n",
        "    top_span_indices.set_shape([1, None])\n",
        "    top_span_indices = tf.squeeze(top_span_indices, 0) # [k]\n",
        "\n",
        "    top_span_starts = tf.gather(candidate_starts, top_span_indices) # [k]\n",
        "    top_span_ends = tf.gather(candidate_ends, top_span_indices) # [k]\n",
        "    top_span_emb = tf.gather(candidate_span_emb, top_span_indices) # [k, emb]\n",
        "    top_span_cluster_ids = tf.gather(candidate_cluster_ids, top_span_indices) # [k]\n",
        "    top_span_mention_scores = tf.gather(candidate_mention_scores, top_span_indices) # [k]\n",
        "    top_span_sentence_indices = tf.gather(candidate_sentence_indices, top_span_indices) # [k]\n",
        "    top_span_speaker_ids = tf.gather(speaker_ids, top_span_starts) # [k]\n",
        "\n",
        "    c = tf.minimum(self.config[\"max_top_antecedents\"], k)\n",
        "\n",
        "    if self.config[\"coarse_to_fine\"]:\n",
        "      top_antecedents, top_antecedents_mask, top_fast_antecedent_scores, top_antecedent_offsets = self.coarse_to_fine_pruning(top_span_emb, top_span_mention_scores, c)\n",
        "    else:\n",
        "      top_antecedents, top_antecedents_mask, top_fast_antecedent_scores, top_antecedent_offsets = self.distance_pruning(top_span_emb, top_span_mention_scores, c)\n",
        "\n",
        "    dummy_scores = tf.zeros([k, 1]) # [k, 1]\n",
        "    for i in range(self.config[\"coref_depth\"]):\n",
        "      with tf.variable_scope(\"coref_layer\", reuse=(i > 0)):\n",
        "        top_antecedent_emb = tf.gather(top_span_emb, top_antecedents) # [k, c, emb]\n",
        "        top_antecedent_scores = top_fast_antecedent_scores + self.get_slow_antecedent_scores(top_span_emb, top_antecedents, top_antecedent_emb, top_antecedent_offsets, top_span_speaker_ids, genre_emb) # [k, c]\n",
        "        top_antecedent_weights = tf.nn.softmax(tf.concat([dummy_scores, top_antecedent_scores], 1)) # [k, c + 1]\n",
        "        top_antecedent_emb = tf.concat([tf.expand_dims(top_span_emb, 1), top_antecedent_emb], 1) # [k, c + 1, emb]\n",
        "        attended_span_emb = tf.reduce_sum(tf.expand_dims(top_antecedent_weights, 2) * top_antecedent_emb, 1) # [k, emb]\n",
        "        with tf.variable_scope(\"f\"):\n",
        "          f = tf.sigmoid(projection(tf.concat([top_span_emb, attended_span_emb], 1), shape(top_span_emb, -1))) # [k, emb]\n",
        "          top_span_emb = f * attended_span_emb + (1 - f) * top_span_emb # [k, emb]\n",
        "\n",
        "    top_antecedent_scores = tf.concat([dummy_scores, top_antecedent_scores], 1) # [k, c + 1]\n",
        "\n",
        "    top_antecedent_cluster_ids = tf.gather(top_span_cluster_ids, top_antecedents) # [k, c]\n",
        "    top_antecedent_cluster_ids += tf.to_int32(tf.log(tf.to_float(top_antecedents_mask))) # [k, c]\n",
        "    same_cluster_indicator = tf.equal(top_antecedent_cluster_ids, tf.expand_dims(top_span_cluster_ids, 1)) # [k, c]\n",
        "    non_dummy_indicator = tf.expand_dims(top_span_cluster_ids > 0, 1) # [k, 1]\n",
        "    pairwise_labels = tf.logical_and(same_cluster_indicator, non_dummy_indicator) # [k, c]\n",
        "    dummy_labels = tf.logical_not(tf.reduce_any(pairwise_labels, 1, keepdims=True)) # [k, 1]\n",
        "    top_antecedent_labels = tf.concat([dummy_labels, pairwise_labels], 1) # [k, c + 1]\n",
        "    loss = self.softmax_loss(top_antecedent_scores, top_antecedent_labels) # [k]\n",
        "    loss = tf.reduce_sum(loss) # []\n",
        "\n",
        "    return [candidate_starts, candidate_ends, candidate_mention_scores, top_span_starts, top_span_ends, top_antecedents, top_antecedent_scores], loss\n",
        "\n",
        "  def get_span_emb(self, head_emb, context_outputs, span_starts, span_ends):\n",
        "    span_emb_list = []\n",
        "\n",
        "    span_start_emb = tf.gather(context_outputs, span_starts) # [k, emb]\n",
        "    span_emb_list.append(span_start_emb)\n",
        "\n",
        "    span_end_emb = tf.gather(context_outputs, span_ends) # [k, emb]\n",
        "    span_emb_list.append(span_end_emb)\n",
        "\n",
        "    span_width = 1 + span_ends - span_starts # [k]\n",
        "\n",
        "    if self.config[\"use_features\"]:\n",
        "      span_width_index = span_width - 1 # [k]\n",
        "      span_width_emb = tf.gather(tf.get_variable(\"span_width_embeddings\", [self.config[\"max_span_width\"], self.config[\"feature_size\"]]), span_width_index) # [k, emb]\n",
        "      span_width_emb = tf.nn.dropout(span_width_emb, self.dropout)\n",
        "      span_emb_list.append(span_width_emb)\n",
        "\n",
        "    if self.config[\"model_heads\"]:\n",
        "      span_indices = tf.expand_dims(tf.range(self.config[\"max_span_width\"]), 0) + tf.expand_dims(span_starts, 1) # [k, max_span_width]\n",
        "      span_indices = tf.minimum(shape(context_outputs, 0) - 1, span_indices) # [k, max_span_width]\n",
        "      span_text_emb = tf.gather(head_emb, span_indices) # [k, max_span_width, emb]\n",
        "      with tf.variable_scope(\"head_scores\"):\n",
        "        self.head_scores = projection(context_outputs, 1) # [num_words, 1]\n",
        "      span_head_scores = tf.gather(self.head_scores, span_indices) # [k, max_span_width, 1]\n",
        "      span_mask = tf.expand_dims(tf.sequence_mask(span_width, self.config[\"max_span_width\"], dtype=tf.float32), 2) # [k, max_span_width, 1]\n",
        "      span_head_scores += tf.log(span_mask) # [k, max_span_width, 1]\n",
        "      span_attention = tf.nn.softmax(span_head_scores, 1) # [k, max_span_width, 1]\n",
        "      span_head_emb = tf.reduce_sum(span_attention * span_text_emb, 1) # [k, emb]\n",
        "      span_emb_list.append(span_head_emb)\n",
        "\n",
        "    span_emb = tf.concat(span_emb_list, 1) # [k, emb]\n",
        "    return span_emb # [k, emb]\n",
        "\n",
        "  def get_mention_scores(self, span_emb):\n",
        "    with tf.variable_scope(\"mention_scores\"):\n",
        "      return ffnn(span_emb, self.config[\"ffnn_depth\"], self.config[\"ffnn_size\"], 1, self.dropout) # [k, 1]\n",
        "\n",
        "  def softmax_loss(self, antecedent_scores, antecedent_labels):\n",
        "    gold_scores = antecedent_scores + tf.log(tf.to_float(antecedent_labels)) # [k, max_ant + 1]\n",
        "    marginalized_gold_scores = tf.reduce_logsumexp(gold_scores, [1]) # [k]\n",
        "    log_norm = tf.reduce_logsumexp(antecedent_scores, [1]) # [k]\n",
        "    return log_norm - marginalized_gold_scores # [k]\n",
        "\n",
        "  def bucket_distance(self, distances):\n",
        "    \"\"\"\n",
        "    Places the given values (designed for distances) into 10 semi-logscale buckets:\n",
        "    [0, 1, 2, 3, 4, 5-7, 8-15, 16-31, 32-63, 64+].\n",
        "    \"\"\"\n",
        "    logspace_idx = tf.to_int32(tf.floor(tf.log(tf.to_float(distances))/math.log(2))) + 3\n",
        "    use_identity = tf.to_int32(distances <= 4)\n",
        "    combined_idx = use_identity * distances + (1 - use_identity) * logspace_idx\n",
        "    return tf.clip_by_value(combined_idx, 0, 9)\n",
        "\n",
        "  def get_slow_antecedent_scores(self, top_span_emb, top_antecedents, top_antecedent_emb, top_antecedent_offsets, top_span_speaker_ids, genre_emb):\n",
        "    k = shape(top_span_emb, 0)\n",
        "    c = shape(top_antecedents, 1)\n",
        "\n",
        "    feature_emb_list = []\n",
        "\n",
        "    if self.config[\"use_metadata\"]:\n",
        "      top_antecedent_speaker_ids = tf.gather(top_span_speaker_ids, top_antecedents) # [k, c]\n",
        "      same_speaker = tf.equal(tf.expand_dims(top_span_speaker_ids, 1), top_antecedent_speaker_ids) # [k, c]\n",
        "      speaker_pair_emb = tf.gather(tf.get_variable(\"same_speaker_emb\", [2, self.config[\"feature_size\"]]), tf.to_int32(same_speaker)) # [k, c, emb]\n",
        "      feature_emb_list.append(speaker_pair_emb)\n",
        "\n",
        "      tiled_genre_emb = tf.tile(tf.expand_dims(tf.expand_dims(genre_emb, 0), 0), [k, c, 1]) # [k, c, emb]\n",
        "      feature_emb_list.append(tiled_genre_emb)\n",
        "\n",
        "    if self.config[\"use_features\"]:\n",
        "      antecedent_distance_buckets = self.bucket_distance(top_antecedent_offsets) # [k, c]\n",
        "      antecedent_distance_emb = tf.gather(tf.get_variable(\"antecedent_distance_emb\", [10, self.config[\"feature_size\"]]), antecedent_distance_buckets) # [k, c]\n",
        "      feature_emb_list.append(antecedent_distance_emb)\n",
        "\n",
        "    feature_emb = tf.concat(feature_emb_list, 2) # [k, c, emb]\n",
        "    feature_emb = tf.nn.dropout(feature_emb, self.dropout) # [k, c, emb]\n",
        "\n",
        "    target_emb = tf.expand_dims(top_span_emb, 1) # [k, 1, emb]\n",
        "    similarity_emb = top_antecedent_emb * target_emb # [k, c, emb]\n",
        "    target_emb = tf.tile(target_emb, [1, c, 1]) # [k, c, emb]\n",
        "\n",
        "    pair_emb = tf.concat([target_emb, top_antecedent_emb, similarity_emb, feature_emb], 2) # [k, c, emb]\n",
        "\n",
        "    with tf.variable_scope(\"slow_antecedent_scores\"):\n",
        "      slow_antecedent_scores =ffnn(pair_emb, self.config[\"ffnn_depth\"], self.config[\"ffnn_size\"], 1, self.dropout) # [k, c, 1]\n",
        "    slow_antecedent_scores = tf.squeeze(slow_antecedent_scores, 2) # [k, c]\n",
        "    return slow_antecedent_scores # [k, c]\n",
        "\n",
        "  def get_fast_antecedent_scores(self, top_span_emb):\n",
        "    with tf.variable_scope(\"src_projection\"):\n",
        "      source_top_span_emb = tf.nn.dropout(projection(top_span_emb, shape(top_span_emb, -1)), self.dropout) # [k, emb]\n",
        "    target_top_span_emb = tf.nn.dropout(top_span_emb, self.dropout) # [k, emb]\n",
        "    return tf.matmul(source_top_span_emb, target_top_span_emb, transpose_b=True) # [k, k]\n",
        "\n",
        "  def flatten_emb_by_sentence(self, emb, text_len_mask):\n",
        "    num_sentences = tf.shape(emb)[0]\n",
        "    max_sentence_length = tf.shape(emb)[1]\n",
        "\n",
        "    emb_rank = len(emb.get_shape())\n",
        "    if emb_rank  == 2:\n",
        "      flattened_emb = tf.reshape(emb, [num_sentences * max_sentence_length])\n",
        "    elif emb_rank == 3:\n",
        "      flattened_emb = tf.reshape(emb, [num_sentences * max_sentence_length,shape(emb, 2)])\n",
        "    else:\n",
        "      raise ValueError(\"Unsupported rank: {}\".format(emb_rank))\n",
        "    return tf.boolean_mask(flattened_emb, tf.reshape(text_len_mask, [num_sentences * max_sentence_length]))\n",
        "\n",
        "  def lstm_contextualize(self, text_emb, text_len, text_len_mask):\n",
        "    num_sentences = tf.shape(text_emb)[0]\n",
        "\n",
        "    current_inputs = text_emb # [num_sentences, max_sentence_length, emb]\n",
        "\n",
        "    for layer in range(self.config[\"contextualization_layers\"]):\n",
        "      with tf.variable_scope(\"layer_{}\".format(layer)):\n",
        "        with tf.variable_scope(\"fw_cell\"):\n",
        "          cell_fw = CustomLSTMCell(self.config[\"contextualization_size\"], num_sentences, self.lstm_dropout)\n",
        "        with tf.variable_scope(\"bw_cell\"):\n",
        "          cell_bw = CustomLSTMCell(self.config[\"contextualization_size\"], num_sentences, self.lstm_dropout)\n",
        "        state_fw = tf.contrib.rnn.LSTMStateTuple(tf.tile(cell_fw.initial_state.c, [num_sentences, 1]), tf.tile(cell_fw.initial_state.h, [num_sentences, 1]))\n",
        "        state_bw = tf.contrib.rnn.LSTMStateTuple(tf.tile(cell_bw.initial_state.c, [num_sentences, 1]), tf.tile(cell_bw.initial_state.h, [num_sentences, 1]))\n",
        "\n",
        "        (fw_outputs, bw_outputs), _ = tf.nn.bidirectional_dynamic_rnn(\n",
        "          cell_fw=cell_fw,\n",
        "          cell_bw=cell_bw,\n",
        "          inputs=current_inputs,\n",
        "          sequence_length=text_len,\n",
        "          initial_state_fw=state_fw,\n",
        "          initial_state_bw=state_bw)\n",
        "\n",
        "        text_outputs = tf.concat([fw_outputs, bw_outputs], 2) # [num_sentences, max_sentence_length, emb]\n",
        "        text_outputs = tf.nn.dropout(text_outputs, self.lstm_dropout)\n",
        "        if layer > 0:\n",
        "          highway_gates = tf.sigmoid(projection(text_outputs, shape(text_outputs, 2))) # [num_sentences, max_sentence_length, emb]\n",
        "          text_outputs = highway_gates * text_outputs + (1 - highway_gates) * current_inputs\n",
        "        current_inputs = text_outputs\n",
        "\n",
        "    return self.flatten_emb_by_sentence(text_outputs, text_len_mask)\n",
        "\n",
        "  def get_predicted_antecedents(self, antecedents, antecedent_scores):\n",
        "    predicted_antecedents = []\n",
        "    for i, index in enumerate(np.argmax(antecedent_scores, axis=1) - 1):\n",
        "      if index < 0:\n",
        "        predicted_antecedents.append(-1)\n",
        "      else:\n",
        "        predicted_antecedents.append(antecedents[i, index])\n",
        "    return predicted_antecedents\n",
        "\n",
        "  def get_predicted_clusters(self, top_span_starts, top_span_ends, predicted_antecedents):\n",
        "    mention_to_predicted = {}\n",
        "    predicted_clusters = []\n",
        "    for i, predicted_index in enumerate(predicted_antecedents):\n",
        "      if predicted_index < 0:\n",
        "        continue\n",
        "      assert i > predicted_index\n",
        "      predicted_antecedent = (int(top_span_starts[predicted_index]), int(top_span_ends[predicted_index]))\n",
        "      if predicted_antecedent in mention_to_predicted:\n",
        "        predicted_cluster = mention_to_predicted[predicted_antecedent]\n",
        "      else:\n",
        "        predicted_cluster = len(predicted_clusters)\n",
        "        predicted_clusters.append([predicted_antecedent])\n",
        "        mention_to_predicted[predicted_antecedent] = predicted_cluster\n",
        "\n",
        "      mention = (int(top_span_starts[i]), int(top_span_ends[i]))\n",
        "      predicted_clusters[predicted_cluster].append(mention)\n",
        "      mention_to_predicted[mention] = predicted_cluster\n",
        "\n",
        "    predicted_clusters = [tuple(pc) for pc in predicted_clusters]\n",
        "    mention_to_predicted = { m:predicted_clusters[i] for m,i in mention_to_predicted.items() }\n",
        "\n",
        "    return predicted_clusters, mention_to_predicted\n",
        "\n",
        "  def evaluate_coref(self, top_span_starts, top_span_ends, predicted_antecedents, gold_clusters, evaluator):\n",
        "    gold_clusters = [tuple(tuple(m) for m in gc) for gc in gold_clusters]\n",
        "    mention_to_gold = {}\n",
        "    for gc in gold_clusters:\n",
        "      for mention in gc:\n",
        "        mention_to_gold[mention] = gc\n",
        "\n",
        "    predicted_clusters, mention_to_predicted = self.get_predicted_clusters(top_span_starts, top_span_ends, predicted_antecedents)\n",
        "    evaluator.update(predicted_clusters, gold_clusters, mention_to_predicted, mention_to_gold)\n",
        "    return predicted_clusters\n",
        "\n",
        "  def load_eval_data(self):\n",
        "    if self.eval_data is None:\n",
        "      def load_line(line):\n",
        "        example = json.loads(line)\n",
        "        return self.tensorize_example(example, is_training=False), example\n",
        "      with open(self.config[\"eval_path\"]) as f:\n",
        "        self.eval_data = [load_line(l) for l in f.readlines()]\n",
        "      num_words = sum(tensorized_example[2].sum() for tensorized_example, _ in self.eval_data)\n",
        "      print(\"Loaded {} eval examples.\".format(len(self.eval_data)))\n",
        "\n",
        "  def evaluate(self, session, official_stdout=False):\n",
        "    self.load_eval_data()\n",
        "\n",
        "    coref_predictions = {}\n",
        "    coref_evaluator = metrics.CorefEvaluator()\n",
        "\n",
        "    for example_num, (tensorized_example, example) in enumerate(self.eval_data):\n",
        "      _, _, _, _, _, _, _, _, _, gold_starts, gold_ends, _ = tensorized_example\n",
        "      feed_dict = {i:t for i,t in zip(self.input_tensors, tensorized_example)}\n",
        "      candidate_starts, candidate_ends, candidate_mention_scores, top_span_starts, top_span_ends, top_antecedents, top_antecedent_scores = session.run(self.predictions, feed_dict=feed_dict)\n",
        "      predicted_antecedents = self.get_predicted_antecedents(top_antecedents, top_antecedent_scores)\n",
        "      coref_predictions[example[\"doc_key\"]] = self.evaluate_coref(top_span_starts, top_span_ends, predicted_antecedents, example[\"clusters\"], coref_evaluator)\n",
        "      if example_num % 10 == 0:\n",
        "        print(\"Evaluated {}/{} examples.\".format(example_num + 1, len(self.eval_data)))\n",
        "\n",
        "    summary_dict = {}\n",
        "    conll_results = conll.evaluate_conll(self.config[\"conll_eval_path\"], coref_predictions, official_stdout)\n",
        "    average_f1 = sum(results[\"f\"] for results in conll_results.values()) / len(conll_results)\n",
        "    summary_dict[\"Average F1 (conll)\"] = average_f1\n",
        "    print(\"Average F1 (conll): {:.2f}%\".format(average_f1))\n",
        "\n",
        "    p,r,f = coref_evaluator.get_prf()\n",
        "    summary_dict[\"Average F1 (py)\"] = f\n",
        "    print(\"Average F1 (py): {:.2f}%\".format(f * 100))\n",
        "    summary_dict[\"Average precision (py)\"] = p\n",
        "    print(\"Average precision (py): {:.2f}%\".format(p * 100))\n",
        "    summary_dict[\"Average recall (py)\"] = r\n",
        "    print(\"Average recall (py): {:.2f}%\".format(r * 100))\n",
        "\n",
        "    return make_summary(summary_dict), average_f1"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eiAeK7Rm--u",
        "colab_type": "code",
        "outputId": "414897bd-f9ae-4eb0-a976-a38f08fd4e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 29511
        }
      },
      "source": [
        "# Run experiment \n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import sys\n",
        "import json\n",
        "\n",
        "import tensorflow as tf\n",
        "# import coref_model as cm\n",
        "# import util\n",
        "\n",
        "\n",
        "config = initialize_from_env()\n",
        "\n",
        "# Input file in .jsonlines format.\n",
        "input_filename = \"/content/e2e-coref/test.english.jsonlines\"\n",
        "# Predictions will be written to this file in .jsonlines format.\n",
        "output_filename = \"/content/e2e-coref/output.jsonlines\"\n",
        "\n",
        "model = CorefModel(config)\n",
        "\n",
        "with tf.Session() as session:\n",
        "  model.restore(session)\n",
        "\n",
        "  with open(output_filename, \"w\") as output_file:\n",
        "    with open(input_filename) as input_file:\n",
        "      for example_num, line in enumerate(input_file.readlines()):\n",
        "        example = json.loads(line)\n",
        "        tensorized_example = model.tensorize_example(example, is_training=False)\n",
        "        feed_dict = {i:t for i,t in zip(model.input_tensors, tensorized_example)}\n",
        "        _, _, _, top_span_starts, top_span_ends, top_antecedents, top_antecedent_scores = session.run(model.predictions, feed_dict=feed_dict)\n",
        "        predicted_antecedents = model.get_predicted_antecedents(top_antecedents, top_antecedent_scores)\n",
        "        print(predicted_antecedents)\n",
        "        print('\\n')\n",
        "        example[\"predicted_clusters\"], _ = model.get_predicted_clusters(top_span_starts, top_span_ends, predicted_antecedents)\n",
        "        print(example_num)\n",
        "        output_file.write(json.dumps(example))\n",
        "        output_file.write(\"\\n\")\n",
        "        if example_num % 100 == 0:\n",
        "          print(\"Decoded {} examples.\".format(example_num + 1))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting CUDA_VISIBLE_DEVICES to: \n",
            "Running experiment: final\n",
            "max_top_antecedents = 50\n",
            "max_training_sentences = 50\n",
            "top_span_ratio = 0.4\n",
            "filter_widths = [\n",
            "  3\n",
            "  4\n",
            "  5\n",
            "]\n",
            "filter_size = 50\n",
            "char_embedding_size = 8\n",
            "char_vocab_path = \"char_vocab.english.txt\"\n",
            "context_embeddings {\n",
            "  path = \"glove.840B.300d.txt\"\n",
            "  size = 300\n",
            "}\n",
            "head_embeddings {\n",
            "  path = \"glove_50_300_2.txt\"\n",
            "  size = 300\n",
            "}\n",
            "contextualization_size = 200\n",
            "contextualization_layers = 3\n",
            "ffnn_size = 150\n",
            "ffnn_depth = 2\n",
            "feature_size = 20\n",
            "max_span_width = 30\n",
            "use_metadata = true\n",
            "use_features = true\n",
            "model_heads = true\n",
            "coref_depth = 2\n",
            "lm_layers = 3\n",
            "lm_size = 1024\n",
            "coarse_to_fine = true\n",
            "max_gradient_norm = 5.0\n",
            "lstm_dropout_rate = 0.4\n",
            "lexical_dropout_rate = 0.5\n",
            "dropout_rate = 0.2\n",
            "optimizer = \"adam\"\n",
            "learning_rate = 0.001\n",
            "decay_rate = 0.999\n",
            "decay_frequency = 100\n",
            "train_path = \"train.english.jsonlines\"\n",
            "eval_path = \"test.english.jsonlines\"\n",
            "conll_eval_path = \"test.english.v4_gold_conll\"\n",
            "lm_path = \"elmo_cache.hdf5\"\n",
            "genres = [\n",
            "  \"bc\"\n",
            "  \"bn\"\n",
            "  \"mz\"\n",
            "  \"nw\"\n",
            "  \"pt\"\n",
            "  \"tc\"\n",
            "  \"wb\"\n",
            "]\n",
            "eval_frequency = 5000\n",
            "report_frequency = 100\n",
            "log_root = \"logs\"\n",
            "cluster {\n",
            "  addresses {\n",
            "    ps = [\n",
            "      \"localhost:2222\"\n",
            "    ]\n",
            "    worker = [\n",
            "      \"localhost:2223\"\n",
            "      \"localhost:2224\"\n",
            "    ]\n",
            "  }\n",
            "  gpus = [\n",
            "    0\n",
            "    1\n",
            "  ]\n",
            "}\n",
            "log_dir = \"logs/final\"\n",
            "Loading word embeddings from glove.840B.300d.txt...\n",
            "Done loading word embeddings.\n",
            "Loading word embeddings from glove_50_300_2.txt...\n",
            "Done loading word embeddings.\n",
            "WARNING:tensorflow:From <ipython-input-18-3cf3902c37d2>:201: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0615 01:07:40.329972 140561358612352 deprecation.py:323] From <ipython-input-18-3cf3902c37d2>:201: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0615 01:07:40.364275 140561358612352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-18-3cf3902c37d2>:272: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0615 01:07:40.544330 140561358612352 deprecation.py:506] From <ipython-input-18-3cf3902c37d2>:272: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-18-3cf3902c37d2>:479: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0615 01:07:42.876100 140561358612352 deprecation.py:323] From <ipython-input-18-3cf3902c37d2>:479: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0615 01:07:42.878173 140561358612352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0615 01:07:42.944278 140561358612352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0615 01:07:52.729589 140561358612352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Restoring from logs/final/model.max.ckpt\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0615 01:07:55.030933 140561358612352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from logs/final/model.max.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0615 01:07:55.037799 140561358612352 saver.py:1270] Restoring parameters from logs/final/model.max.ckpt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "0\n",
            "Decoded 1 examples.\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "1\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "2\n",
            "[-1, -1, -1, 0, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "3\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "4\n",
            "[-1, -1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "5\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "6\n",
            "[-1, -1, -1, 0, 2, -1]\n",
            "\n",
            "\n",
            "7\n",
            "[-1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "8\n",
            "[-1, -1, -1, -1, 2]\n",
            "\n",
            "\n",
            "9\n",
            "[-1, -1, 1, -1, -1, -1]\n",
            "\n",
            "\n",
            "10\n",
            "[-1, -1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "11\n",
            "[-1, -1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "12\n",
            "[-1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "13\n",
            "[-1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "14\n",
            "[-1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "15\n",
            "[-1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "16\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "17\n",
            "[-1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "18\n",
            "[-1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "19\n",
            "[-1, -1, 1, -1]\n",
            "\n",
            "\n",
            "20\n",
            "[-1, -1, -1, -1, 2]\n",
            "\n",
            "\n",
            "21\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "22\n",
            "[-1, -1, -1, 1, -1, -1]\n",
            "\n",
            "\n",
            "23\n",
            "[-1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "24\n",
            "[-1, -1, -1, -1, -1, 4]\n",
            "\n",
            "\n",
            "25\n",
            "[-1, -1, -1, -1, 3, -1]\n",
            "\n",
            "\n",
            "26\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "27\n",
            "[-1, -1, -1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "28\n",
            "[-1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "29\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "30\n",
            "[-1, -1, -1, 2, 1]\n",
            "\n",
            "\n",
            "31\n",
            "[-1, -1, -1, 1, -1]\n",
            "\n",
            "\n",
            "32\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "33\n",
            "[-1, -1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "34\n",
            "[-1, -1, -1, -1, 2]\n",
            "\n",
            "\n",
            "35\n",
            "[-1, -1, 0, 2]\n",
            "\n",
            "\n",
            "36\n",
            "[-1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "37\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "38\n",
            "[-1, -1, 0, -1, -1, -1, -1]\n",
            "\n",
            "\n",
            "39\n",
            "[-1, -1, 1, -1, -1, -1]\n",
            "\n",
            "\n",
            "40\n",
            "[-1, -1, 1, -1]\n",
            "\n",
            "\n",
            "41\n",
            "[-1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "42\n",
            "[-1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "43\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "44\n",
            "[-1, -1, 1, -1]\n",
            "\n",
            "\n",
            "45\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "46\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "47\n",
            "[-1, -1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "48\n",
            "[-1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "49\n",
            "[-1, -1, -1, 0]\n",
            "\n",
            "\n",
            "50\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "51\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "52\n",
            "[-1, -1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "53\n",
            "[-1, -1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "54\n",
            "[-1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "55\n",
            "[-1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "56\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "57\n",
            "[-1, -1, -1, -1, 1]\n",
            "\n",
            "\n",
            "58\n",
            "[-1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "59\n",
            "[-1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "60\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "61\n",
            "[-1, -1, -1, -1, -1, 3]\n",
            "\n",
            "\n",
            "62\n",
            "[-1, -1, -1, -1, -1, 3]\n",
            "\n",
            "\n",
            "63\n",
            "[-1, -1, -1, -1, -1, 0, 2]\n",
            "\n",
            "\n",
            "64\n",
            "[-1, -1, -1, -1, 0, 2]\n",
            "\n",
            "\n",
            "65\n",
            "[-1, -1, -1, 2, -1, -1, -1]\n",
            "\n",
            "\n",
            "66\n",
            "[-1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "67\n",
            "[-1, -1, 1, -1, -1]\n",
            "\n",
            "\n",
            "68\n",
            "[-1, -1, 1, -1]\n",
            "\n",
            "\n",
            "69\n",
            "[-1, -1, 1, -1]\n",
            "\n",
            "\n",
            "70\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "71\n",
            "[-1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "72\n",
            "[-1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "73\n",
            "[-1, -1, -1, 1, -1]\n",
            "\n",
            "\n",
            "74\n",
            "[-1, -1, -1, -1, 1, -1]\n",
            "\n",
            "\n",
            "75\n",
            "[-1, -1, -1, -1, 1, -1]\n",
            "\n",
            "\n",
            "76\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "77\n",
            "[-1, -1, -1, 0, 2, -1]\n",
            "\n",
            "\n",
            "78\n",
            "[-1, -1, -1, -1, 0, -1, 3]\n",
            "\n",
            "\n",
            "79\n",
            "[-1, -1, -1, 0, 1]\n",
            "\n",
            "\n",
            "80\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "81\n",
            "[-1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "82\n",
            "[-1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "83\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "84\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "85\n",
            "[-1, -1, -1, 1]\n",
            "\n",
            "\n",
            "86\n",
            "[-1, -1, -1, 2]\n",
            "\n",
            "\n",
            "87\n",
            "[-1, -1, -1, 2]\n",
            "\n",
            "\n",
            "88\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "89\n",
            "[-1, -1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "90\n",
            "[-1, -1, 0, 1, -1]\n",
            "\n",
            "\n",
            "91\n",
            "[-1, -1, 1, -1, -1]\n",
            "\n",
            "\n",
            "92\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "93\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "94\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "95\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "96\n",
            "[-1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "97\n",
            "[-1, -1, -1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "98\n",
            "[-1, -1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "99\n",
            "[-1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "100\n",
            "Decoded 101 examples.\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "101\n",
            "[-1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "102\n",
            "[-1, -1, 1, 2, -1]\n",
            "\n",
            "\n",
            "103\n",
            "[-1, -1, 1, -1]\n",
            "\n",
            "\n",
            "104\n",
            "[-1, -1, 1, 1, -1]\n",
            "\n",
            "\n",
            "105\n",
            "[-1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "106\n",
            "[-1, -1, 1, -1, -1]\n",
            "\n",
            "\n",
            "107\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "108\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "109\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "110\n",
            "[-1, -1, -1, 2]\n",
            "\n",
            "\n",
            "111\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "112\n",
            "[-1, -1, 0, 2, -1]\n",
            "\n",
            "\n",
            "113\n",
            "[-1, -1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "114\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "115\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "116\n",
            "[-1, -1, 1, -1]\n",
            "\n",
            "\n",
            "117\n",
            "[-1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "118\n",
            "[-1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "119\n",
            "[-1, -1, -1, 0, -1, 2]\n",
            "\n",
            "\n",
            "120\n",
            "[-1, -1, -1, -1, 1, 3]\n",
            "\n",
            "\n",
            "121\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "122\n",
            "[-1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "123\n",
            "[-1, -1, -1, 1]\n",
            "\n",
            "\n",
            "124\n",
            "[-1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "125\n",
            "[-1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "126\n",
            "[-1, -1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "127\n",
            "[-1, -1, -1, -1, 2]\n",
            "\n",
            "\n",
            "128\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "129\n",
            "[-1, -1, 1, -1]\n",
            "\n",
            "\n",
            "130\n",
            "[-1, -1, -1, 0]\n",
            "\n",
            "\n",
            "131\n",
            "[-1, -1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "132\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "133\n",
            "[-1, -1, -1, 1, -1, -1]\n",
            "\n",
            "\n",
            "134\n",
            "[-1, -1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "135\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "136\n",
            "[-1, -1, -1, 0]\n",
            "\n",
            "\n",
            "137\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "138\n",
            "[-1, -1, 1, -1, 2, -1]\n",
            "\n",
            "\n",
            "139\n",
            "[-1, -1, -1, 0]\n",
            "\n",
            "\n",
            "140\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "141\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "142\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "143\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "144\n",
            "[-1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "145\n",
            "[-1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "146\n",
            "[-1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "147\n",
            "[-1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "148\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "149\n",
            "[-1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "150\n",
            "[-1, -1, -1, 0]\n",
            "\n",
            "\n",
            "151\n",
            "[-1, -1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "152\n",
            "[-1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "153\n",
            "[-1, -1, 1, -1, -1, -1]\n",
            "\n",
            "\n",
            "154\n",
            "[-1, -1, -1, 2]\n",
            "\n",
            "\n",
            "155\n",
            "[-1, -1, -1, -1]\n",
            "\n",
            "\n",
            "156\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "157\n",
            "[-1, -1, -1, -1, 1]\n",
            "\n",
            "\n",
            "158\n",
            "[-1, -1, -1, -1, 1, -1]\n",
            "\n",
            "\n",
            "159\n",
            "[-1, -1, -1, 0, 2]\n",
            "\n",
            "\n",
            "160\n",
            "[-1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "161\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "162\n",
            "[-1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "163\n",
            "[-1, -1, -1, -1, 3]\n",
            "\n",
            "\n",
            "164\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "165\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "166\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "167\n",
            "[-1, -1, -1, 0, -1, 3, -1, -1]\n",
            "\n",
            "\n",
            "168\n",
            "[-1, -1, -1, 0, -1, 3]\n",
            "\n",
            "\n",
            "169\n",
            "[-1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "170\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "171\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "172\n",
            "[-1, -1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "173\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "174\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "175\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "176\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "177\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "178\n",
            "[-1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "179\n",
            "[-1, -1, -1, 1, 1, -1]\n",
            "\n",
            "\n",
            "180\n",
            "[-1, -1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "181\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "182\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "183\n",
            "[-1, -1, -1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "184\n",
            "[-1, -1, -1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "185\n",
            "[-1, -1, -1, 0, 3, -1]\n",
            "\n",
            "\n",
            "186\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "187\n",
            "[-1, -1, 0, -1, -1, -1, -1, -1]\n",
            "\n",
            "\n",
            "188\n",
            "[-1, -1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "189\n",
            "[-1, -1, -1, 1, -1, -1, -1]\n",
            "\n",
            "\n",
            "190\n",
            "[-1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "191\n",
            "[-1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "192\n",
            "[-1, -1, 0, 0, -1]\n",
            "\n",
            "\n",
            "193\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "194\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "195\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "196\n",
            "[-1, -1, -1, 2, -1, -1, 3, -1]\n",
            "\n",
            "\n",
            "197\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "198\n",
            "[-1, -1, -1, 0, -1, -1, -1, -1]\n",
            "\n",
            "\n",
            "199\n",
            "[-1, -1, -1, 0, 3, -1, -1]\n",
            "\n",
            "\n",
            "200\n",
            "Decoded 201 examples.\n",
            "[-1, -1, -1, 2, -1, -1, -1]\n",
            "\n",
            "\n",
            "201\n",
            "[-1, -1, 0, -1, 2, -1]\n",
            "\n",
            "\n",
            "202\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "203\n",
            "[-1, -1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "204\n",
            "[-1, -1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "205\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "206\n",
            "[-1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "207\n",
            "[-1, -1, -1, 0]\n",
            "\n",
            "\n",
            "208\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "209\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "210\n",
            "[-1, -1, 1, -1]\n",
            "\n",
            "\n",
            "211\n",
            "[-1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "212\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "213\n",
            "[-1, -1, -1, -1, 2, 0]\n",
            "\n",
            "\n",
            "214\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "215\n",
            "[-1, -1, -1, -1, 0, 2]\n",
            "\n",
            "\n",
            "216\n",
            "[-1, -1, -1, -1, 2]\n",
            "\n",
            "\n",
            "217\n",
            "[-1, -1, 1, -1]\n",
            "\n",
            "\n",
            "218\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "219\n",
            "[-1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "220\n",
            "[-1, -1, -1, 2]\n",
            "\n",
            "\n",
            "221\n",
            "[-1, -1, -1, 2]\n",
            "\n",
            "\n",
            "222\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "223\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "224\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "225\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "226\n",
            "[-1, -1, -1, 0, 2, 1]\n",
            "\n",
            "\n",
            "227\n",
            "[-1, -1, -1, -1, 1, -1]\n",
            "\n",
            "\n",
            "228\n",
            "[-1, -1, -1, 0]\n",
            "\n",
            "\n",
            "229\n",
            "[-1, -1, 1, -1]\n",
            "\n",
            "\n",
            "230\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "231\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "232\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "233\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "234\n",
            "[-1, -1, 1, 1, -1]\n",
            "\n",
            "\n",
            "235\n",
            "[-1, -1, 1, 2, -1]\n",
            "\n",
            "\n",
            "236\n",
            "[-1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "237\n",
            "[-1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "238\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "239\n",
            "[-1, -1, 1, -1, -1, -1]\n",
            "\n",
            "\n",
            "240\n",
            "[-1, -1, 1, -1, -1]\n",
            "\n",
            "\n",
            "241\n",
            "[-1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "242\n",
            "[-1, -1, 1, 2, -1]\n",
            "\n",
            "\n",
            "243\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "244\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "245\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "246\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "247\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "248\n",
            "[-1, -1, -1, -1, 0, 1]\n",
            "\n",
            "\n",
            "249\n",
            "[-1, -1, -1, -1, 0, 1]\n",
            "\n",
            "\n",
            "250\n",
            "[-1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "251\n",
            "[-1, -1, -1, 1, -1]\n",
            "\n",
            "\n",
            "252\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "253\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "254\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "255\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "256\n",
            "[-1, -1, -1, 1]\n",
            "\n",
            "\n",
            "257\n",
            "[-1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "258\n",
            "[-1, -1, -1, 2]\n",
            "\n",
            "\n",
            "259\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "260\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "261\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "262\n",
            "[-1, -1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "263\n",
            "[-1, -1, -1, 0]\n",
            "\n",
            "\n",
            "264\n",
            "[-1, -1, -1, 0]\n",
            "\n",
            "\n",
            "265\n",
            "[-1, -1, -1, 0, 1]\n",
            "\n",
            "\n",
            "266\n",
            "[-1, -1, -1, -1, -1, 0, 3]\n",
            "\n",
            "\n",
            "267\n",
            "[-1, -1, -1, -1, -1, -1, 3]\n",
            "\n",
            "\n",
            "268\n",
            "[-1, -1, -1, -1, 2]\n",
            "\n",
            "\n",
            "269\n",
            "[-1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "270\n",
            "[-1, -1, -1, -1, -1, 2, 5, -1]\n",
            "\n",
            "\n",
            "271\n",
            "[-1, -1, -1, 0, -1, -1, 3]\n",
            "\n",
            "\n",
            "272\n",
            "[-1, -1, -1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "273\n",
            "[-1, -1, -1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "274\n",
            "[-1, -1, -1, -1, 3, -1]\n",
            "\n",
            "\n",
            "275\n",
            "[-1, -1, -1, -1, 3]\n",
            "\n",
            "\n",
            "276\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "277\n",
            "[-1, -1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "278\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "279\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "280\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "281\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "282\n",
            "[-1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "283\n",
            "[-1, -1, 1, -1, -1]\n",
            "\n",
            "\n",
            "284\n",
            "[-1, -1, 1, -1]\n",
            "\n",
            "\n",
            "285\n",
            "[-1, -1, -1, -1, 2]\n",
            "\n",
            "\n",
            "286\n",
            "[-1, -1, -1, 0]\n",
            "\n",
            "\n",
            "287\n",
            "[-1, -1, -1, 0]\n",
            "\n",
            "\n",
            "288\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "289\n",
            "[-1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "290\n",
            "[-1, -1, -1, -1, 1, 2]\n",
            "\n",
            "\n",
            "291\n",
            "[-1, -1, -1, -1, 1, 0]\n",
            "\n",
            "\n",
            "292\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "293\n",
            "[-1, -1, -1, -1, -1, 3]\n",
            "\n",
            "\n",
            "294\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "295\n",
            "[-1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "296\n",
            "[-1, -1, -1, -1, 1, -1]\n",
            "\n",
            "\n",
            "297\n",
            "[-1, -1, -1, -1, -1, 2]\n",
            "\n",
            "\n",
            "298\n",
            "[-1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "299\n",
            "[-1, -1, -1, -1, -1, 2]\n",
            "\n",
            "\n",
            "300\n",
            "Decoded 301 examples.\n",
            "[-1, -1, -1, -1, 1, -1]\n",
            "\n",
            "\n",
            "301\n",
            "[-1, -1, -1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "302\n",
            "[-1, -1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "303\n",
            "[-1, -1, -1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "304\n",
            "[-1, -1, -1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "305\n",
            "[-1, -1, -1, 0]\n",
            "\n",
            "\n",
            "306\n",
            "[-1, -1, 1, -1, -1]\n",
            "\n",
            "\n",
            "307\n",
            "[-1, -1, 1, -1]\n",
            "\n",
            "\n",
            "308\n",
            "[-1, -1, 1, -1]\n",
            "\n",
            "\n",
            "309\n",
            "[-1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "310\n",
            "[-1, -1, -1, 1, -1]\n",
            "\n",
            "\n",
            "311\n",
            "[-1, -1, -1, -1, 3, -1]\n",
            "\n",
            "\n",
            "312\n",
            "[-1, -1, -1, -1, -1, 4, -1]\n",
            "\n",
            "\n",
            "313\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "314\n",
            "[-1, -1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "315\n",
            "[-1, -1, -1, 1, -1]\n",
            "\n",
            "\n",
            "316\n",
            "[-1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "317\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "318\n",
            "[-1, -1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "319\n",
            "[-1, -1, -1, -1, 1, -1]\n",
            "\n",
            "\n",
            "320\n",
            "[-1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "321\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "322\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "323\n",
            "[-1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "324\n",
            "[-1, -1, -1, -1, 2]\n",
            "\n",
            "\n",
            "325\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "326\n",
            "[-1, -1, -1, 1, -1]\n",
            "\n",
            "\n",
            "327\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "328\n",
            "[-1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "329\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "330\n",
            "[-1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "331\n",
            "[-1, -1, -1, -1, 3, 3]\n",
            "\n",
            "\n",
            "332\n",
            "[-1, -1, -1, 2, 2, -1]\n",
            "\n",
            "\n",
            "333\n",
            "[-1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "334\n",
            "[-1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "335\n",
            "[-1, -1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "336\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "337\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "338\n",
            "[-1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "339\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "340\n",
            "[-1, -1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "341\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "342\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "343\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "344\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "345\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "346\n",
            "[-1, -1, -1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "347\n",
            "[-1, -1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "348\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "349\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "350\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "351\n",
            "[-1, -1, -1, 0]\n",
            "\n",
            "\n",
            "352\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "353\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "354\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "355\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "356\n",
            "[-1, -1, -1, 0, 1]\n",
            "\n",
            "\n",
            "357\n",
            "[-1, -1, -1, 2]\n",
            "\n",
            "\n",
            "358\n",
            "[-1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "359\n",
            "[-1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "360\n",
            "[-1, -1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "361\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "362\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "363\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "364\n",
            "[-1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "365\n",
            "[-1, -1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "366\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "367\n",
            "[-1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "368\n",
            "[-1, -1, -1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "369\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "370\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "371\n",
            "[-1, -1, -1, -1, 3, -1]\n",
            "\n",
            "\n",
            "372\n",
            "[-1, -1, -1, 2]\n",
            "\n",
            "\n",
            "373\n",
            "[-1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "374\n",
            "[-1, -1, 1, -1, -1]\n",
            "\n",
            "\n",
            "375\n",
            "[-1, -1, -1, 1, -1]\n",
            "\n",
            "\n",
            "376\n",
            "[-1, -1, -1, -1, 2]\n",
            "\n",
            "\n",
            "377\n",
            "[-1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "378\n",
            "[-1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "379\n",
            "[-1, -1, -1, -1, 0, -1]\n",
            "\n",
            "\n",
            "380\n",
            "[-1, -1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "381\n",
            "[-1, -1, -1, 2, -1, -1]\n",
            "\n",
            "\n",
            "382\n",
            "[-1, -1, -1, 2, -1, -1, -1, -1]\n",
            "\n",
            "\n",
            "383\n",
            "[-1, -1, 0, -1, -1, -1]\n",
            "\n",
            "\n",
            "384\n",
            "[-1, -1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "385\n",
            "[-1, -1, -1, -1, 1]\n",
            "\n",
            "\n",
            "386\n",
            "[-1, -1, -1, -1, -1, -1, 2]\n",
            "\n",
            "\n",
            "387\n",
            "[-1, -1, -1, -1, -1, 2, -1]\n",
            "\n",
            "\n",
            "388\n",
            "[-1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "389\n",
            "[-1, -1, -1, -1, 0, 3]\n",
            "\n",
            "\n",
            "390\n",
            "[-1, -1, 0, -1, -1]\n",
            "\n",
            "\n",
            "391\n",
            "[-1, -1, -1, -1, 0]\n",
            "\n",
            "\n",
            "392\n",
            "[-1, -1, -1, 2]\n",
            "\n",
            "\n",
            "393\n",
            "[-1, -1, 0, -1]\n",
            "\n",
            "\n",
            "394\n",
            "[-1, -1, -1, -1, 3]\n",
            "\n",
            "\n",
            "395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GHfl4o4l39U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b68e2b9-f1bb-49c7-f635-0b8d5933dc66"
      },
      "source": [
        "import numpy as np\n",
        "i = 0\n",
        "y_pred = []\n",
        "with open('output.jsonlines') as output:\n",
        "  for line in output:\n",
        "    i +=1\n",
        "    result = eval(line)\n",
        "#     print(result['predicted_clusters']), print(result['clusters'][0])\n",
        "    is_in = 1\n",
        "    if result['predicted_clusters']!=[]:\n",
        "      for item in result['predicted_clusters'][0]:\n",
        "        if item not in result['clusters'][0]:\n",
        "          is_in = 0\n",
        "      if is_in == 1 :\n",
        "        y_pred.append(1)\n",
        "      else:\n",
        "        y_pred.append(0)\n",
        "    else:\n",
        "      y_pred.append(0)\n",
        "\n",
        "y_truth = np.ones(i)\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "print(f1_score(y_truth, y_pred))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7368421052631579\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}